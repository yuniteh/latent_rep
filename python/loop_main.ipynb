{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "from matplotlib import pyplot as plt\n",
    "import loop\n",
    "import session\n",
    "import plot_utils \n",
    "set_gpu()\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sub 1, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.255 0.061 1.3   2.893 0.008 0.572 0.248]\n",
      "[6.301 0.036 1.104 5.16  0.008 0.608 0.249]\n",
      "[6.06  0.034 1.014 5.012 0.008 0.63  0.249]\n",
      "[6.342 0.031 0.971 5.339 0.008 0.642 0.226]\n",
      "[6.141 0.029 0.97  5.141 0.008 0.65  0.239]\n",
      "[6.561 0.028 0.935 5.597 0.008 0.667 0.235]\n",
      "[7.029 0.027 0.931 6.071 0.008 0.671 0.252]\n",
      "[7.712 0.026 0.99  6.695 0.008 0.661 0.238]\n",
      "[8.209 0.025 0.976 7.207 0.008 0.664 0.245]\n",
      "[9.125 0.025 1.033 8.066 0.008 0.657 0.243]\n",
      "[9.29  0.024 1.007 8.258 0.008 0.665 0.237]\n",
      "[9.656 0.024 1.038 8.594 0.008 0.666 0.241]\n",
      "[10.199  0.023  1.045  9.131  0.008  0.663  0.25 ]\n",
      "[10.532  0.023  1.093  9.416  0.008  0.667  0.241]\n",
      "[10.399  0.023  1.06   9.316  0.008  0.674  0.253]\n",
      "[1.65  0.046 1.024 0.579 0.008 0.669 0.235]\n",
      "[1.329 0.045 1.031 0.253 0.008 0.678 0.241]\n",
      "[1.202 0.045 1.038 0.12  0.008 0.68  0.247]\n",
      "[1.189 0.045 1.082 0.063 0.008 0.675 0.241]\n",
      "[1.192 0.044 1.11  0.038 0.008 0.671 0.246]\n",
      "[1.208 0.045 1.138 0.025 0.008 0.673 0.249]\n",
      "[1.165 0.044 1.095 0.026 0.008 0.681 0.259]\n",
      "[1.156 0.045 1.096 0.016 0.008 0.684 0.262]\n",
      "[1.201 0.044 1.141 0.015 0.008 0.681 0.264]\n",
      "[1.221 0.044 1.165 0.011 0.008 0.684 0.262]\n",
      "[1.246 0.045 1.193 0.009 0.008 0.684 0.268]\n",
      "[1.225 0.045 1.174 0.006 0.008 0.688 0.27 ]\n",
      "[1.249 0.044 1.197 0.008 0.008 0.689 0.282]\n",
      "[1.224 0.044 1.174 0.006 0.008 0.692 0.276]\n",
      "[1.226 0.044 1.176 0.005 0.008 0.696 0.279]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 129us/sample - loss: 39.1461 - accuracy: 0.3971 - val_loss: 47.2048 - val_accuracy: 0.2276\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 27.7354 - accuracy: 0.5981 - val_loss: 28.0936 - val_accuracy: 0.5652\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 20.7246 - accuracy: 0.7134 - val_loss: 25.4814 - val_accuracy: 0.6094\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 16.1614 - accuracy: 0.7817 - val_loss: 20.5204 - val_accuracy: 0.6846\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 13.2620 - accuracy: 0.8212 - val_loss: 20.7449 - val_accuracy: 0.6770\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 11.3444 - accuracy: 0.8458 - val_loss: 22.0002 - val_accuracy: 0.6659\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 9.9752 - accuracy: 0.8611 - val_loss: 19.2869 - val_accuracy: 0.7032\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 9.2567 - accuracy: 0.8715 - val_loss: 18.8459 - val_accuracy: 0.7024\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 8.3673 - accuracy: 0.8815 - val_loss: 19.9624 - val_accuracy: 0.7003\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 7.8277 - accuracy: 0.8883 - val_loss: 21.4655 - val_accuracy: 0.6832\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 7.4656 - accuracy: 0.8940 - val_loss: 18.3151 - val_accuracy: 0.7237\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 6.9455 - accuracy: 0.9017 - val_loss: 19.5105 - val_accuracy: 0.7106\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 6.6537 - accuracy: 0.9046 - val_loss: 18.4072 - val_accuracy: 0.7260\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 87us/sample - loss: 6.3082 - accuracy: 0.9098 - val_loss: 25.3274 - val_accuracy: 0.6797\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 6.1630 - accuracy: 0.9106 - val_loss: 20.5793 - val_accuracy: 0.7173\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 5.9561 - accuracy: 0.9135 - val_loss: 18.8560 - val_accuracy: 0.7259\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 93us/sample - loss: 5.8064 - accuracy: 0.9162 - val_loss: 21.4681 - val_accuracy: 0.7097\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 5.6038 - accuracy: 0.9180 - val_loss: 19.0322 - val_accuracy: 0.7256\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 5.4216 - accuracy: 0.9220 - val_loss: 20.8682 - val_accuracy: 0.7162\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 5.0737 - accuracy: 0.9276 - val_loss: 20.8920 - val_accuracy: 0.7213\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 5.0875 - accuracy: 0.9273 - val_loss: 21.5301 - val_accuracy: 0.7119\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.0281 - accuracy: 0.9264 - val_loss: 22.0795 - val_accuracy: 0.7117\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 4.8394 - accuracy: 0.9300 - val_loss: 19.3629 - val_accuracy: 0.7438\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 4.7322 - accuracy: 0.9321 - val_loss: 21.3580 - val_accuracy: 0.7217\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.6272 - accuracy: 0.9334 - val_loss: 20.9402 - val_accuracy: 0.7325\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.5661 - accuracy: 0.9346 - val_loss: 23.0674 - val_accuracy: 0.7106\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.5669 - accuracy: 0.9360 - val_loss: 22.1317 - val_accuracy: 0.7214\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 4.3489 - accuracy: 0.9376 - val_loss: 24.0488 - val_accuracy: 0.7078\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.3541 - accuracy: 0.9377 - val_loss: 22.4431 - val_accuracy: 0.7233\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.2236 - accuracy: 0.9397 - val_loss: 24.1706 - val_accuracy: 0.7114\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 1.8922 - accuracy: 0.2570 - val_loss: 1.8470 - val_accuracy: 0.2403\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 1.5520 - accuracy: 0.4160 - val_loss: 1.5178 - val_accuracy: 0.4108\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 1.3121 - accuracy: 0.5153 - val_loss: 1.3175 - val_accuracy: 0.5051\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 1.1261 - accuracy: 0.5906 - val_loss: 1.1806 - val_accuracy: 0.5529\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.9777 - accuracy: 0.6467 - val_loss: 1.0539 - val_accuracy: 0.6117\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8697 - accuracy: 0.6855 - val_loss: 0.9741 - val_accuracy: 0.6473\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7954 - accuracy: 0.7124 - val_loss: 0.9114 - val_accuracy: 0.6630\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7346 - accuracy: 0.7378 - val_loss: 0.8589 - val_accuracy: 0.6770\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6981 - accuracy: 0.7487 - val_loss: 0.8594 - val_accuracy: 0.6706\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6668 - accuracy: 0.7599 - val_loss: 0.8471 - val_accuracy: 0.6743\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.6448 - accuracy: 0.7665 - val_loss: 0.7995 - val_accuracy: 0.6860\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.6233 - accuracy: 0.7725 - val_loss: 0.8532 - val_accuracy: 0.6754\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.6019 - accuracy: 0.7809 - val_loss: 0.8550 - val_accuracy: 0.6675\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.5847 - accuracy: 0.7865 - val_loss: 0.7978 - val_accuracy: 0.6900\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5730 - accuracy: 0.7890 - val_loss: 0.8203 - val_accuracy: 0.6830\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5543 - accuracy: 0.7986 - val_loss: 0.8106 - val_accuracy: 0.6903\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.5475 - accuracy: 0.7981 - val_loss: 0.7765 - val_accuracy: 0.6989\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5397 - accuracy: 0.8019 - val_loss: 0.8080 - val_accuracy: 0.6905\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5284 - accuracy: 0.8034 - val_loss: 0.8158 - val_accuracy: 0.6948\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.5208 - accuracy: 0.8085 - val_loss: 0.7231 - val_accuracy: 0.7192\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.5120 - accuracy: 0.8102 - val_loss: 0.7400 - val_accuracy: 0.7137\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.5065 - accuracy: 0.8146 - val_loss: 0.7174 - val_accuracy: 0.7219\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.4993 - accuracy: 0.8165 - val_loss: 0.7622 - val_accuracy: 0.7094\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.4916 - accuracy: 0.8166 - val_loss: 0.7048 - val_accuracy: 0.7268\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4865 - accuracy: 0.8190 - val_loss: 0.7234 - val_accuracy: 0.7210\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4809 - accuracy: 0.8212 - val_loss: 0.7189 - val_accuracy: 0.7244\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 0.4821 - accuracy: 0.8207 - val_loss: 0.7296 - val_accuracy: 0.7195\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.4713 - accuracy: 0.8256 - val_loss: 0.7661 - val_accuracy: 0.7108\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4725 - accuracy: 0.8256 - val_loss: 0.7111 - val_accuracy: 0.7340\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4588 - accuracy: 0.8312 - val_loss: 0.6933 - val_accuracy: 0.7367\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 1.3567 - accuracy: 0.5252 - val_loss: 1.7351 - val_accuracy: 0.3321\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.8712 - accuracy: 0.7517 - val_loss: 1.2463 - val_accuracy: 0.5324\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.6213 - accuracy: 0.8243 - val_loss: 0.9659 - val_accuracy: 0.6546\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.4779 - accuracy: 0.8551 - val_loss: 0.8733 - val_accuracy: 0.6822\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.3978 - accuracy: 0.8742 - val_loss: 1.1510 - val_accuracy: 0.6260\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.3547 - accuracy: 0.8845 - val_loss: 0.9866 - val_accuracy: 0.6548\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.3212 - accuracy: 0.8943 - val_loss: 0.8544 - val_accuracy: 0.6927\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2968 - accuracy: 0.8975 - val_loss: 0.9802 - val_accuracy: 0.6790\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2765 - accuracy: 0.9046 - val_loss: 0.8509 - val_accuracy: 0.7110\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.2630 - accuracy: 0.9087 - val_loss: 0.9982 - val_accuracy: 0.6843\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.2525 - accuracy: 0.9107 - val_loss: 1.0009 - val_accuracy: 0.6954\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.2391 - accuracy: 0.9175 - val_loss: 0.8904 - val_accuracy: 0.7111\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.2300 - accuracy: 0.9190 - val_loss: 1.0274 - val_accuracy: 0.6868\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2190 - accuracy: 0.9221 - val_loss: 1.0234 - val_accuracy: 0.7005\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2118 - accuracy: 0.9242 - val_loss: 1.0400 - val_accuracy: 0.6930\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2005 - accuracy: 0.9286 - val_loss: 1.0833 - val_accuracy: 0.6906\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2004 - accuracy: 0.9293 - val_loss: 1.0487 - val_accuracy: 0.6951\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1938 - accuracy: 0.9318 - val_loss: 1.1414 - val_accuracy: 0.6779\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1878 - accuracy: 0.9324 - val_loss: 1.0334 - val_accuracy: 0.7038\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1841 - accuracy: 0.9327 - val_loss: 0.9718 - val_accuracy: 0.7151\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1794 - accuracy: 0.9361 - val_loss: 1.1095 - val_accuracy: 0.6998\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1766 - accuracy: 0.9363 - val_loss: 1.0663 - val_accuracy: 0.6919\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1706 - accuracy: 0.9404 - val_loss: 1.1055 - val_accuracy: 0.6906\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1668 - accuracy: 0.9400 - val_loss: 1.0585 - val_accuracy: 0.7003\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1661 - accuracy: 0.9422 - val_loss: 0.9104 - val_accuracy: 0.7335\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1648 - accuracy: 0.9405 - val_loss: 1.0899 - val_accuracy: 0.6986\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1623 - accuracy: 0.9423 - val_loss: 0.9186 - val_accuracy: 0.7306\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1554 - accuracy: 0.9441 - val_loss: 0.9875 - val_accuracy: 0.7210\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.1522 - accuracy: 0.9444 - val_loss: 0.9876 - val_accuracy: 0.7083\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1584 - accuracy: 0.9435 - val_loss: 1.0486 - val_accuracy: 0.7149\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 197us/sample - loss: 1.3474 - decoder_loss: 0.2153 - clf_loss: 1.1304 - decoder_accuracy: 0.0073 - clf_accuracy: 0.6149 - val_loss: 1.3791 - val_decoder_loss: 0.1452 - val_clf_loss: 1.2326 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.5527\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.7327 - decoder_loss: 0.1643 - clf_loss: 0.5669 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8224 - val_loss: 1.1954 - val_decoder_loss: 0.1589 - val_clf_loss: 1.0375 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.5933\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.5671 - decoder_loss: 0.1554 - clf_loss: 0.4102 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8660 - val_loss: 1.2514 - val_decoder_loss: 0.1472 - val_clf_loss: 1.1085 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.6408\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 123us/sample - loss: 0.4901 - decoder_loss: 0.1507 - clf_loss: 0.3379 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8876 - val_loss: 1.0954 - val_decoder_loss: 0.1451 - val_clf_loss: 0.9469 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.6730\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.4430 - decoder_loss: 0.1463 - clf_loss: 0.2951 - decoder_accuracy: 0.0078 - clf_accuracy: 0.8993 - val_loss: 1.0941 - val_decoder_loss: 0.1439 - val_clf_loss: 0.9564 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6668\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.4149 - decoder_loss: 0.1427 - clf_loss: 0.2706 - decoder_accuracy: 0.0078 - clf_accuracy: 0.9066 - val_loss: 1.0246 - val_decoder_loss: 0.1298 - val_clf_loss: 0.8909 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6862\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 131us/sample - loss: 0.3934 - decoder_loss: 0.1396 - clf_loss: 0.2523 - decoder_accuracy: 0.0078 - clf_accuracy: 0.9142 - val_loss: 1.1363 - val_decoder_loss: 0.1299 - val_clf_loss: 0.9997 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6686\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.3744 - decoder_loss: 0.1367 - clf_loss: 0.2361 - decoder_accuracy: 0.0078 - clf_accuracy: 0.9185 - val_loss: 1.1425 - val_decoder_loss: 0.1268 - val_clf_loss: 1.0142 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6713\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 123us/sample - loss: 0.3660 - decoder_loss: 0.1341 - clf_loss: 0.2304 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9196 - val_loss: 0.9950 - val_decoder_loss: 0.1306 - val_clf_loss: 0.8579 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7041\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.3479 - decoder_loss: 0.1314 - clf_loss: 0.2149 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9255 - val_loss: 1.1077 - val_decoder_loss: 0.1250 - val_clf_loss: 0.9790 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6751\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.3387 - decoder_loss: 0.1296 - clf_loss: 0.2076 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9279 - val_loss: 1.1500 - val_decoder_loss: 0.1237 - val_clf_loss: 1.0323 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6895\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.3254 - decoder_loss: 0.1273 - clf_loss: 0.1966 - decoder_accuracy: 0.0078 - clf_accuracy: 0.9314 - val_loss: 1.1578 - val_decoder_loss: 0.1207 - val_clf_loss: 1.0381 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6867\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.3225 - decoder_loss: 0.1263 - clf_loss: 0.1947 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9330 - val_loss: 1.0870 - val_decoder_loss: 0.1209 - val_clf_loss: 0.9599 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6848\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.3100 - decoder_loss: 0.1241 - clf_loss: 0.1843 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9345 - val_loss: 1.1295 - val_decoder_loss: 0.1208 - val_clf_loss: 1.0159 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6895\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 130us/sample - loss: 0.3051 - decoder_loss: 0.1225 - clf_loss: 0.1810 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9362 - val_loss: 1.0313 - val_decoder_loss: 0.1152 - val_clf_loss: 0.9057 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7048\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.3033 - decoder_loss: 0.1210 - clf_loss: 0.1807 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9361 - val_loss: 1.1126 - val_decoder_loss: 0.1187 - val_clf_loss: 0.9953 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6990\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2933 - decoder_loss: 0.1194 - clf_loss: 0.1723 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9401 - val_loss: 1.0018 - val_decoder_loss: 0.1142 - val_clf_loss: 0.8803 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7156\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.2915 - decoder_loss: 0.1181 - clf_loss: 0.1719 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9392 - val_loss: 0.9325 - val_decoder_loss: 0.1116 - val_clf_loss: 0.8187 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7235\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.2834 - decoder_loss: 0.1175 - clf_loss: 0.1644 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9412 - val_loss: 1.1789 - val_decoder_loss: 0.1155 - val_clf_loss: 1.0667 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6881\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.2824 - decoder_loss: 0.1165 - clf_loss: 0.1643 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9419 - val_loss: 1.1055 - val_decoder_loss: 0.1114 - val_clf_loss: 0.9870 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6887\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.2773 - decoder_loss: 0.1151 - clf_loss: 0.1607 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9430 - val_loss: 1.0553 - val_decoder_loss: 0.1120 - val_clf_loss: 0.9376 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7051\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 117us/sample - loss: 0.2729 - decoder_loss: 0.1144 - clf_loss: 0.1570 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9438 - val_loss: 1.3145 - val_decoder_loss: 0.1111 - val_clf_loss: 1.2000 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6722\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.2697 - decoder_loss: 0.1134 - clf_loss: 0.1548 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9449 - val_loss: 1.0621 - val_decoder_loss: 0.1101 - val_clf_loss: 0.9428 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7110\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.2688 - decoder_loss: 0.1127 - clf_loss: 0.1546 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9439 - val_loss: 1.1090 - val_decoder_loss: 0.1083 - val_clf_loss: 0.9965 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.6995\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2634 - decoder_loss: 0.1120 - clf_loss: 0.1499 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9471 - val_loss: 1.0233 - val_decoder_loss: 0.1091 - val_clf_loss: 0.9103 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7133\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.2633 - decoder_loss: 0.1113 - clf_loss: 0.1504 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9455 - val_loss: 1.0986 - val_decoder_loss: 0.1080 - val_clf_loss: 0.9791 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7038\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 123us/sample - loss: 0.2593 - decoder_loss: 0.1106 - clf_loss: 0.1473 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9469 - val_loss: 1.1440 - val_decoder_loss: 0.1072 - val_clf_loss: 1.0392 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.6962\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.2519 - decoder_loss: 0.1102 - clf_loss: 0.1403 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9506 - val_loss: 0.8976 - val_decoder_loss: 0.1069 - val_clf_loss: 0.7949 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7483\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.2499 - decoder_loss: 0.1093 - clf_loss: 0.1391 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9506 - val_loss: 1.0295 - val_decoder_loss: 0.1052 - val_clf_loss: 0.9250 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7076\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2505 - decoder_loss: 0.1088 - clf_loss: 0.1403 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9506 - val_loss: 1.0771 - val_decoder_loss: 0.1049 - val_clf_loss: 0.9759 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7122\n",
      "Running sub 2, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[5.166 0.049 1.485 3.632 0.007 0.413 0.323]\n",
      "[8.26  0.038 1.242 6.979 0.007 0.535 0.22 ]\n",
      "[7.171 0.037 1.073 6.06  0.007 0.631 0.21 ]\n",
      "[6.976 0.035 0.945 5.995 0.007 0.683 0.175]\n",
      "[7.211 0.033 0.862 6.316 0.007 0.714 0.202]\n",
      "[7.201 0.03  0.838 6.332 0.007 0.731 0.197]\n",
      "[7.664 0.028 0.792 6.844 0.007 0.751 0.196]\n",
      "[7.713 0.027 0.803 6.882 0.007 0.758 0.196]\n",
      "[8.051 0.025 0.783 7.243 0.007 0.765 0.186]\n",
      "[8.889 0.024 0.807 8.058 0.007 0.764 0.181]\n",
      "[9.149 0.024 0.812 8.313 0.007 0.761 0.179]\n",
      "[9.738 0.023 0.817 8.898 0.007 0.76  0.179]\n",
      "[10.508  0.022  0.843  9.642  0.007  0.763  0.16 ]\n",
      "[10.697  0.022  0.82   9.856  0.007  0.764  0.17 ]\n",
      "[11.002  0.021  0.845 10.136  0.007  0.762  0.162]\n",
      "[1.499 0.047 0.829 0.623 0.007 0.767 0.176]\n",
      "[1.167 0.047 0.819 0.301 0.007 0.734 0.145]\n",
      "[0.994 0.047 0.805 0.143 0.007 0.754 0.174]\n",
      "[0.906 0.047 0.801 0.058 0.007 0.762 0.172]\n",
      "[0.88  0.046 0.807 0.026 0.007 0.762 0.159]\n",
      "[0.881 0.047 0.826 0.008 0.007 0.762 0.158]\n",
      "[0.899 0.046 0.851 0.002 0.007 0.761 0.164]\n",
      "[0.916 0.046 0.868 0.001 0.007 0.76  0.16 ]\n",
      "[0.908 0.046 0.861 0.001 0.007 0.759 0.16 ]\n",
      "[0.917 0.046 0.87  0.001 0.007 0.759 0.166]\n",
      "[0.907 0.046 0.861 0.    0.007 0.761 0.155]\n",
      "[0.912 0.046 0.865 0.001 0.007 0.76  0.158]\n",
      "[0.932 0.047 0.885 0.001 0.007 0.761 0.156]\n",
      "[0.927 0.047 0.879 0.001 0.007 0.764 0.158]\n",
      "[0.941 0.046 0.894 0.001 0.007 0.761 0.163]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 134us/sample - loss: 43.4696 - accuracy: 0.3295 - val_loss: 50.5051 - val_accuracy: 0.1938\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 32.7748 - accuracy: 0.4821 - val_loss: 34.1855 - val_accuracy: 0.4486\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 27.1998 - accuracy: 0.5760 - val_loss: 27.7052 - val_accuracy: 0.5529\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 22.6969 - accuracy: 0.6546 - val_loss: 25.7644 - val_accuracy: 0.6032\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 18.5686 - accuracy: 0.7344 - val_loss: 23.6533 - val_accuracy: 0.6586\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 15.4876 - accuracy: 0.7810 - val_loss: 22.1894 - val_accuracy: 0.6900\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 13.4476 - accuracy: 0.8108 - val_loss: 20.7843 - val_accuracy: 0.7154\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 86us/sample - loss: 12.0968 - accuracy: 0.8292 - val_loss: 19.8534 - val_accuracy: 0.7359\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 11.1757 - accuracy: 0.8415 - val_loss: 19.4956 - val_accuracy: 0.7390\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 10.4821 - accuracy: 0.8498 - val_loss: 19.4193 - val_accuracy: 0.7465\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 9.7926 - accuracy: 0.8600 - val_loss: 19.2254 - val_accuracy: 0.7495\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 9.4801 - accuracy: 0.8635 - val_loss: 18.7003 - val_accuracy: 0.7567\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 9.0648 - accuracy: 0.8711 - val_loss: 18.0535 - val_accuracy: 0.7694\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 86us/sample - loss: 8.8498 - accuracy: 0.8746 - val_loss: 19.6372 - val_accuracy: 0.7503\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 8.4948 - accuracy: 0.8781 - val_loss: 19.1626 - val_accuracy: 0.7583\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 8.1472 - accuracy: 0.8817 - val_loss: 18.6612 - val_accuracy: 0.7656\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 7.9894 - accuracy: 0.8837 - val_loss: 19.2230 - val_accuracy: 0.7557\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 7.7013 - accuracy: 0.8906 - val_loss: 18.7319 - val_accuracy: 0.7662\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 7.6587 - accuracy: 0.8883 - val_loss: 18.5035 - val_accuracy: 0.7711\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 7.4424 - accuracy: 0.8912 - val_loss: 17.3655 - val_accuracy: 0.7890\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 7.2716 - accuracy: 0.8937 - val_loss: 20.1771 - val_accuracy: 0.7533\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 7.1628 - accuracy: 0.8958 - val_loss: 19.6114 - val_accuracy: 0.7660\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 6.9982 - accuracy: 0.8993 - val_loss: 18.2827 - val_accuracy: 0.7665\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 6.8148 - accuracy: 0.9008 - val_loss: 18.7134 - val_accuracy: 0.7703\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 6.6493 - accuracy: 0.9038 - val_loss: 19.3215 - val_accuracy: 0.7606\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 6.5400 - accuracy: 0.9054 - val_loss: 17.6123 - val_accuracy: 0.7775\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 6.4336 - accuracy: 0.9060 - val_loss: 19.3850 - val_accuracy: 0.7716\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 6.4279 - accuracy: 0.9063 - val_loss: 17.9048 - val_accuracy: 0.7786\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 6.3629 - accuracy: 0.9062 - val_loss: 18.5196 - val_accuracy: 0.7681\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 6.1665 - accuracy: 0.9108 - val_loss: 18.0098 - val_accuracy: 0.7813\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 90us/sample - loss: 1.9400 - accuracy: 0.2371 - val_loss: 1.8373 - val_accuracy: 0.2657\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 1.6419 - accuracy: 0.3583 - val_loss: 1.6522 - val_accuracy: 0.3440\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 1.4814 - accuracy: 0.4202 - val_loss: 1.5299 - val_accuracy: 0.3995\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 1.3568 - accuracy: 0.4746 - val_loss: 1.4405 - val_accuracy: 0.4606\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 1.2616 - accuracy: 0.5147 - val_loss: 1.3547 - val_accuracy: 0.5068\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 1.1849 - accuracy: 0.5403 - val_loss: 1.3023 - val_accuracy: 0.5159\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 1.1229 - accuracy: 0.5609 - val_loss: 1.2463 - val_accuracy: 0.5305\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 1.0662 - accuracy: 0.5821 - val_loss: 1.2049 - val_accuracy: 0.5483\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 1.0271 - accuracy: 0.5980 - val_loss: 1.2314 - val_accuracy: 0.5457\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.9902 - accuracy: 0.6150 - val_loss: 1.1828 - val_accuracy: 0.5565\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.9636 - accuracy: 0.6250 - val_loss: 1.1634 - val_accuracy: 0.5735\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.9413 - accuracy: 0.6327 - val_loss: 1.1716 - val_accuracy: 0.5616\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.9232 - accuracy: 0.6434 - val_loss: 1.1098 - val_accuracy: 0.5816\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8999 - accuracy: 0.6534 - val_loss: 1.1726 - val_accuracy: 0.5675\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.8841 - accuracy: 0.6593 - val_loss: 1.1269 - val_accuracy: 0.5819\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8733 - accuracy: 0.6666 - val_loss: 1.0951 - val_accuracy: 0.5967\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.8583 - accuracy: 0.6713 - val_loss: 1.1076 - val_accuracy: 0.5938\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8440 - accuracy: 0.6767 - val_loss: 1.1150 - val_accuracy: 0.5881\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8346 - accuracy: 0.6833 - val_loss: 1.1356 - val_accuracy: 0.5838\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8261 - accuracy: 0.6885 - val_loss: 1.0989 - val_accuracy: 0.5963\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.8182 - accuracy: 0.6922 - val_loss: 1.0710 - val_accuracy: 0.6157\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8118 - accuracy: 0.6914 - val_loss: 1.0752 - val_accuracy: 0.6154\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.7958 - accuracy: 0.6988 - val_loss: 1.0690 - val_accuracy: 0.6159\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 0.7905 - accuracy: 0.7035 - val_loss: 1.1436 - val_accuracy: 0.6024\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7810 - accuracy: 0.7073 - val_loss: 1.1113 - val_accuracy: 0.6217\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.7684 - accuracy: 0.7104 - val_loss: 1.0602 - val_accuracy: 0.6222\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.7677 - accuracy: 0.7082 - val_loss: 1.1125 - val_accuracy: 0.6210\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.7527 - accuracy: 0.7163 - val_loss: 1.0999 - val_accuracy: 0.6125\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.7477 - accuracy: 0.7179 - val_loss: 1.0611 - val_accuracy: 0.6294\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.7412 - accuracy: 0.7206 - val_loss: 1.0985 - val_accuracy: 0.6271\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 1.3811 - accuracy: 0.5083 - val_loss: 1.8190 - val_accuracy: 0.1900\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.9370 - accuracy: 0.7137 - val_loss: 1.0539 - val_accuracy: 0.6573\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.7096 - accuracy: 0.7824 - val_loss: 0.9166 - val_accuracy: 0.6971\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5791 - accuracy: 0.8144 - val_loss: 0.8885 - val_accuracy: 0.6846\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.5057 - accuracy: 0.8352 - val_loss: 0.8088 - val_accuracy: 0.7335\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.4658 - accuracy: 0.8433 - val_loss: 0.8166 - val_accuracy: 0.7357\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4353 - accuracy: 0.8483 - val_loss: 0.8340 - val_accuracy: 0.7394\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4077 - accuracy: 0.8591 - val_loss: 0.7840 - val_accuracy: 0.7332\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.3863 - accuracy: 0.8646 - val_loss: 0.8457 - val_accuracy: 0.7500\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.3652 - accuracy: 0.8754 - val_loss: 0.7944 - val_accuracy: 0.7595\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.3627 - accuracy: 0.8716 - val_loss: 0.7551 - val_accuracy: 0.7614\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.3442 - accuracy: 0.8766 - val_loss: 0.7412 - val_accuracy: 0.7710\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3344 - accuracy: 0.8821 - val_loss: 0.8344 - val_accuracy: 0.7656\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3278 - accuracy: 0.8838 - val_loss: 0.7892 - val_accuracy: 0.7637\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3157 - accuracy: 0.8871 - val_loss: 0.8119 - val_accuracy: 0.7689\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3124 - accuracy: 0.8895 - val_loss: 0.8241 - val_accuracy: 0.7583\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.3076 - accuracy: 0.8920 - val_loss: 0.8851 - val_accuracy: 0.7419\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3010 - accuracy: 0.8927 - val_loss: 0.8051 - val_accuracy: 0.7683\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2917 - accuracy: 0.8958 - val_loss: 0.7998 - val_accuracy: 0.7597\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2908 - accuracy: 0.8940 - val_loss: 0.8337 - val_accuracy: 0.7662\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2827 - accuracy: 0.8980 - val_loss: 0.7727 - val_accuracy: 0.7724\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2739 - accuracy: 0.9016 - val_loss: 0.8678 - val_accuracy: 0.7937\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2743 - accuracy: 0.9018 - val_loss: 0.8040 - val_accuracy: 0.7930\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2733 - accuracy: 0.9001 - val_loss: 0.8451 - val_accuracy: 0.7832\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2658 - accuracy: 0.9045 - val_loss: 0.8327 - val_accuracy: 0.7611\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2684 - accuracy: 0.9032 - val_loss: 0.8305 - val_accuracy: 0.7632\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2593 - accuracy: 0.9052 - val_loss: 0.8413 - val_accuracy: 0.7705\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2492 - accuracy: 0.9114 - val_loss: 0.8823 - val_accuracy: 0.7800\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2548 - accuracy: 0.9067 - val_loss: 0.8136 - val_accuracy: 0.7662\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2458 - accuracy: 0.9093 - val_loss: 0.8392 - val_accuracy: 0.7808\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 181us/sample - loss: 1.5537 - decoder_loss: 0.2262 - clf_loss: 1.3262 - decoder_accuracy: 0.0068 - clf_accuracy: 0.5223 - val_loss: 1.8009 - val_decoder_loss: 0.1795 - val_clf_loss: 1.6236 - val_decoder_accuracy: 0.0074 - val_clf_accuracy: 0.3949\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.9682 - decoder_loss: 0.1630 - clf_loss: 0.8037 - decoder_accuracy: 0.0071 - clf_accuracy: 0.7347 - val_loss: 1.0887 - val_decoder_loss: 0.1626 - val_clf_loss: 0.9240 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.6648\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.7301 - decoder_loss: 0.1605 - clf_loss: 0.5680 - decoder_accuracy: 0.0073 - clf_accuracy: 0.8112 - val_loss: 1.0264 - val_decoder_loss: 0.1737 - val_clf_loss: 0.8492 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7151\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.6300 - decoder_loss: 0.1592 - clf_loss: 0.4695 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8393 - val_loss: 0.8786 - val_decoder_loss: 0.1600 - val_clf_loss: 0.7204 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7543\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.5715 - decoder_loss: 0.1553 - clf_loss: 0.4148 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8575 - val_loss: 0.8551 - val_decoder_loss: 0.1633 - val_clf_loss: 0.6923 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7660\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.5483 - decoder_loss: 0.1513 - clf_loss: 0.3955 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8628 - val_loss: 0.8197 - val_decoder_loss: 0.1514 - val_clf_loss: 0.6686 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7668\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.5162 - decoder_loss: 0.1475 - clf_loss: 0.3673 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8729 - val_loss: 0.7941 - val_decoder_loss: 0.1461 - val_clf_loss: 0.6515 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7743\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.5057 - decoder_loss: 0.1442 - clf_loss: 0.3600 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8732 - val_loss: 0.8262 - val_decoder_loss: 0.1490 - val_clf_loss: 0.6780 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7781\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4854 - decoder_loss: 0.1414 - clf_loss: 0.3425 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8790 - val_loss: 0.8102 - val_decoder_loss: 0.1445 - val_clf_loss: 0.6599 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7817\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4687 - decoder_loss: 0.1389 - clf_loss: 0.3284 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8836 - val_loss: 0.7587 - val_decoder_loss: 0.1457 - val_clf_loss: 0.6103 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8030\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.4576 - decoder_loss: 0.1363 - clf_loss: 0.3198 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8869 - val_loss: 0.7584 - val_decoder_loss: 0.1351 - val_clf_loss: 0.6187 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7897\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4434 - decoder_loss: 0.1348 - clf_loss: 0.3071 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8906 - val_loss: 0.7458 - val_decoder_loss: 0.1364 - val_clf_loss: 0.6034 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8060\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.4417 - decoder_loss: 0.1322 - clf_loss: 0.3080 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8913 - val_loss: 0.7361 - val_decoder_loss: 0.1321 - val_clf_loss: 0.6006 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8073\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4311 - decoder_loss: 0.1308 - clf_loss: 0.2988 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8929 - val_loss: 0.7966 - val_decoder_loss: 0.1311 - val_clf_loss: 0.6611 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7798\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.4235 - decoder_loss: 0.1292 - clf_loss: 0.2928 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8963 - val_loss: 0.7381 - val_decoder_loss: 0.1329 - val_clf_loss: 0.5964 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8086\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 122us/sample - loss: 0.4168 - decoder_loss: 0.1283 - clf_loss: 0.2869 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8975 - val_loss: 0.7528 - val_decoder_loss: 0.1324 - val_clf_loss: 0.6234 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8014\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4075 - decoder_loss: 0.1267 - clf_loss: 0.2793 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9006 - val_loss: 0.7313 - val_decoder_loss: 0.1287 - val_clf_loss: 0.5973 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8094\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.4021 - decoder_loss: 0.1255 - clf_loss: 0.2752 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9014 - val_loss: 0.7775 - val_decoder_loss: 0.1274 - val_clf_loss: 0.6513 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7859\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.3964 - decoder_loss: 0.1241 - clf_loss: 0.2709 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9026 - val_loss: 0.7338 - val_decoder_loss: 0.1259 - val_clf_loss: 0.6078 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8105\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3919 - decoder_loss: 0.1229 - clf_loss: 0.2676 - decoder_accuracy: 0.0075 - clf_accuracy: 0.9045 - val_loss: 0.7607 - val_decoder_loss: 0.1280 - val_clf_loss: 0.6320 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7917\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3830 - decoder_loss: 0.1217 - clf_loss: 0.2598 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9081 - val_loss: 0.7197 - val_decoder_loss: 0.1243 - val_clf_loss: 0.5943 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8114\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3800 - decoder_loss: 0.1213 - clf_loss: 0.2573 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9083 - val_loss: 0.7544 - val_decoder_loss: 0.1228 - val_clf_loss: 0.6342 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8049\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3790 - decoder_loss: 0.1207 - clf_loss: 0.2568 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9069 - val_loss: 0.8185 - val_decoder_loss: 0.1227 - val_clf_loss: 0.6991 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7710\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.3776 - decoder_loss: 0.1197 - clf_loss: 0.2564 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9056 - val_loss: 0.7749 - val_decoder_loss: 0.1239 - val_clf_loss: 0.6520 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7959\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3685 - decoder_loss: 0.1186 - clf_loss: 0.2485 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9115 - val_loss: 0.7568 - val_decoder_loss: 0.1214 - val_clf_loss: 0.6340 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7948\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3661 - decoder_loss: 0.1180 - clf_loss: 0.2466 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9133 - val_loss: 0.7550 - val_decoder_loss: 0.1199 - val_clf_loss: 0.6299 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8011\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.3619 - decoder_loss: 0.1173 - clf_loss: 0.2431 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9115 - val_loss: 0.8045 - val_decoder_loss: 0.1182 - val_clf_loss: 0.6794 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7790\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3598 - decoder_loss: 0.1166 - clf_loss: 0.2418 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9129 - val_loss: 0.7989 - val_decoder_loss: 0.1184 - val_clf_loss: 0.6841 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7813\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3537 - decoder_loss: 0.1159 - clf_loss: 0.2364 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9151 - val_loss: 0.7961 - val_decoder_loss: 0.1186 - val_clf_loss: 0.6810 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8065\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3524 - decoder_loss: 0.1156 - clf_loss: 0.2354 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9163 - val_loss: 0.7775 - val_decoder_loss: 0.1192 - val_clf_loss: 0.6595 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7921\n",
      "Running sub 3, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.255 0.066 1.23  2.958 0.007 0.612 0.352]\n",
      "[4.667 0.035 0.908 3.723 0.007 0.74  0.272]\n",
      "[4.926 0.03  0.817 4.078 0.007 0.758 0.267]\n",
      "[5.454 0.027 0.742 4.685 0.007 0.792 0.266]\n",
      "[5.966 0.025 0.741 5.198 0.007 0.787 0.286]\n",
      "[6.637 0.024 0.718 5.895 0.007 0.798 0.273]\n",
      "[7.187 0.022 0.748 6.415 0.007 0.792 0.299]\n",
      "[7.808 0.021 0.746 7.04  0.007 0.799 0.297]\n",
      "[8.554 0.021 0.764 7.769 0.007 0.791 0.293]\n",
      "[9.336 0.02  0.758 8.557 0.007 0.791 0.287]\n",
      "[9.955 0.019 0.764 9.171 0.007 0.801 0.285]\n",
      "[10.568  0.019  0.759  9.79   0.007  0.81   0.283]\n",
      "[10.99   0.018  0.787 10.184  0.007  0.795  0.296]\n",
      "[11.579  0.018  0.808 10.753  0.007  0.799  0.289]\n",
      "[11.919  0.017  0.82  11.081  0.007  0.79   0.286]\n",
      "[1.565 0.041 0.824 0.7   0.007 0.79  0.277]\n",
      "[1.261 0.041 0.814 0.406 0.007 0.804 0.282]\n",
      "[1.058 0.041 0.817 0.2   0.007 0.798 0.273]\n",
      "[0.957 0.041 0.813 0.103 0.007 0.807 0.273]\n",
      "[0.905 0.041 0.818 0.046 0.007 0.808 0.28 ]\n",
      "[0.89  0.041 0.824 0.026 0.007 0.806 0.28 ]\n",
      "[0.891 0.041 0.833 0.017 0.007 0.799 0.284]\n",
      "[0.854 0.041 0.803 0.011 0.007 0.812 0.269]\n",
      "[0.856 0.041 0.806 0.009 0.007 0.816 0.263]\n",
      "[0.848 0.041 0.8   0.007 0.007 0.82  0.257]\n",
      "[0.847 0.041 0.8   0.007 0.007 0.825 0.26 ]\n",
      "[0.847 0.04  0.801 0.006 0.007 0.827 0.252]\n",
      "[0.863 0.04  0.819 0.004 0.007 0.825 0.259]\n",
      "[0.9   0.04  0.856 0.004 0.007 0.82  0.256]\n",
      "[0.895 0.04  0.851 0.004 0.007 0.811 0.255]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 37.0839 - accuracy: 0.4302 - val_loss: 51.1782 - val_accuracy: 0.2092\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 26.3845 - accuracy: 0.6153 - val_loss: 33.3779 - val_accuracy: 0.5078\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 19.9841 - accuracy: 0.7295 - val_loss: 26.0207 - val_accuracy: 0.6516\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 15.3266 - accuracy: 0.8026 - val_loss: 23.0189 - val_accuracy: 0.7054\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 12.4265 - accuracy: 0.8398 - val_loss: 20.9530 - val_accuracy: 0.7384\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 10.3257 - accuracy: 0.8683 - val_loss: 19.8212 - val_accuracy: 0.7395\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 9.0019 - accuracy: 0.8803 - val_loss: 18.8640 - val_accuracy: 0.7624\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 7.8427 - accuracy: 0.8943 - val_loss: 18.4939 - val_accuracy: 0.7973\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 7.3442 - accuracy: 0.9020 - val_loss: 19.5764 - val_accuracy: 0.7679\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 6.7322 - accuracy: 0.9084 - val_loss: 17.5552 - val_accuracy: 0.7910\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.1975 - accuracy: 0.9146 - val_loss: 18.2949 - val_accuracy: 0.7837\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 5.7034 - accuracy: 0.9211 - val_loss: 16.1859 - val_accuracy: 0.8159\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 5.5309 - accuracy: 0.9230 - val_loss: 18.2165 - val_accuracy: 0.8143\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 5.1815 - accuracy: 0.9291 - val_loss: 19.0369 - val_accuracy: 0.7981\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.8995 - accuracy: 0.9319 - val_loss: 17.2492 - val_accuracy: 0.8033\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.8060 - accuracy: 0.9315 - val_loss: 17.8768 - val_accuracy: 0.8294\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 4.5920 - accuracy: 0.9356 - val_loss: 18.4445 - val_accuracy: 0.8284\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.2998 - accuracy: 0.9400 - val_loss: 19.5213 - val_accuracy: 0.7984\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.2516 - accuracy: 0.9410 - val_loss: 17.0192 - val_accuracy: 0.8287\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 4.1248 - accuracy: 0.9433 - val_loss: 18.4122 - val_accuracy: 0.8202\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.8293 - accuracy: 0.9451 - val_loss: 19.9112 - val_accuracy: 0.7892\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 3.8797 - accuracy: 0.9464 - val_loss: 18.7359 - val_accuracy: 0.8243\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 3.6824 - accuracy: 0.9476 - val_loss: 20.2681 - val_accuracy: 0.7975\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.7395 - accuracy: 0.9463 - val_loss: 20.8914 - val_accuracy: 0.7992\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.6286 - accuracy: 0.9483 - val_loss: 18.7900 - val_accuracy: 0.8124\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.4691 - accuracy: 0.9515 - val_loss: 22.8142 - val_accuracy: 0.7840\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 3.4080 - accuracy: 0.9528 - val_loss: 19.0003 - val_accuracy: 0.8221\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.3717 - accuracy: 0.9526 - val_loss: 20.9938 - val_accuracy: 0.8019\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.3302 - accuracy: 0.9536 - val_loss: 21.2464 - val_accuracy: 0.8265\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 3.2932 - accuracy: 0.9538 - val_loss: 17.8489 - val_accuracy: 0.8473\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 1.7736 - accuracy: 0.3546 - val_loss: 1.7612 - val_accuracy: 0.3390\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.3984 - accuracy: 0.4746 - val_loss: 1.4638 - val_accuracy: 0.4778\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.2007 - accuracy: 0.5403 - val_loss: 1.3291 - val_accuracy: 0.5562\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.0412 - accuracy: 0.6139 - val_loss: 1.2238 - val_accuracy: 0.5976\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.9186 - accuracy: 0.6659 - val_loss: 1.1303 - val_accuracy: 0.6197\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8410 - accuracy: 0.6963 - val_loss: 1.0996 - val_accuracy: 0.6365\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7761 - accuracy: 0.7188 - val_loss: 1.0775 - val_accuracy: 0.6468\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.7373 - accuracy: 0.7328 - val_loss: 1.0961 - val_accuracy: 0.6367\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.7033 - accuracy: 0.7471 - val_loss: 1.1458 - val_accuracy: 0.6271\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.6685 - accuracy: 0.7614 - val_loss: 1.0853 - val_accuracy: 0.6463\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.6414 - accuracy: 0.7676 - val_loss: 1.0707 - val_accuracy: 0.6710\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6236 - accuracy: 0.7778 - val_loss: 1.0409 - val_accuracy: 0.6757\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.6056 - accuracy: 0.7814 - val_loss: 1.0298 - val_accuracy: 0.6824\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.5962 - accuracy: 0.7854 - val_loss: 1.0833 - val_accuracy: 0.6773\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.5802 - accuracy: 0.7890 - val_loss: 1.0387 - val_accuracy: 0.6873\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5650 - accuracy: 0.7971 - val_loss: 1.0607 - val_accuracy: 0.6792\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5581 - accuracy: 0.8004 - val_loss: 1.0242 - val_accuracy: 0.6881\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5557 - accuracy: 0.8024 - val_loss: 1.0530 - val_accuracy: 0.6838\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5304 - accuracy: 0.8094 - val_loss: 1.0142 - val_accuracy: 0.7060\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5322 - accuracy: 0.8094 - val_loss: 1.0114 - val_accuracy: 0.6959\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5243 - accuracy: 0.8114 - val_loss: 1.0285 - val_accuracy: 0.6887\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5148 - accuracy: 0.8152 - val_loss: 1.0570 - val_accuracy: 0.6943\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5018 - accuracy: 0.8217 - val_loss: 1.0435 - val_accuracy: 0.6871\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5029 - accuracy: 0.8194 - val_loss: 1.0323 - val_accuracy: 0.6778\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5024 - accuracy: 0.8183 - val_loss: 0.9831 - val_accuracy: 0.7163\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.4935 - accuracy: 0.8239 - val_loss: 1.0513 - val_accuracy: 0.6844\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4865 - accuracy: 0.8247 - val_loss: 1.0103 - val_accuracy: 0.7048\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4913 - accuracy: 0.8235 - val_loss: 1.0721 - val_accuracy: 0.6821\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.4881 - accuracy: 0.8276 - val_loss: 1.0356 - val_accuracy: 0.7011\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.4785 - accuracy: 0.8280 - val_loss: 1.0342 - val_accuracy: 0.7025\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 85us/sample - loss: 1.2228 - accuracy: 0.5909 - val_loss: 1.8824 - val_accuracy: 0.3078\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.7396 - accuracy: 0.8103 - val_loss: 0.9516 - val_accuracy: 0.7040\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.4920 - accuracy: 0.8682 - val_loss: 0.7413 - val_accuracy: 0.7711\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.3794 - accuracy: 0.8892 - val_loss: 0.6433 - val_accuracy: 0.8068\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3069 - accuracy: 0.9077 - val_loss: 0.5746 - val_accuracy: 0.8306\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.2739 - accuracy: 0.9117 - val_loss: 0.5773 - val_accuracy: 0.8357\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.2480 - accuracy: 0.9192 - val_loss: 0.5986 - val_accuracy: 0.8292\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2217 - accuracy: 0.9265 - val_loss: 0.5403 - val_accuracy: 0.8437\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2038 - accuracy: 0.9319 - val_loss: 0.5389 - val_accuracy: 0.8462\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1965 - accuracy: 0.9337 - val_loss: 0.6178 - val_accuracy: 0.8044\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1830 - accuracy: 0.9383 - val_loss: 0.7051 - val_accuracy: 0.8117\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1748 - accuracy: 0.9407 - val_loss: 0.7120 - val_accuracy: 0.8057\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1708 - accuracy: 0.9417 - val_loss: 0.6356 - val_accuracy: 0.8381\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1646 - accuracy: 0.9429 - val_loss: 0.6121 - val_accuracy: 0.8451\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1584 - accuracy: 0.9451 - val_loss: 0.6582 - val_accuracy: 0.8349\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.1582 - accuracy: 0.9449 - val_loss: 0.8045 - val_accuracy: 0.8079\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1466 - accuracy: 0.9479 - val_loss: 0.5884 - val_accuracy: 0.8475\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1397 - accuracy: 0.9502 - val_loss: 0.5869 - val_accuracy: 0.8449\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1391 - accuracy: 0.9517 - val_loss: 0.6770 - val_accuracy: 0.8425\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1287 - accuracy: 0.9538 - val_loss: 0.6643 - val_accuracy: 0.8411\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.1319 - accuracy: 0.9536 - val_loss: 0.5962 - val_accuracy: 0.8435\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.1245 - accuracy: 0.9567 - val_loss: 0.6522 - val_accuracy: 0.8330\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1275 - accuracy: 0.9546 - val_loss: 0.6477 - val_accuracy: 0.8435\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1194 - accuracy: 0.9580 - val_loss: 0.7134 - val_accuracy: 0.8419\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1186 - accuracy: 0.9580 - val_loss: 0.7858 - val_accuracy: 0.8337\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1151 - accuracy: 0.9598 - val_loss: 0.7251 - val_accuracy: 0.8367\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1104 - accuracy: 0.9611 - val_loss: 0.7417 - val_accuracy: 0.8070\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1105 - accuracy: 0.9611 - val_loss: 0.6910 - val_accuracy: 0.8462\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1099 - accuracy: 0.9619 - val_loss: 0.7458 - val_accuracy: 0.8289\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1096 - accuracy: 0.9616 - val_loss: 0.7015 - val_accuracy: 0.8392\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 182us/sample - loss: 1.2613 - decoder_loss: 0.2265 - clf_loss: 1.0332 - decoder_accuracy: 0.0060 - clf_accuracy: 0.6925 - val_loss: 1.3437 - val_decoder_loss: 0.1623 - val_clf_loss: 1.1799 - val_decoder_accuracy: 0.0074 - val_clf_accuracy: 0.6292\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.6647 - decoder_loss: 0.1694 - clf_loss: 0.4938 - decoder_accuracy: 0.0063 - clf_accuracy: 0.8641 - val_loss: 0.9791 - val_decoder_loss: 0.1552 - val_clf_loss: 0.8241 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7117\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.5181 - decoder_loss: 0.1639 - clf_loss: 0.3526 - decoder_accuracy: 0.0066 - clf_accuracy: 0.8950 - val_loss: 0.9700 - val_decoder_loss: 0.1606 - val_clf_loss: 0.8039 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7349\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4524 - decoder_loss: 0.1570 - clf_loss: 0.2939 - decoder_accuracy: 0.0067 - clf_accuracy: 0.9062 - val_loss: 0.8943 - val_decoder_loss: 0.1450 - val_clf_loss: 0.7452 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7571\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4087 - decoder_loss: 0.1504 - clf_loss: 0.2567 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9168 - val_loss: 0.8540 - val_decoder_loss: 0.1396 - val_clf_loss: 0.7083 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7843\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3778 - decoder_loss: 0.1459 - clf_loss: 0.2303 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9246 - val_loss: 0.7786 - val_decoder_loss: 0.1328 - val_clf_loss: 0.6450 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8022\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3542 - decoder_loss: 0.1416 - clf_loss: 0.2111 - decoder_accuracy: 0.0069 - clf_accuracy: 0.9312 - val_loss: 0.8745 - val_decoder_loss: 0.1492 - val_clf_loss: 0.7221 - val_decoder_accuracy: 0.0070 - val_clf_accuracy: 0.8071\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3376 - decoder_loss: 0.1384 - clf_loss: 0.1976 - decoder_accuracy: 0.0069 - clf_accuracy: 0.9348 - val_loss: 0.7916 - val_decoder_loss: 0.1304 - val_clf_loss: 0.6605 - val_decoder_accuracy: 0.0070 - val_clf_accuracy: 0.8060\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3167 - decoder_loss: 0.1344 - clf_loss: 0.1807 - decoder_accuracy: 0.0069 - clf_accuracy: 0.9420 - val_loss: 0.7430 - val_decoder_loss: 0.1277 - val_clf_loss: 0.6077 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8340\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.3099 - decoder_loss: 0.1318 - clf_loss: 0.1765 - decoder_accuracy: 0.0069 - clf_accuracy: 0.9397 - val_loss: 0.7126 - val_decoder_loss: 0.1246 - val_clf_loss: 0.5908 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8316\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3051 - decoder_loss: 0.1286 - clf_loss: 0.1750 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9403 - val_loss: 0.7936 - val_decoder_loss: 0.1291 - val_clf_loss: 0.6662 - val_decoder_accuracy: 0.0070 - val_clf_accuracy: 0.8178\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2882 - decoder_loss: 0.1257 - clf_loss: 0.1609 - decoder_accuracy: 0.0069 - clf_accuracy: 0.9446 - val_loss: 0.7799 - val_decoder_loss: 0.1204 - val_clf_loss: 0.6542 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7987\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2840 - decoder_loss: 0.1239 - clf_loss: 0.1586 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9459 - val_loss: 0.9405 - val_decoder_loss: 0.1317 - val_clf_loss: 0.8051 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7797\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2774 - decoder_loss: 0.1217 - clf_loss: 0.1542 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9471 - val_loss: 0.7051 - val_decoder_loss: 0.1168 - val_clf_loss: 0.5818 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8311\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.2666 - decoder_loss: 0.1198 - clf_loss: 0.1452 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9501 - val_loss: 0.7926 - val_decoder_loss: 0.1174 - val_clf_loss: 0.6704 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8016\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.2618 - decoder_loss: 0.1181 - clf_loss: 0.1421 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9521 - val_loss: 0.8144 - val_decoder_loss: 0.1181 - val_clf_loss: 0.6907 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8154\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2558 - decoder_loss: 0.1164 - clf_loss: 0.1378 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9520 - val_loss: 0.8006 - val_decoder_loss: 0.1145 - val_clf_loss: 0.6819 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8049\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2486 - decoder_loss: 0.1151 - clf_loss: 0.1319 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9548 - val_loss: 0.7243 - val_decoder_loss: 0.1131 - val_clf_loss: 0.6074 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8321\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 126us/sample - loss: 0.2414 - decoder_loss: 0.1136 - clf_loss: 0.1264 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9578 - val_loss: 0.7307 - val_decoder_loss: 0.1106 - val_clf_loss: 0.6182 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8367\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.2390 - decoder_loss: 0.1124 - clf_loss: 0.1251 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9572 - val_loss: 0.7082 - val_decoder_loss: 0.1110 - val_clf_loss: 0.5938 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8446\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2362 - decoder_loss: 0.1108 - clf_loss: 0.1239 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9571 - val_loss: 0.7817 - val_decoder_loss: 0.1109 - val_clf_loss: 0.6650 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8102\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2301 - decoder_loss: 0.1101 - clf_loss: 0.1185 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9592 - val_loss: 0.7789 - val_decoder_loss: 0.1099 - val_clf_loss: 0.6702 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8203\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2290 - decoder_loss: 0.1089 - clf_loss: 0.1186 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9590 - val_loss: 0.7554 - val_decoder_loss: 0.1094 - val_clf_loss: 0.6463 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8148\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2217 - decoder_loss: 0.1077 - clf_loss: 0.1126 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9601 - val_loss: 0.8362 - val_decoder_loss: 0.1093 - val_clf_loss: 0.7249 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8152\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2216 - decoder_loss: 0.1067 - clf_loss: 0.1134 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9603 - val_loss: 0.7539 - val_decoder_loss: 0.1076 - val_clf_loss: 0.6422 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8181\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.2190 - decoder_loss: 0.1062 - clf_loss: 0.1113 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9608 - val_loss: 0.8469 - val_decoder_loss: 0.1058 - val_clf_loss: 0.7387 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7902\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2126 - decoder_loss: 0.1053 - clf_loss: 0.1058 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9631 - val_loss: 0.9248 - val_decoder_loss: 0.1079 - val_clf_loss: 0.8165 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7837\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2126 - decoder_loss: 0.1042 - clf_loss: 0.1069 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9620 - val_loss: 0.8878 - val_decoder_loss: 0.1048 - val_clf_loss: 0.7752 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8135\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2093 - decoder_loss: 0.1037 - clf_loss: 0.1040 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9643 - val_loss: 0.8242 - val_decoder_loss: 0.1050 - val_clf_loss: 0.7104 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8190\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.2053 - decoder_loss: 0.1030 - clf_loss: 0.1008 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9654 - val_loss: 0.8587 - val_decoder_loss: 0.1052 - val_clf_loss: 0.7542 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8178\n",
      "Running sub 4, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.044 0.06  1.308 2.676 0.007 0.52  0.393]\n",
      "[5.65  0.037 1.05  4.562 0.007 0.645 0.377]\n",
      "[5.504 0.034 0.935 4.534 0.007 0.681 0.365]\n",
      "[5.914 0.031 0.931 4.951 0.007 0.693 0.338]\n",
      "[5.855 0.03  0.939 4.885 0.007 0.706 0.329]\n",
      "[6.095 0.029 0.947 5.119 0.007 0.716 0.286]\n",
      "[6.436 0.027 0.966 5.442 0.007 0.719 0.281]\n",
      "[6.666 0.026 0.987 5.653 0.007 0.716 0.276]\n",
      "[7.377 0.025 1.007 6.345 0.007 0.717 0.277]\n",
      "[7.67  0.025 1.009 6.635 0.007 0.727 0.253]\n",
      "[8.403 0.024 1.068 7.311 0.007 0.718 0.266]\n",
      "[9.051 0.023 1.061 7.967 0.007 0.717 0.265]\n",
      "[9.364 0.023 1.081 8.26  0.007 0.716 0.263]\n",
      "[9.723 0.022 1.117 8.583 0.007 0.72  0.269]\n",
      "[9.971 0.021 1.095 8.855 0.007 0.731 0.25 ]\n",
      "[1.791 0.045 1.097 0.649 0.007 0.72  0.242]\n",
      "[1.428 0.045 1.092 0.291 0.007 0.724 0.247]\n",
      "[1.317 0.045 1.129 0.143 0.007 0.719 0.253]\n",
      "[1.255 0.046 1.147 0.062 0.007 0.729 0.246]\n",
      "[1.206 0.045 1.134 0.027 0.007 0.726 0.246]\n",
      "[1.206 0.045 1.148 0.012 0.007 0.733 0.25 ]\n",
      "[1.19  0.046 1.137 0.006 0.007 0.73  0.238]\n",
      "[1.183 0.045 1.133 0.004 0.007 0.732 0.23 ]\n",
      "[1.211 0.045 1.162 0.004 0.007 0.733 0.236]\n",
      "[1.176 0.045 1.128 0.003 0.007 0.73  0.225]\n",
      "[1.218 0.045 1.17  0.003 0.007 0.727 0.233]\n",
      "[1.202 0.045 1.154 0.003 0.007 0.732 0.222]\n",
      "[1.212 0.045 1.165 0.002 0.007 0.735 0.235]\n",
      "[1.19  0.045 1.144 0.001 0.007 0.736 0.237]\n",
      "[1.196 0.045 1.15  0.001 0.007 0.738 0.234]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 117us/sample - loss: 41.8972 - accuracy: 0.3435 - val_loss: 47.6410 - val_accuracy: 0.2276\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 30.8177 - accuracy: 0.4971 - val_loss: 32.2410 - val_accuracy: 0.5165\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 24.6743 - accuracy: 0.6181 - val_loss: 28.1807 - val_accuracy: 0.6083\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 20.1934 - accuracy: 0.6944 - val_loss: 24.2243 - val_accuracy: 0.6621\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 16.9947 - accuracy: 0.7457 - val_loss: 23.8416 - val_accuracy: 0.6681\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 14.6439 - accuracy: 0.7847 - val_loss: 23.4498 - val_accuracy: 0.7059\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 13.1004 - accuracy: 0.8096 - val_loss: 24.1199 - val_accuracy: 0.7106\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 11.6339 - accuracy: 0.8329 - val_loss: 24.0834 - val_accuracy: 0.7083\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 10.8088 - accuracy: 0.8443 - val_loss: 23.6053 - val_accuracy: 0.7265\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 10.0594 - accuracy: 0.8542 - val_loss: 23.0849 - val_accuracy: 0.7297\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 9.4530 - accuracy: 0.8631 - val_loss: 25.8289 - val_accuracy: 0.7160\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 8.9168 - accuracy: 0.8712 - val_loss: 23.1615 - val_accuracy: 0.7289\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.5845 - accuracy: 0.8755 - val_loss: 24.7467 - val_accuracy: 0.7330\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 8.2876 - accuracy: 0.8823 - val_loss: 23.2871 - val_accuracy: 0.7221\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 7.8753 - accuracy: 0.8875 - val_loss: 25.5779 - val_accuracy: 0.7322\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.7295 - accuracy: 0.8867 - val_loss: 25.4205 - val_accuracy: 0.7324\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.4449 - accuracy: 0.8935 - val_loss: 25.3986 - val_accuracy: 0.7422\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 7.1999 - accuracy: 0.8964 - val_loss: 28.4139 - val_accuracy: 0.7298\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.0919 - accuracy: 0.8977 - val_loss: 25.9777 - val_accuracy: 0.7459\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 6.9324 - accuracy: 0.9014 - val_loss: 26.4138 - val_accuracy: 0.7319\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 6.7148 - accuracy: 0.9025 - val_loss: 27.6525 - val_accuracy: 0.7310\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 6.5299 - accuracy: 0.9071 - val_loss: 27.0193 - val_accuracy: 0.7448\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.3953 - accuracy: 0.9094 - val_loss: 27.5762 - val_accuracy: 0.7387\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.3727 - accuracy: 0.9079 - val_loss: 28.8408 - val_accuracy: 0.7459\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 6.2245 - accuracy: 0.9109 - val_loss: 27.4082 - val_accuracy: 0.7459\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 5.9931 - accuracy: 0.9148 - val_loss: 28.6575 - val_accuracy: 0.7329\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 6.0896 - accuracy: 0.9144 - val_loss: 27.0761 - val_accuracy: 0.7394\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 5.8497 - accuracy: 0.9159 - val_loss: 29.8552 - val_accuracy: 0.7368\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 5.7639 - accuracy: 0.9165 - val_loss: 26.0846 - val_accuracy: 0.7497\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.7121 - accuracy: 0.9183 - val_loss: 26.2997 - val_accuracy: 0.7446\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 1.8612 - accuracy: 0.2617 - val_loss: 1.7819 - val_accuracy: 0.3011\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 1.5192 - accuracy: 0.4355 - val_loss: 1.4263 - val_accuracy: 0.4675\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.2579 - accuracy: 0.5309 - val_loss: 1.2610 - val_accuracy: 0.5216\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.0582 - accuracy: 0.6147 - val_loss: 1.0922 - val_accuracy: 0.6081\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.9268 - accuracy: 0.6611 - val_loss: 1.0609 - val_accuracy: 0.6151\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.8493 - accuracy: 0.6834 - val_loss: 1.0350 - val_accuracy: 0.6462\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.8001 - accuracy: 0.6952 - val_loss: 1.0201 - val_accuracy: 0.6378\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7760 - accuracy: 0.6981 - val_loss: 1.0262 - val_accuracy: 0.6346\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.7524 - accuracy: 0.7068 - val_loss: 1.0266 - val_accuracy: 0.6462\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7344 - accuracy: 0.7116 - val_loss: 1.0173 - val_accuracy: 0.6506\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7226 - accuracy: 0.7158 - val_loss: 1.0035 - val_accuracy: 0.6622\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7113 - accuracy: 0.7194 - val_loss: 1.0068 - val_accuracy: 0.6651\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.6989 - accuracy: 0.7224 - val_loss: 1.0143 - val_accuracy: 0.6762\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.6889 - accuracy: 0.7312 - val_loss: 1.0102 - val_accuracy: 0.6605\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6780 - accuracy: 0.7318 - val_loss: 1.0101 - val_accuracy: 0.6843\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6721 - accuracy: 0.7326 - val_loss: 1.0065 - val_accuracy: 0.6608\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6602 - accuracy: 0.7384 - val_loss: 1.0250 - val_accuracy: 0.6524\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.6550 - accuracy: 0.7428 - val_loss: 0.9969 - val_accuracy: 0.6900\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6425 - accuracy: 0.7486 - val_loss: 0.9759 - val_accuracy: 0.6862\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6291 - accuracy: 0.7527 - val_loss: 1.0099 - val_accuracy: 0.6849\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.6264 - accuracy: 0.7570 - val_loss: 1.0157 - val_accuracy: 0.6902\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.6239 - accuracy: 0.7578 - val_loss: 0.9891 - val_accuracy: 0.6962\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6084 - accuracy: 0.7619 - val_loss: 0.9758 - val_accuracy: 0.6952\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.6027 - accuracy: 0.7662 - val_loss: 1.0054 - val_accuracy: 0.6806\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5979 - accuracy: 0.7686 - val_loss: 0.9937 - val_accuracy: 0.6889\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5888 - accuracy: 0.7729 - val_loss: 0.9865 - val_accuracy: 0.6903\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5872 - accuracy: 0.7706 - val_loss: 0.9894 - val_accuracy: 0.6867\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5841 - accuracy: 0.7756 - val_loss: 1.0248 - val_accuracy: 0.6946\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5761 - accuracy: 0.7761 - val_loss: 1.0116 - val_accuracy: 0.6889\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5665 - accuracy: 0.7811 - val_loss: 0.9989 - val_accuracy: 0.6894\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 1.3445 - accuracy: 0.5408 - val_loss: 1.6406 - val_accuracy: 0.3175\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.8893 - accuracy: 0.7202 - val_loss: 0.8959 - val_accuracy: 0.6775\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.6562 - accuracy: 0.7900 - val_loss: 0.8034 - val_accuracy: 0.7013\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.5293 - accuracy: 0.8258 - val_loss: 0.8131 - val_accuracy: 0.7148\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.4561 - accuracy: 0.8459 - val_loss: 0.8286 - val_accuracy: 0.7271\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4131 - accuracy: 0.8582 - val_loss: 0.9320 - val_accuracy: 0.7233\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3782 - accuracy: 0.8686 - val_loss: 0.9340 - val_accuracy: 0.7378\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3546 - accuracy: 0.8777 - val_loss: 0.8715 - val_accuracy: 0.7390\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3326 - accuracy: 0.8840 - val_loss: 1.1959 - val_accuracy: 0.7141\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3155 - accuracy: 0.8900 - val_loss: 1.0405 - val_accuracy: 0.7259\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3073 - accuracy: 0.8924 - val_loss: 1.0541 - val_accuracy: 0.7251\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2940 - accuracy: 0.8961 - val_loss: 1.1582 - val_accuracy: 0.7235\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2859 - accuracy: 0.8992 - val_loss: 1.1920 - val_accuracy: 0.7305\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2719 - accuracy: 0.9026 - val_loss: 1.1501 - val_accuracy: 0.7317\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2668 - accuracy: 0.9064 - val_loss: 1.1920 - val_accuracy: 0.7205\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2598 - accuracy: 0.9065 - val_loss: 1.1310 - val_accuracy: 0.7341\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.2508 - accuracy: 0.9114 - val_loss: 1.1751 - val_accuracy: 0.7370\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.2466 - accuracy: 0.9114 - val_loss: 1.1957 - val_accuracy: 0.7365\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2365 - accuracy: 0.9143 - val_loss: 1.2538 - val_accuracy: 0.7359\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2350 - accuracy: 0.9158 - val_loss: 1.3706 - val_accuracy: 0.7227\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2272 - accuracy: 0.9200 - val_loss: 1.2713 - val_accuracy: 0.7408\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2226 - accuracy: 0.9219 - val_loss: 1.2468 - val_accuracy: 0.7311\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2189 - accuracy: 0.9218 - val_loss: 1.2402 - val_accuracy: 0.7390\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2127 - accuracy: 0.9242 - val_loss: 1.3330 - val_accuracy: 0.7324\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2129 - accuracy: 0.9232 - val_loss: 1.3579 - val_accuracy: 0.7352\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2079 - accuracy: 0.9265 - val_loss: 1.4262 - val_accuracy: 0.7254\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2068 - accuracy: 0.9240 - val_loss: 1.4053 - val_accuracy: 0.7344\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2065 - accuracy: 0.9252 - val_loss: 1.3296 - val_accuracy: 0.7497\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1964 - accuracy: 0.9298 - val_loss: 1.4410 - val_accuracy: 0.7425\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1932 - accuracy: 0.9325 - val_loss: 1.5750 - val_accuracy: 0.7387\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 186us/sample - loss: 1.3921 - decoder_loss: 0.2311 - clf_loss: 1.1591 - decoder_accuracy: 0.0063 - clf_accuracy: 0.5856 - val_loss: 1.3617 - val_decoder_loss: 0.1602 - val_clf_loss: 1.1993 - val_decoder_accuracy: 0.0070 - val_clf_accuracy: 0.5600\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.8428 - decoder_loss: 0.1696 - clf_loss: 0.6713 - decoder_accuracy: 0.0066 - clf_accuracy: 0.7652 - val_loss: 1.0199 - val_decoder_loss: 0.1709 - val_clf_loss: 0.8508 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7060\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.6891 - decoder_loss: 0.1647 - clf_loss: 0.5226 - decoder_accuracy: 0.0069 - clf_accuracy: 0.8161 - val_loss: 1.0152 - val_decoder_loss: 0.1654 - val_clf_loss: 0.8506 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.7025\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.6079 - decoder_loss: 0.1638 - clf_loss: 0.4423 - decoder_accuracy: 0.0070 - clf_accuracy: 0.8434 - val_loss: 1.0327 - val_decoder_loss: 0.1636 - val_clf_loss: 0.8698 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7200\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.5636 - decoder_loss: 0.1623 - clf_loss: 0.3996 - decoder_accuracy: 0.0070 - clf_accuracy: 0.8576 - val_loss: 1.0812 - val_decoder_loss: 0.1591 - val_clf_loss: 0.9249 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7179\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.5293 - decoder_loss: 0.1609 - clf_loss: 0.3666 - decoder_accuracy: 0.0070 - clf_accuracy: 0.8691 - val_loss: 1.1607 - val_decoder_loss: 0.1654 - val_clf_loss: 0.9911 - val_decoder_accuracy: 0.0065 - val_clf_accuracy: 0.7216\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.5030 - decoder_loss: 0.1581 - clf_loss: 0.3432 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8765 - val_loss: 1.0741 - val_decoder_loss: 0.1560 - val_clf_loss: 0.9128 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7225\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4810 - decoder_loss: 0.1559 - clf_loss: 0.3233 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8860 - val_loss: 1.1629 - val_decoder_loss: 0.1662 - val_clf_loss: 0.9913 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7306\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4662 - decoder_loss: 0.1535 - clf_loss: 0.3109 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8870 - val_loss: 1.0980 - val_decoder_loss: 0.1487 - val_clf_loss: 0.9409 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7306\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4487 - decoder_loss: 0.1511 - clf_loss: 0.2958 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8956 - val_loss: 1.2671 - val_decoder_loss: 0.1546 - val_clf_loss: 1.1149 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7233\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.4423 - decoder_loss: 0.1497 - clf_loss: 0.2909 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8968 - val_loss: 1.1082 - val_decoder_loss: 0.1486 - val_clf_loss: 0.9543 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7383\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4288 - decoder_loss: 0.1480 - clf_loss: 0.2790 - decoder_accuracy: 0.0070 - clf_accuracy: 0.9005 - val_loss: 1.0661 - val_decoder_loss: 0.1468 - val_clf_loss: 0.9181 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7565\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4187 - decoder_loss: 0.1450 - clf_loss: 0.2719 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9021 - val_loss: 1.0630 - val_decoder_loss: 0.1419 - val_clf_loss: 0.9194 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7394\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4140 - decoder_loss: 0.1434 - clf_loss: 0.2689 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9038 - val_loss: 1.1868 - val_decoder_loss: 0.1470 - val_clf_loss: 1.0348 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7332\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4017 - decoder_loss: 0.1412 - clf_loss: 0.2588 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9062 - val_loss: 1.0904 - val_decoder_loss: 0.1416 - val_clf_loss: 0.9455 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7478\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4017 - decoder_loss: 0.1400 - clf_loss: 0.2600 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9045 - val_loss: 1.1556 - val_decoder_loss: 0.1436 - val_clf_loss: 1.0029 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7435\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3890 - decoder_loss: 0.1379 - clf_loss: 0.2493 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9099 - val_loss: 1.1157 - val_decoder_loss: 0.1399 - val_clf_loss: 0.9708 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7498\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3847 - decoder_loss: 0.1359 - clf_loss: 0.2471 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9104 - val_loss: 1.1676 - val_decoder_loss: 0.1377 - val_clf_loss: 1.0339 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7444\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3769 - decoder_loss: 0.1347 - clf_loss: 0.2405 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9137 - val_loss: 1.2060 - val_decoder_loss: 0.1332 - val_clf_loss: 1.0727 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7379\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3714 - decoder_loss: 0.1333 - clf_loss: 0.2365 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9152 - val_loss: 1.1229 - val_decoder_loss: 0.1327 - val_clf_loss: 0.9839 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7554\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3634 - decoder_loss: 0.1310 - clf_loss: 0.2308 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9158 - val_loss: 1.2042 - val_decoder_loss: 0.1325 - val_clf_loss: 1.0734 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7524\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3590 - decoder_loss: 0.1305 - clf_loss: 0.2269 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9196 - val_loss: 1.1762 - val_decoder_loss: 0.1326 - val_clf_loss: 1.0371 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7537\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.3514 - decoder_loss: 0.1291 - clf_loss: 0.2205 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9203 - val_loss: 1.2221 - val_decoder_loss: 0.1304 - val_clf_loss: 1.0860 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7427\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3538 - decoder_loss: 0.1279 - clf_loss: 0.2243 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9186 - val_loss: 1.1731 - val_decoder_loss: 0.1245 - val_clf_loss: 1.0398 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7417\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3458 - decoder_loss: 0.1259 - clf_loss: 0.2183 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9209 - val_loss: 1.2089 - val_decoder_loss: 0.1278 - val_clf_loss: 1.0744 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7546\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3425 - decoder_loss: 0.1255 - clf_loss: 0.2154 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9212 - val_loss: 1.1354 - val_decoder_loss: 0.1233 - val_clf_loss: 1.0094 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7424\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.3419 - decoder_loss: 0.1241 - clf_loss: 0.2160 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9218 - val_loss: 1.1330 - val_decoder_loss: 0.1235 - val_clf_loss: 1.0104 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7516\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3307 - decoder_loss: 0.1232 - clf_loss: 0.2058 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9264 - val_loss: 1.1881 - val_decoder_loss: 0.1223 - val_clf_loss: 1.0659 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7398\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3309 - decoder_loss: 0.1227 - clf_loss: 0.2066 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9249 - val_loss: 1.2420 - val_decoder_loss: 0.1237 - val_clf_loss: 1.1150 - val_decoder_accuracy: 0.0069 - val_clf_accuracy: 0.7473\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3266 - decoder_loss: 0.1216 - clf_loss: 0.2033 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9256 - val_loss: 1.1243 - val_decoder_loss: 0.1211 - val_clf_loss: 0.9949 - val_decoder_accuracy: 0.0068 - val_clf_accuracy: 0.7571\n",
      "Running sub 5, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.336 0.06  1.497 2.779 0.    0.449 0.354]\n",
      "[4.797 0.035 1.122 3.639 0.    0.614 0.346]\n",
      "[4.708 0.032 0.889 3.786 0.    0.685 0.428]\n",
      "[4.742 0.03  0.793 3.919 0.    0.739 0.357]\n",
      "[5.334 0.027 0.739 4.566 0.    0.744 0.379]\n",
      "[5.465 0.026 0.748 4.69  0.    0.747 0.344]\n",
      "[5.941 0.024 0.755 5.161 0.    0.747 0.338]\n",
      "[6.347 0.024 0.775 5.548 0.    0.747 0.34 ]\n",
      "[6.979 0.022 0.775 6.181 0.    0.753 0.324]\n",
      "[7.463 0.022 0.732 6.709 0.    0.764 0.326]\n",
      "[7.792 0.022 0.764 7.006 0.    0.763 0.314]\n",
      "[8.554 0.021 0.762 7.771 0.    0.761 0.324]\n",
      "[8.991 0.02  0.75  8.22  0.    0.771 0.333]\n",
      "[9.57  0.019 0.752 8.799 0.    0.782 0.318]\n",
      "[9.647 0.019 0.747 8.882 0.    0.774 0.316]\n",
      "[1.321 0.046 0.702 0.573 0.    0.806 0.272]\n",
      "[1.039 0.044 0.703 0.292 0.    0.807 0.278]\n",
      "[0.894 0.045 0.674 0.175 0.    0.812 0.29 ]\n",
      "[0.794 0.044 0.662 0.088 0.    0.818 0.296]\n",
      "[0.77  0.044 0.683 0.044 0.    0.81  0.29 ]\n",
      "[0.75  0.044 0.682 0.024 0.    0.806 0.305]\n",
      "[0.757 0.044 0.697 0.016 0.    0.799 0.305]\n",
      "[0.737 0.044 0.68  0.013 0.    0.809 0.306]\n",
      "[0.772 0.044 0.718 0.011 0.    0.802 0.293]\n",
      "[0.783 0.044 0.73  0.009 0.    0.797 0.295]\n",
      "[0.793 0.044 0.741 0.008 0.    0.793 0.3  ]\n",
      "[0.833 0.044 0.783 0.007 0.    0.782 0.3  ]\n",
      "[0.828 0.044 0.776 0.008 0.    0.785 0.304]\n",
      "[0.823 0.044 0.772 0.007 0.    0.785 0.309]\n",
      "[0.819 0.044 0.769 0.006 0.    0.793 0.297]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 127us/sample - loss: 47.7869 - accuracy: 0.2571 - val_loss: 47.4533 - val_accuracy: 0.2089\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 37.2987 - accuracy: 0.4038 - val_loss: 34.6121 - val_accuracy: 0.4614\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 30.8436 - accuracy: 0.5292 - val_loss: 27.8358 - val_accuracy: 0.5837\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 24.9980 - accuracy: 0.6320 - val_loss: 25.3857 - val_accuracy: 0.6175\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 20.8227 - accuracy: 0.6988 - val_loss: 22.6604 - val_accuracy: 0.6433\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 17.4930 - accuracy: 0.7507 - val_loss: 21.6662 - val_accuracy: 0.6743\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 15.4937 - accuracy: 0.7795 - val_loss: 20.6119 - val_accuracy: 0.6695\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 14.2803 - accuracy: 0.7938 - val_loss: 19.1828 - val_accuracy: 0.7162\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 13.1825 - accuracy: 0.8082 - val_loss: 19.1566 - val_accuracy: 0.7192\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 12.3501 - accuracy: 0.8206 - val_loss: 20.0906 - val_accuracy: 0.7111\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 11.7932 - accuracy: 0.8290 - val_loss: 19.6660 - val_accuracy: 0.7167\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 11.0162 - accuracy: 0.8390 - val_loss: 18.6305 - val_accuracy: 0.7211\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 10.4965 - accuracy: 0.8473 - val_loss: 17.4667 - val_accuracy: 0.7524\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 10.2961 - accuracy: 0.8521 - val_loss: 17.2097 - val_accuracy: 0.7613\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 9.8320 - accuracy: 0.8585 - val_loss: 19.0998 - val_accuracy: 0.7268\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 9.5838 - accuracy: 0.8608 - val_loss: 17.3064 - val_accuracy: 0.7503\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 84us/sample - loss: 9.3064 - accuracy: 0.8656 - val_loss: 19.5696 - val_accuracy: 0.7302\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 9.1020 - accuracy: 0.8693 - val_loss: 17.1898 - val_accuracy: 0.7733\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.7611 - accuracy: 0.8736 - val_loss: 17.0527 - val_accuracy: 0.7595\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 8.6542 - accuracy: 0.8764 - val_loss: 16.0846 - val_accuracy: 0.7716\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 8.2114 - accuracy: 0.8834 - val_loss: 18.6445 - val_accuracy: 0.7530\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 8.1706 - accuracy: 0.8843 - val_loss: 15.4469 - val_accuracy: 0.7924\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 8.1705 - accuracy: 0.8819 - val_loss: 15.8040 - val_accuracy: 0.7884\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 7.9393 - accuracy: 0.8853 - val_loss: 16.5434 - val_accuracy: 0.7729\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.7717 - accuracy: 0.8892 - val_loss: 15.9208 - val_accuracy: 0.7763\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.7150 - accuracy: 0.8902 - val_loss: 15.3498 - val_accuracy: 0.7873\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.5422 - accuracy: 0.8914 - val_loss: 16.6505 - val_accuracy: 0.7770\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 7.3652 - accuracy: 0.8949 - val_loss: 16.9687 - val_accuracy: 0.7744\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.3296 - accuracy: 0.8958 - val_loss: 17.3414 - val_accuracy: 0.7816\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.2422 - accuracy: 0.8946 - val_loss: 15.4212 - val_accuracy: 0.7905\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 1.9698 - accuracy: 0.2125 - val_loss: 1.8786 - val_accuracy: 0.2606\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.7682 - accuracy: 0.3030 - val_loss: 1.6662 - val_accuracy: 0.3546\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.6031 - accuracy: 0.3671 - val_loss: 1.5150 - val_accuracy: 0.4149\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.4723 - accuracy: 0.4270 - val_loss: 1.4018 - val_accuracy: 0.4354\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.3509 - accuracy: 0.4772 - val_loss: 1.3609 - val_accuracy: 0.4522\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.2527 - accuracy: 0.5200 - val_loss: 1.3169 - val_accuracy: 0.4648\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.1838 - accuracy: 0.5440 - val_loss: 1.2854 - val_accuracy: 0.4775\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.1336 - accuracy: 0.5619 - val_loss: 1.2874 - val_accuracy: 0.4817\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.0989 - accuracy: 0.5769 - val_loss: 1.2069 - val_accuracy: 0.5119\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0647 - accuracy: 0.5872 - val_loss: 1.2330 - val_accuracy: 0.5146\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0275 - accuracy: 0.6000 - val_loss: 1.1745 - val_accuracy: 0.5390\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9950 - accuracy: 0.6158 - val_loss: 1.1269 - val_accuracy: 0.5554\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.9656 - accuracy: 0.6265 - val_loss: 1.0971 - val_accuracy: 0.5594\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.9337 - accuracy: 0.6423 - val_loss: 1.0882 - val_accuracy: 0.5713\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.8992 - accuracy: 0.6523 - val_loss: 1.1155 - val_accuracy: 0.5721\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8714 - accuracy: 0.6642 - val_loss: 1.0443 - val_accuracy: 0.5884\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8501 - accuracy: 0.6733 - val_loss: 1.0852 - val_accuracy: 0.5765\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8299 - accuracy: 0.6805 - val_loss: 1.1297 - val_accuracy: 0.5863\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8123 - accuracy: 0.6867 - val_loss: 1.0200 - val_accuracy: 0.6033\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8002 - accuracy: 0.6931 - val_loss: 1.0198 - val_accuracy: 0.6103\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.7887 - accuracy: 0.6973 - val_loss: 1.0118 - val_accuracy: 0.6021\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7769 - accuracy: 0.7036 - val_loss: 1.0253 - val_accuracy: 0.6029\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.7649 - accuracy: 0.7093 - val_loss: 0.9873 - val_accuracy: 0.6157\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7548 - accuracy: 0.7140 - val_loss: 1.0008 - val_accuracy: 0.6189\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7515 - accuracy: 0.7177 - val_loss: 0.9735 - val_accuracy: 0.6214\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7373 - accuracy: 0.7206 - val_loss: 0.9979 - val_accuracy: 0.6137\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.7314 - accuracy: 0.7244 - val_loss: 1.0569 - val_accuracy: 0.5989\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7196 - accuracy: 0.7288 - val_loss: 0.9818 - val_accuracy: 0.6197\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7142 - accuracy: 0.7279 - val_loss: 0.9429 - val_accuracy: 0.6306\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7044 - accuracy: 0.7367 - val_loss: 0.8965 - val_accuracy: 0.6659\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 93us/sample - loss: 1.4858 - accuracy: 0.4471 - val_loss: 1.8260 - val_accuracy: 0.2514\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.0921 - accuracy: 0.6225 - val_loss: 1.1387 - val_accuracy: 0.5352\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.8910 - accuracy: 0.6830 - val_loss: 0.9538 - val_accuracy: 0.6170\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.7643 - accuracy: 0.7309 - val_loss: 0.9309 - val_accuracy: 0.6543\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.6762 - accuracy: 0.7681 - val_loss: 0.9295 - val_accuracy: 0.6549\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.6010 - accuracy: 0.7931 - val_loss: 0.8242 - val_accuracy: 0.6865\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.5458 - accuracy: 0.8077 - val_loss: 0.7706 - val_accuracy: 0.7206\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.5074 - accuracy: 0.8193 - val_loss: 0.8439 - val_accuracy: 0.6884\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.4739 - accuracy: 0.8300 - val_loss: 0.8209 - val_accuracy: 0.7314\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4556 - accuracy: 0.8368 - val_loss: 0.6557 - val_accuracy: 0.7633\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4306 - accuracy: 0.8466 - val_loss: 0.7804 - val_accuracy: 0.7256\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4122 - accuracy: 0.8515 - val_loss: 0.7417 - val_accuracy: 0.7390\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3978 - accuracy: 0.8578 - val_loss: 0.6891 - val_accuracy: 0.7678\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3888 - accuracy: 0.8623 - val_loss: 0.6787 - val_accuracy: 0.7630\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3796 - accuracy: 0.8645 - val_loss: 0.6987 - val_accuracy: 0.7705\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3620 - accuracy: 0.8717 - val_loss: 0.6785 - val_accuracy: 0.7668\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3594 - accuracy: 0.8702 - val_loss: 0.8261 - val_accuracy: 0.7216\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3533 - accuracy: 0.8738 - val_loss: 0.7087 - val_accuracy: 0.7511\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3484 - accuracy: 0.8762 - val_loss: 0.7157 - val_accuracy: 0.7538\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3428 - accuracy: 0.8780 - val_loss: 0.7071 - val_accuracy: 0.7643\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3328 - accuracy: 0.8794 - val_loss: 0.6490 - val_accuracy: 0.7716\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3266 - accuracy: 0.8829 - val_loss: 0.7212 - val_accuracy: 0.7495\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3257 - accuracy: 0.8817 - val_loss: 0.6854 - val_accuracy: 0.7675\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3159 - accuracy: 0.8861 - val_loss: 0.8895 - val_accuracy: 0.7225\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3063 - accuracy: 0.8896 - val_loss: 0.6568 - val_accuracy: 0.7857\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3112 - accuracy: 0.8872 - val_loss: 0.6521 - val_accuracy: 0.7800\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3018 - accuracy: 0.8929 - val_loss: 0.6347 - val_accuracy: 0.7790\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3004 - accuracy: 0.8905 - val_loss: 0.5718 - val_accuracy: 0.8089\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.2958 - accuracy: 0.8962 - val_loss: 0.6419 - val_accuracy: 0.7873\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2927 - accuracy: 0.8949 - val_loss: 0.6763 - val_accuracy: 0.7716\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 197us/sample - loss: 1.5749 - decoder_loss: 0.2364 - clf_loss: 1.3369 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.5149 - val_loss: 1.4106 - val_decoder_loss: 0.1740 - val_clf_loss: 1.2351 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.5616\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 1.0323 - decoder_loss: 0.1731 - clf_loss: 0.8577 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.7032 - val_loss: 0.9525 - val_decoder_loss: 0.1772 - val_clf_loss: 0.7761 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7308\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.8146 - decoder_loss: 0.1657 - clf_loss: 0.6474 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.7736 - val_loss: 0.8426 - val_decoder_loss: 0.1773 - val_clf_loss: 0.6619 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7503\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.7053 - decoder_loss: 0.1618 - clf_loss: 0.5419 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8088 - val_loss: 0.8738 - val_decoder_loss: 0.1677 - val_clf_loss: 0.7018 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7356\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.6377 - decoder_loss: 0.1578 - clf_loss: 0.4784 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8311 - val_loss: 0.7716 - val_decoder_loss: 0.1638 - val_clf_loss: 0.6058 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7662\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.5921 - decoder_loss: 0.1539 - clf_loss: 0.4367 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8434 - val_loss: 0.7416 - val_decoder_loss: 0.1506 - val_clf_loss: 0.5878 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7846\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.5592 - decoder_loss: 0.1503 - clf_loss: 0.4073 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8559 - val_loss: 0.6759 - val_decoder_loss: 0.1495 - val_clf_loss: 0.5275 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8098\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.5312 - decoder_loss: 0.1467 - clf_loss: 0.3830 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.8636 - val_loss: 0.7259 - val_decoder_loss: 0.1506 - val_clf_loss: 0.5739 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7789\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.5178 - decoder_loss: 0.1433 - clf_loss: 0.3728 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.8667 - val_loss: 0.6880 - val_decoder_loss: 0.1443 - val_clf_loss: 0.5497 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8043\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.4918 - decoder_loss: 0.1406 - clf_loss: 0.3497 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8759 - val_loss: 0.6964 - val_decoder_loss: 0.1411 - val_clf_loss: 0.5541 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7987\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4858 - decoder_loss: 0.1382 - clf_loss: 0.3459 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8769 - val_loss: 0.7343 - val_decoder_loss: 0.1391 - val_clf_loss: 0.5925 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7813\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4694 - decoder_loss: 0.1354 - clf_loss: 0.3324 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8825 - val_loss: 0.6625 - val_decoder_loss: 0.1325 - val_clf_loss: 0.5278 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8095\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4598 - decoder_loss: 0.1329 - clf_loss: 0.3254 - decoder_accuracy: 2.8108e-05 - clf_accuracy: 0.8844 - val_loss: 0.6659 - val_decoder_loss: 0.1311 - val_clf_loss: 0.5343 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8075\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 100us/sample - loss: 0.4496 - decoder_loss: 0.1312 - clf_loss: 0.3169 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.8885 - val_loss: 0.7060 - val_decoder_loss: 0.1309 - val_clf_loss: 0.5786 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7946\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4431 - decoder_loss: 0.1295 - clf_loss: 0.3120 - decoder_accuracy: 3.4722e-05 - clf_accuracy: 0.8882 - val_loss: 0.6553 - val_decoder_loss: 0.1275 - val_clf_loss: 0.5266 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8176\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4370 - decoder_loss: 0.1280 - clf_loss: 0.3075 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8881 - val_loss: 0.6974 - val_decoder_loss: 0.1271 - val_clf_loss: 0.5707 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7970\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4242 - decoder_loss: 0.1269 - clf_loss: 0.2958 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8938 - val_loss: 0.6664 - val_decoder_loss: 0.1267 - val_clf_loss: 0.5332 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8111\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4204 - decoder_loss: 0.1254 - clf_loss: 0.2934 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8948 - val_loss: 0.6900 - val_decoder_loss: 0.1223 - val_clf_loss: 0.5680 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7992\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4153 - decoder_loss: 0.1246 - clf_loss: 0.2891 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8946 - val_loss: 0.7743 - val_decoder_loss: 0.1243 - val_clf_loss: 0.6430 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7725\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4085 - decoder_loss: 0.1235 - clf_loss: 0.2834 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.8987 - val_loss: 0.7543 - val_decoder_loss: 0.1213 - val_clf_loss: 0.6290 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7811\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.4051 - decoder_loss: 0.1223 - clf_loss: 0.2813 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.8981 - val_loss: 0.6895 - val_decoder_loss: 0.1196 - val_clf_loss: 0.5660 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8079\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3994 - decoder_loss: 0.1217 - clf_loss: 0.2761 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.9005 - val_loss: 0.7018 - val_decoder_loss: 0.1198 - val_clf_loss: 0.5814 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8117\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3963 - decoder_loss: 0.1208 - clf_loss: 0.2740 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.8997 - val_loss: 0.6992 - val_decoder_loss: 0.1189 - val_clf_loss: 0.5785 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7975\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3898 - decoder_loss: 0.1195 - clf_loss: 0.2687 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.9050 - val_loss: 0.7356 - val_decoder_loss: 0.1183 - val_clf_loss: 0.6165 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7913\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3818 - decoder_loss: 0.1188 - clf_loss: 0.2615 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.9068 - val_loss: 0.6637 - val_decoder_loss: 0.1167 - val_clf_loss: 0.5454 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8067\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3810 - decoder_loss: 0.1181 - clf_loss: 0.2613 - decoder_accuracy: 2.8108e-05 - clf_accuracy: 0.9035 - val_loss: 0.7256 - val_decoder_loss: 0.1170 - val_clf_loss: 0.6053 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7857\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3770 - decoder_loss: 0.1175 - clf_loss: 0.2580 - decoder_accuracy: 3.3069e-05 - clf_accuracy: 0.9070 - val_loss: 0.6743 - val_decoder_loss: 0.1148 - val_clf_loss: 0.5571 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8130\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3762 - decoder_loss: 0.1168 - clf_loss: 0.2579 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.9069 - val_loss: 0.6438 - val_decoder_loss: 0.1136 - val_clf_loss: 0.5270 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8151\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.3683 - decoder_loss: 0.1163 - clf_loss: 0.2504 - decoder_accuracy: 2.9762e-05 - clf_accuracy: 0.9116 - val_loss: 0.6876 - val_decoder_loss: 0.1168 - val_clf_loss: 0.5721 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.8108\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3679 - decoder_loss: 0.1158 - clf_loss: 0.2505 - decoder_accuracy: 3.1415e-05 - clf_accuracy: 0.9095 - val_loss: 0.7214 - val_decoder_loss: 0.1158 - val_clf_loss: 0.6073 - val_decoder_accuracy: 0.0000e+00 - val_clf_accuracy: 0.7932\n",
      "Running sub 6, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.216 0.055 1.12  3.04  0.008 0.613 0.161]\n",
      "[5.818 0.04  0.781 4.996 0.008 0.73  0.154]\n",
      "[6.901 0.034 0.627 6.239 0.008 0.778 0.158]\n",
      "[6.44  0.031 0.554 5.854 0.008 0.803 0.137]\n",
      "[6.861 0.029 0.523 6.309 0.008 0.816 0.161]\n",
      "[6.915 0.027 0.469 6.418 0.008 0.834 0.167]\n",
      "[7.668 0.026 0.478 7.162 0.008 0.833 0.181]\n",
      "[7.967 0.025 0.464 7.477 0.008 0.839 0.163]\n",
      "[8.492 0.023 0.499 7.968 0.008 0.838 0.155]\n",
      "[8.97  0.023 0.538 8.408 0.008 0.822 0.157]\n",
      "[9.641 0.022 0.557 9.063 0.008 0.821 0.16 ]\n",
      "[10.201  0.021  0.576  9.603  0.008  0.82   0.166]\n",
      "[10.87   0.02   0.539 10.311  0.008  0.828  0.155]\n",
      "[11.193  0.02   0.546 10.627  0.008  0.832  0.155]\n",
      "[11.197  0.019  0.573 10.604  0.008  0.825  0.165]\n",
      "[1.301 0.047 0.499 0.754 0.008 0.833 0.127]\n",
      "[0.957 0.047 0.536 0.374 0.008 0.833 0.159]\n",
      "[0.769 0.047 0.538 0.184 0.008 0.837 0.164]\n",
      "[0.69  0.047 0.556 0.087 0.008 0.834 0.169]\n",
      "[0.655 0.047 0.571 0.037 0.008 0.833 0.172]\n",
      "[0.668 0.047 0.606 0.015 0.008 0.831 0.177]\n",
      "[0.655 0.047 0.601 0.007 0.008 0.83  0.171]\n",
      "[0.679 0.047 0.629 0.003 0.008 0.823 0.171]\n",
      "[0.678 0.047 0.629 0.002 0.008 0.83  0.174]\n",
      "[0.697 0.047 0.648 0.002 0.008 0.825 0.165]\n",
      "[0.704 0.047 0.656 0.001 0.008 0.829 0.176]\n",
      "[0.713 0.047 0.665 0.001 0.008 0.83  0.162]\n",
      "[0.682 0.047 0.634 0.001 0.008 0.836 0.164]\n",
      "[0.744 0.047 0.696 0.001 0.008 0.824 0.152]\n",
      "[0.7   0.047 0.652 0.001 0.008 0.833 0.168]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 43.6918 - accuracy: 0.3277 - val_loss: 50.7817 - val_accuracy: 0.2162\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 30.4365 - accuracy: 0.5432 - val_loss: 28.1095 - val_accuracy: 0.5671\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 22.5693 - accuracy: 0.6823 - val_loss: 22.1488 - val_accuracy: 0.6692\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 17.0176 - accuracy: 0.7713 - val_loss: 20.4743 - val_accuracy: 0.6932\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 13.4853 - accuracy: 0.8216 - val_loss: 17.0472 - val_accuracy: 0.7654\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 11.2087 - accuracy: 0.8491 - val_loss: 16.7045 - val_accuracy: 0.7414\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 9.8147 - accuracy: 0.8670 - val_loss: 15.2005 - val_accuracy: 0.7806\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 8.6410 - accuracy: 0.8801 - val_loss: 13.4362 - val_accuracy: 0.8138\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.9105 - accuracy: 0.8900 - val_loss: 12.3554 - val_accuracy: 0.8252\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 7.3091 - accuracy: 0.8977 - val_loss: 11.9851 - val_accuracy: 0.8352\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 6.6423 - accuracy: 0.9069 - val_loss: 12.3019 - val_accuracy: 0.8370\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 6.3031 - accuracy: 0.9130 - val_loss: 12.0308 - val_accuracy: 0.8316\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.9498 - accuracy: 0.9154 - val_loss: 12.4536 - val_accuracy: 0.8351\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.6601 - accuracy: 0.9209 - val_loss: 11.9014 - val_accuracy: 0.8465\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.4619 - accuracy: 0.9220 - val_loss: 12.4105 - val_accuracy: 0.8403\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 5.2409 - accuracy: 0.9247 - val_loss: 12.6369 - val_accuracy: 0.8379\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 5.1569 - accuracy: 0.9262 - val_loss: 11.9166 - val_accuracy: 0.8473\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.8394 - accuracy: 0.9319 - val_loss: 12.4543 - val_accuracy: 0.8316\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.7076 - accuracy: 0.9335 - val_loss: 12.3983 - val_accuracy: 0.8411\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.5989 - accuracy: 0.9356 - val_loss: 12.6812 - val_accuracy: 0.8467\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.4004 - accuracy: 0.9383 - val_loss: 13.0307 - val_accuracy: 0.8376\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.4223 - accuracy: 0.9374 - val_loss: 12.6307 - val_accuracy: 0.8457\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 4.1455 - accuracy: 0.9406 - val_loss: 11.7522 - val_accuracy: 0.8502\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 4.1484 - accuracy: 0.9412 - val_loss: 11.6093 - val_accuracy: 0.8535\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.0226 - accuracy: 0.9440 - val_loss: 12.7232 - val_accuracy: 0.8471\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.9208 - accuracy: 0.9454 - val_loss: 10.9979 - val_accuracy: 0.8578\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.7638 - accuracy: 0.9472 - val_loss: 12.8458 - val_accuracy: 0.8541\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.7778 - accuracy: 0.9471 - val_loss: 12.7928 - val_accuracy: 0.8351\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 3.7049 - accuracy: 0.9490 - val_loss: 12.2355 - val_accuracy: 0.8514\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.6466 - accuracy: 0.9483 - val_loss: 11.4944 - val_accuracy: 0.8649\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 1.8577 - accuracy: 0.2679 - val_loss: 1.7363 - val_accuracy: 0.3210\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.4953 - accuracy: 0.4044 - val_loss: 1.4756 - val_accuracy: 0.4000\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.2658 - accuracy: 0.4923 - val_loss: 1.2977 - val_accuracy: 0.4743\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 1.1335 - accuracy: 0.5537 - val_loss: 1.1993 - val_accuracy: 0.5348\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0401 - accuracy: 0.5893 - val_loss: 1.1228 - val_accuracy: 0.5675\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.9661 - accuracy: 0.6162 - val_loss: 1.0585 - val_accuracy: 0.5968\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.9264 - accuracy: 0.6318 - val_loss: 1.0108 - val_accuracy: 0.6113\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8934 - accuracy: 0.6437 - val_loss: 0.9264 - val_accuracy: 0.6638\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8613 - accuracy: 0.6510 - val_loss: 0.9135 - val_accuracy: 0.6657\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8338 - accuracy: 0.6625 - val_loss: 0.8899 - val_accuracy: 0.6490\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8130 - accuracy: 0.6715 - val_loss: 0.8822 - val_accuracy: 0.6681\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.7924 - accuracy: 0.6773 - val_loss: 0.8959 - val_accuracy: 0.6468\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.7783 - accuracy: 0.6813 - val_loss: 0.8184 - val_accuracy: 0.6900\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.7661 - accuracy: 0.6881 - val_loss: 0.8406 - val_accuracy: 0.6730\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.7545 - accuracy: 0.6937 - val_loss: 0.8324 - val_accuracy: 0.6835\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7383 - accuracy: 0.6996 - val_loss: 0.7976 - val_accuracy: 0.7032\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7285 - accuracy: 0.7030 - val_loss: 0.8923 - val_accuracy: 0.6629\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.7179 - accuracy: 0.7096 - val_loss: 0.8208 - val_accuracy: 0.6935\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.7166 - accuracy: 0.7073 - val_loss: 0.7603 - val_accuracy: 0.7200\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7031 - accuracy: 0.7141 - val_loss: 0.7386 - val_accuracy: 0.7297\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.6972 - accuracy: 0.7174 - val_loss: 0.7506 - val_accuracy: 0.7238\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.6950 - accuracy: 0.7194 - val_loss: 0.7333 - val_accuracy: 0.7321\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.6853 - accuracy: 0.7213 - val_loss: 0.7428 - val_accuracy: 0.7219\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6815 - accuracy: 0.7268 - val_loss: 0.7669 - val_accuracy: 0.7210\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.6742 - accuracy: 0.7249 - val_loss: 0.7081 - val_accuracy: 0.7333\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.6582 - accuracy: 0.7360 - val_loss: 0.7014 - val_accuracy: 0.7413\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6262 - accuracy: 0.7525 - val_loss: 0.7252 - val_accuracy: 0.7310\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.6061 - accuracy: 0.7610 - val_loss: 0.7626 - val_accuracy: 0.7097\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.5915 - accuracy: 0.7652 - val_loss: 0.6209 - val_accuracy: 0.7735\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5843 - accuracy: 0.7702 - val_loss: 0.6586 - val_accuracy: 0.7425\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 1.2948 - accuracy: 0.5753 - val_loss: 1.6426 - val_accuracy: 0.4202\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.7265 - accuracy: 0.8144 - val_loss: 0.7119 - val_accuracy: 0.7787\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.5012 - accuracy: 0.8636 - val_loss: 0.6126 - val_accuracy: 0.7829\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3906 - accuracy: 0.8852 - val_loss: 0.7116 - val_accuracy: 0.7413\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3273 - accuracy: 0.8976 - val_loss: 0.5487 - val_accuracy: 0.7995\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2846 - accuracy: 0.9084 - val_loss: 0.5170 - val_accuracy: 0.8183\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2588 - accuracy: 0.9173 - val_loss: 0.4927 - val_accuracy: 0.8267\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2372 - accuracy: 0.9237 - val_loss: 0.4425 - val_accuracy: 0.8529\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2212 - accuracy: 0.9267 - val_loss: 0.5026 - val_accuracy: 0.8178\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.2058 - accuracy: 0.9321 - val_loss: 0.5261 - val_accuracy: 0.8216\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.1956 - accuracy: 0.9335 - val_loss: 0.5224 - val_accuracy: 0.8341\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1888 - accuracy: 0.9350 - val_loss: 0.5498 - val_accuracy: 0.8359\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1790 - accuracy: 0.9383 - val_loss: 0.5545 - val_accuracy: 0.8237\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1706 - accuracy: 0.9413 - val_loss: 0.5180 - val_accuracy: 0.8389\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1668 - accuracy: 0.9422 - val_loss: 0.4408 - val_accuracy: 0.8522\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1631 - accuracy: 0.9431 - val_loss: 0.5259 - val_accuracy: 0.8303\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1574 - accuracy: 0.9440 - val_loss: 0.5135 - val_accuracy: 0.8267\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1482 - accuracy: 0.9485 - val_loss: 0.4713 - val_accuracy: 0.8495\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1489 - accuracy: 0.9475 - val_loss: 0.4919 - val_accuracy: 0.8411\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1482 - accuracy: 0.9483 - val_loss: 0.5624 - val_accuracy: 0.8305\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.1447 - accuracy: 0.9499 - val_loss: 0.5105 - val_accuracy: 0.8441\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1404 - accuracy: 0.9499 - val_loss: 0.5944 - val_accuracy: 0.8265\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1306 - accuracy: 0.9544 - val_loss: 0.5708 - val_accuracy: 0.8290\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1341 - accuracy: 0.9518 - val_loss: 0.5648 - val_accuracy: 0.8338\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1324 - accuracy: 0.9527 - val_loss: 0.5136 - val_accuracy: 0.8402\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1235 - accuracy: 0.9559 - val_loss: 0.6522 - val_accuracy: 0.8095\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1223 - accuracy: 0.9564 - val_loss: 0.5961 - val_accuracy: 0.8248\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1202 - accuracy: 0.9565 - val_loss: 0.5695 - val_accuracy: 0.8395\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.1201 - accuracy: 0.9565 - val_loss: 0.5930 - val_accuracy: 0.8344\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1173 - accuracy: 0.9593 - val_loss: 0.5402 - val_accuracy: 0.8438\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 183us/sample - loss: 1.3949 - decoder_loss: 0.2380 - clf_loss: 1.1552 - decoder_accuracy: 0.0073 - clf_accuracy: 0.6073 - val_loss: 1.3917 - val_decoder_loss: 0.1893 - val_clf_loss: 1.2017 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.5316\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.7471 - decoder_loss: 0.1767 - clf_loss: 0.5688 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8265 - val_loss: 0.7682 - val_decoder_loss: 0.1787 - val_clf_loss: 0.5926 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.8011\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.5700 - decoder_loss: 0.1704 - clf_loss: 0.3982 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8742 - val_loss: 0.7106 - val_decoder_loss: 0.1704 - val_clf_loss: 0.5354 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8129\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.4922 - decoder_loss: 0.1659 - clf_loss: 0.3248 - decoder_accuracy: 0.0078 - clf_accuracy: 0.8954 - val_loss: 0.7500 - val_decoder_loss: 0.1622 - val_clf_loss: 0.5899 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7887\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4415 - decoder_loss: 0.1614 - clf_loss: 0.2786 - decoder_accuracy: 0.0078 - clf_accuracy: 0.9074 - val_loss: 0.6211 - val_decoder_loss: 0.1595 - val_clf_loss: 0.4591 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8378\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4092 - decoder_loss: 0.1563 - clf_loss: 0.2514 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9177 - val_loss: 0.6883 - val_decoder_loss: 0.1569 - val_clf_loss: 0.5303 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8165\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3789 - decoder_loss: 0.1519 - clf_loss: 0.2254 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9248 - val_loss: 0.6380 - val_decoder_loss: 0.1493 - val_clf_loss: 0.4858 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8351\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.3633 - decoder_loss: 0.1477 - clf_loss: 0.2142 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9282 - val_loss: 0.6516 - val_decoder_loss: 0.1539 - val_clf_loss: 0.4956 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8216\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3471 - decoder_loss: 0.1430 - clf_loss: 0.2027 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9315 - val_loss: 0.6513 - val_decoder_loss: 0.1472 - val_clf_loss: 0.4999 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8198\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3348 - decoder_loss: 0.1398 - clf_loss: 0.1935 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9348 - val_loss: 0.6079 - val_decoder_loss: 0.1359 - val_clf_loss: 0.4711 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8259\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3242 - decoder_loss: 0.1368 - clf_loss: 0.1860 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9365 - val_loss: 0.6320 - val_decoder_loss: 0.1351 - val_clf_loss: 0.4927 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8352\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3117 - decoder_loss: 0.1340 - clf_loss: 0.1763 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9396 - val_loss: 0.6126 - val_decoder_loss: 0.1312 - val_clf_loss: 0.4842 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8402\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3049 - decoder_loss: 0.1315 - clf_loss: 0.1720 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9402 - val_loss: 0.6202 - val_decoder_loss: 0.1298 - val_clf_loss: 0.4948 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8278\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.2998 - decoder_loss: 0.1292 - clf_loss: 0.1692 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9414 - val_loss: 0.6252 - val_decoder_loss: 0.1304 - val_clf_loss: 0.4920 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8398\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.2913 - decoder_loss: 0.1269 - clf_loss: 0.1630 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9423 - val_loss: 0.5767 - val_decoder_loss: 0.1245 - val_clf_loss: 0.4507 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8387\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2890 - decoder_loss: 0.1248 - clf_loss: 0.1628 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9444 - val_loss: 0.5776 - val_decoder_loss: 0.1275 - val_clf_loss: 0.4452 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8467\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.2876 - decoder_loss: 0.1236 - clf_loss: 0.1626 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9435 - val_loss: 0.5756 - val_decoder_loss: 0.1223 - val_clf_loss: 0.4474 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8357\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2767 - decoder_loss: 0.1223 - clf_loss: 0.1530 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9471 - val_loss: 0.6407 - val_decoder_loss: 0.1237 - val_clf_loss: 0.5216 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8306\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2730 - decoder_loss: 0.1208 - clf_loss: 0.1507 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9475 - val_loss: 0.5441 - val_decoder_loss: 0.1193 - val_clf_loss: 0.4239 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8511\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2696 - decoder_loss: 0.1194 - clf_loss: 0.1488 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9468 - val_loss: 0.6264 - val_decoder_loss: 0.1186 - val_clf_loss: 0.5094 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8340\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.2642 - decoder_loss: 0.1182 - clf_loss: 0.1445 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9514 - val_loss: 0.5996 - val_decoder_loss: 0.1197 - val_clf_loss: 0.4750 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8356\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.2619 - decoder_loss: 0.1174 - clf_loss: 0.1431 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9505 - val_loss: 0.6509 - val_decoder_loss: 0.1151 - val_clf_loss: 0.5294 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8252\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 123us/sample - loss: 0.2582 - decoder_loss: 0.1164 - clf_loss: 0.1404 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9514 - val_loss: 0.6423 - val_decoder_loss: 0.1139 - val_clf_loss: 0.5248 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8203\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2536 - decoder_loss: 0.1154 - clf_loss: 0.1367 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9531 - val_loss: 0.6249 - val_decoder_loss: 0.1146 - val_clf_loss: 0.5079 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8246\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.2529 - decoder_loss: 0.1145 - clf_loss: 0.1369 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9525 - val_loss: 0.5416 - val_decoder_loss: 0.1128 - val_clf_loss: 0.4286 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8532\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2522 - decoder_loss: 0.1135 - clf_loss: 0.1373 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9531 - val_loss: 0.6665 - val_decoder_loss: 0.1146 - val_clf_loss: 0.5518 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8240\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2455 - decoder_loss: 0.1129 - clf_loss: 0.1312 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9544 - val_loss: 0.6662 - val_decoder_loss: 0.1121 - val_clf_loss: 0.5490 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8187\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2431 - decoder_loss: 0.1122 - clf_loss: 0.1294 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9559 - val_loss: 0.5645 - val_decoder_loss: 0.1114 - val_clf_loss: 0.4488 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8432\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2419 - decoder_loss: 0.1121 - clf_loss: 0.1284 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9539 - val_loss: 0.6394 - val_decoder_loss: 0.1094 - val_clf_loss: 0.5288 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8219\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2394 - decoder_loss: 0.1106 - clf_loss: 0.1273 - decoder_accuracy: 0.0079 - clf_accuracy: 0.9555 - val_loss: 0.5992 - val_decoder_loss: 0.1111 - val_clf_loss: 0.4815 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8348\n",
      "Running sub 7, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.088 0.058 1.095 2.933 0.007 0.64  0.273]\n",
      "[6.251 0.036 0.715 5.498 0.007 0.771 0.183]\n",
      "[5.894 0.033 0.522 5.339 0.007 0.852 0.203]\n",
      "[5.352 0.029 0.421 4.901 0.007 0.874 0.218]\n",
      "[5.297 0.027 0.366 4.903 0.007 0.884 0.268]\n",
      "[5.43  0.026 0.316 5.088 0.007 0.898 0.233]\n",
      "[5.909 0.025 0.291 5.593 0.007 0.904 0.241]\n",
      "[5.982 0.024 0.283 5.674 0.007 0.904 0.242]\n",
      "[6.403 0.023 0.277 6.102 0.007 0.9   0.249]\n",
      "[6.907 0.022 0.277 6.608 0.007 0.898 0.259]\n",
      "[7.338 0.021 0.277 7.04  0.007 0.901 0.254]\n",
      "[7.781 0.02  0.251 7.51  0.007 0.908 0.254]\n",
      "[8.542 0.019 0.243 8.279 0.007 0.912 0.251]\n",
      "[9.113 0.018 0.238 8.856 0.007 0.914 0.247]\n",
      "[9.23  0.018 0.236 8.976 0.007 0.914 0.268]\n",
      "[0.807 0.043 0.277 0.487 0.007 0.9   0.309]\n",
      "[0.517 0.043 0.232 0.242 0.007 0.913 0.27 ]\n",
      "[0.402 0.042 0.227 0.133 0.007 0.914 0.286]\n",
      "[0.336 0.043 0.221 0.071 0.007 0.918 0.274]\n",
      "[0.304 0.042 0.224 0.037 0.007 0.917 0.281]\n",
      "[0.295 0.042 0.232 0.02  0.007 0.914 0.282]\n",
      "[0.275 0.042 0.221 0.011 0.007 0.919 0.274]\n",
      "[0.283 0.043 0.232 0.008 0.007 0.917 0.288]\n",
      "[0.286 0.042 0.238 0.006 0.007 0.911 0.282]\n",
      "[0.286 0.042 0.238 0.006 0.007 0.914 0.267]\n",
      "[0.274 0.042 0.227 0.004 0.007 0.92  0.268]\n",
      "[0.303 0.042 0.256 0.005 0.007 0.91  0.258]\n",
      "[0.279 0.042 0.232 0.004 0.007 0.918 0.253]\n",
      "[0.274 0.043 0.227 0.004 0.007 0.921 0.251]\n",
      "[0.276 0.042 0.23  0.004 0.007 0.922 0.247]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 41.0668 - accuracy: 0.3636 - val_loss: 45.8461 - val_accuracy: 0.2498\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 28.4845 - accuracy: 0.6079 - val_loss: 26.0011 - val_accuracy: 0.6710\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 20.6527 - accuracy: 0.7392 - val_loss: 17.7318 - val_accuracy: 0.7751\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 15.3703 - accuracy: 0.8100 - val_loss: 13.6542 - val_accuracy: 0.8248\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 12.1679 - accuracy: 0.8481 - val_loss: 11.1545 - val_accuracy: 0.8544\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 10.0909 - accuracy: 0.8731 - val_loss: 11.0889 - val_accuracy: 0.8522\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.8077 - accuracy: 0.8878 - val_loss: 8.7803 - val_accuracy: 0.8865\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.8224 - accuracy: 0.8959 - val_loss: 8.7476 - val_accuracy: 0.8827\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 6.9759 - accuracy: 0.9085 - val_loss: 7.6344 - val_accuracy: 0.8992\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.5725 - accuracy: 0.9122 - val_loss: 7.4870 - val_accuracy: 0.8989\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 6.2028 - accuracy: 0.9169 - val_loss: 5.7585 - val_accuracy: 0.9263\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 5.7284 - accuracy: 0.9201 - val_loss: 6.1637 - val_accuracy: 0.9154\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 5.5127 - accuracy: 0.9263 - val_loss: 6.4119 - val_accuracy: 0.9090\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 5.2896 - accuracy: 0.9280 - val_loss: 6.1921 - val_accuracy: 0.9170\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.9228 - accuracy: 0.9327 - val_loss: 6.1565 - val_accuracy: 0.9133\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 4.7484 - accuracy: 0.9340 - val_loss: 4.8315 - val_accuracy: 0.9344\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.7882 - accuracy: 0.9349 - val_loss: 5.9520 - val_accuracy: 0.9154\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.3935 - accuracy: 0.9400 - val_loss: 5.5687 - val_accuracy: 0.9227\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.4019 - accuracy: 0.9395 - val_loss: 5.6105 - val_accuracy: 0.9221\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 4.2819 - accuracy: 0.9425 - val_loss: 5.1977 - val_accuracy: 0.9310\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.0523 - accuracy: 0.9450 - val_loss: 5.2218 - val_accuracy: 0.9308\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.0469 - accuracy: 0.9442 - val_loss: 5.8319 - val_accuracy: 0.9184\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 3.8952 - accuracy: 0.9471 - val_loss: 6.0452 - val_accuracy: 0.9214\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 3.7803 - accuracy: 0.9492 - val_loss: 4.7595 - val_accuracy: 0.9375\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 3.7263 - accuracy: 0.9499 - val_loss: 5.6763 - val_accuracy: 0.9219\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 85us/sample - loss: 3.6228 - accuracy: 0.9492 - val_loss: 5.8214 - val_accuracy: 0.9205\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 3.5025 - accuracy: 0.9525 - val_loss: 4.8520 - val_accuracy: 0.9325\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 3.4982 - accuracy: 0.9503 - val_loss: 4.8549 - val_accuracy: 0.9359\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 3.3718 - accuracy: 0.9534 - val_loss: 5.6412 - val_accuracy: 0.9275\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 3.3886 - accuracy: 0.9535 - val_loss: 4.9955 - val_accuracy: 0.9311\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 1.7944 - accuracy: 0.3118 - val_loss: 1.5866 - val_accuracy: 0.4000\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 1.4626 - accuracy: 0.4376 - val_loss: 1.3020 - val_accuracy: 0.5562\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 1.2544 - accuracy: 0.5419 - val_loss: 1.1057 - val_accuracy: 0.6010\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 1.0757 - accuracy: 0.6104 - val_loss: 0.9951 - val_accuracy: 0.6567\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 0.9486 - accuracy: 0.6591 - val_loss: 0.8644 - val_accuracy: 0.7094\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.8689 - accuracy: 0.6931 - val_loss: 0.7663 - val_accuracy: 0.7386\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7995 - accuracy: 0.7181 - val_loss: 0.7824 - val_accuracy: 0.7319\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.7420 - accuracy: 0.7365 - val_loss: 0.6955 - val_accuracy: 0.7559\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.6996 - accuracy: 0.7517 - val_loss: 0.6445 - val_accuracy: 0.7776\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 0.6610 - accuracy: 0.7668 - val_loss: 0.5839 - val_accuracy: 0.8025\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.6220 - accuracy: 0.7802 - val_loss: 0.5775 - val_accuracy: 0.8017\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.6007 - accuracy: 0.7873 - val_loss: 0.5234 - val_accuracy: 0.8203\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5765 - accuracy: 0.7962 - val_loss: 0.5438 - val_accuracy: 0.8125\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.5605 - accuracy: 0.8013 - val_loss: 0.4822 - val_accuracy: 0.8375\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.5359 - accuracy: 0.8086 - val_loss: 0.5030 - val_accuracy: 0.8275\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.5227 - accuracy: 0.8163 - val_loss: 0.4554 - val_accuracy: 0.8397\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5143 - accuracy: 0.8188 - val_loss: 0.4647 - val_accuracy: 0.8373\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4997 - accuracy: 0.8241 - val_loss: 0.4226 - val_accuracy: 0.8546\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4976 - accuracy: 0.8228 - val_loss: 0.4852 - val_accuracy: 0.8319\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.4790 - accuracy: 0.8327 - val_loss: 0.4187 - val_accuracy: 0.8538\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4704 - accuracy: 0.8344 - val_loss: 0.4229 - val_accuracy: 0.8514\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4667 - accuracy: 0.8349 - val_loss: 0.4377 - val_accuracy: 0.8444\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.4557 - accuracy: 0.8386 - val_loss: 0.4104 - val_accuracy: 0.8610\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4528 - accuracy: 0.8377 - val_loss: 0.4064 - val_accuracy: 0.8544\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4430 - accuracy: 0.8425 - val_loss: 0.3736 - val_accuracy: 0.8706\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.4387 - accuracy: 0.8407 - val_loss: 0.4053 - val_accuracy: 0.8571\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4317 - accuracy: 0.8448 - val_loss: 0.3577 - val_accuracy: 0.8765\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4251 - accuracy: 0.8474 - val_loss: 0.3727 - val_accuracy: 0.8724\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.4262 - accuracy: 0.8463 - val_loss: 0.3836 - val_accuracy: 0.8656\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.4198 - accuracy: 0.8507 - val_loss: 0.3579 - val_accuracy: 0.8800\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 90us/sample - loss: 1.2181 - accuracy: 0.6106 - val_loss: 1.7014 - val_accuracy: 0.3908\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.7000 - accuracy: 0.8216 - val_loss: 0.6137 - val_accuracy: 0.8438\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4906 - accuracy: 0.8672 - val_loss: 0.4213 - val_accuracy: 0.8906\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3891 - accuracy: 0.8838 - val_loss: 0.4361 - val_accuracy: 0.8590\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3247 - accuracy: 0.9001 - val_loss: 0.3192 - val_accuracy: 0.9051\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2871 - accuracy: 0.9093 - val_loss: 0.2781 - val_accuracy: 0.9149\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.2644 - accuracy: 0.9137 - val_loss: 0.2436 - val_accuracy: 0.9184\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2359 - accuracy: 0.9216 - val_loss: 0.2240 - val_accuracy: 0.9263\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2202 - accuracy: 0.9274 - val_loss: 0.2255 - val_accuracy: 0.9237\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2113 - accuracy: 0.9304 - val_loss: 0.2030 - val_accuracy: 0.9325\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2012 - accuracy: 0.9316 - val_loss: 0.2480 - val_accuracy: 0.9178\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.1917 - accuracy: 0.9346 - val_loss: 0.2736 - val_accuracy: 0.9017\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1823 - accuracy: 0.9393 - val_loss: 0.1929 - val_accuracy: 0.9310\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1737 - accuracy: 0.9406 - val_loss: 0.2174 - val_accuracy: 0.9278\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1735 - accuracy: 0.9411 - val_loss: 0.2114 - val_accuracy: 0.9275\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1639 - accuracy: 0.9442 - val_loss: 0.2209 - val_accuracy: 0.9254\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1642 - accuracy: 0.9436 - val_loss: 0.2104 - val_accuracy: 0.9298\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.1568 - accuracy: 0.9471 - val_loss: 0.1980 - val_accuracy: 0.9317\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1545 - accuracy: 0.9485 - val_loss: 0.2577 - val_accuracy: 0.9140\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1455 - accuracy: 0.9504 - val_loss: 0.1849 - val_accuracy: 0.9392\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1430 - accuracy: 0.9513 - val_loss: 0.2289 - val_accuracy: 0.9205\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1398 - accuracy: 0.9534 - val_loss: 0.2220 - val_accuracy: 0.9222\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1356 - accuracy: 0.9534 - val_loss: 0.1987 - val_accuracy: 0.9325\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1352 - accuracy: 0.9540 - val_loss: 0.1778 - val_accuracy: 0.9387\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1320 - accuracy: 0.9546 - val_loss: 0.2208 - val_accuracy: 0.9260\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1300 - accuracy: 0.9536 - val_loss: 0.1755 - val_accuracy: 0.9387\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1274 - accuracy: 0.9577 - val_loss: 0.1682 - val_accuracy: 0.9425\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1271 - accuracy: 0.9567 - val_loss: 0.1623 - val_accuracy: 0.9443\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.1247 - accuracy: 0.9568 - val_loss: 0.2475 - val_accuracy: 0.9159\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1202 - accuracy: 0.9580 - val_loss: 0.1795 - val_accuracy: 0.9376\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 193us/sample - loss: 1.2360 - decoder_loss: 0.2355 - clf_loss: 0.9991 - decoder_accuracy: 0.0064 - clf_accuracy: 0.6663 - val_loss: 1.0406 - val_decoder_loss: 0.1869 - val_clf_loss: 0.8545 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.7324\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.6840 - decoder_loss: 0.1650 - clf_loss: 0.5178 - decoder_accuracy: 0.0067 - clf_accuracy: 0.8381 - val_loss: 0.5470 - val_decoder_loss: 0.1627 - val_clf_loss: 0.3826 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.8833\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.5344 - decoder_loss: 0.1546 - clf_loss: 0.3785 - decoder_accuracy: 0.0070 - clf_accuracy: 0.8797 - val_loss: 0.5084 - val_decoder_loss: 0.1549 - val_clf_loss: 0.3514 - val_decoder_accuracy: 0.0064 - val_clf_accuracy: 0.8868\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4638 - decoder_loss: 0.1485 - clf_loss: 0.3140 - decoder_accuracy: 0.0071 - clf_accuracy: 0.8987 - val_loss: 0.4146 - val_decoder_loss: 0.1443 - val_clf_loss: 0.2670 - val_decoder_accuracy: 0.0065 - val_clf_accuracy: 0.9133\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.4246 - decoder_loss: 0.1441 - clf_loss: 0.2792 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9073 - val_loss: 0.3776 - val_decoder_loss: 0.1447 - val_clf_loss: 0.2327 - val_decoder_accuracy: 0.0065 - val_clf_accuracy: 0.9235\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3963 - decoder_loss: 0.1400 - clf_loss: 0.2550 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9140 - val_loss: 0.3781 - val_decoder_loss: 0.1392 - val_clf_loss: 0.2386 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9206\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3752 - decoder_loss: 0.1368 - clf_loss: 0.2369 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9196 - val_loss: 0.3690 - val_decoder_loss: 0.1335 - val_clf_loss: 0.2355 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9203\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3619 - decoder_loss: 0.1337 - clf_loss: 0.2267 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9235 - val_loss: 0.3812 - val_decoder_loss: 0.1354 - val_clf_loss: 0.2471 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9159\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3425 - decoder_loss: 0.1315 - clf_loss: 0.2096 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9281 - val_loss: 0.3622 - val_decoder_loss: 0.1373 - val_clf_loss: 0.2218 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9240\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3378 - decoder_loss: 0.1290 - clf_loss: 0.2074 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9290 - val_loss: 0.3496 - val_decoder_loss: 0.1242 - val_clf_loss: 0.2237 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9230\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3237 - decoder_loss: 0.1268 - clf_loss: 0.1955 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9343 - val_loss: 0.3337 - val_decoder_loss: 0.1301 - val_clf_loss: 0.2048 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9276\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3098 - decoder_loss: 0.1246 - clf_loss: 0.1838 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9390 - val_loss: 0.3288 - val_decoder_loss: 0.1256 - val_clf_loss: 0.2024 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9295\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3036 - decoder_loss: 0.1235 - clf_loss: 0.1787 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9388 - val_loss: 0.3321 - val_decoder_loss: 0.1291 - val_clf_loss: 0.2016 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9287\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.2983 - decoder_loss: 0.1206 - clf_loss: 0.1762 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9409 - val_loss: 0.2989 - val_decoder_loss: 0.1230 - val_clf_loss: 0.1729 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9403\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.2915 - decoder_loss: 0.1199 - clf_loss: 0.1702 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9416 - val_loss: 0.4244 - val_decoder_loss: 0.1224 - val_clf_loss: 0.2970 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.8889\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2849 - decoder_loss: 0.1181 - clf_loss: 0.1655 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9444 - val_loss: 0.3250 - val_decoder_loss: 0.1225 - val_clf_loss: 0.2012 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9278\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2791 - decoder_loss: 0.1168 - clf_loss: 0.1608 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9464 - val_loss: 0.2949 - val_decoder_loss: 0.1164 - val_clf_loss: 0.1771 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9340\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2795 - decoder_loss: 0.1156 - clf_loss: 0.1624 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9441 - val_loss: 0.2981 - val_decoder_loss: 0.1137 - val_clf_loss: 0.1835 - val_decoder_accuracy: 0.0065 - val_clf_accuracy: 0.9352\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.2743 - decoder_loss: 0.1143 - clf_loss: 0.1586 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9454 - val_loss: 0.3122 - val_decoder_loss: 0.1161 - val_clf_loss: 0.1954 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9279\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2660 - decoder_loss: 0.1130 - clf_loss: 0.1517 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9476 - val_loss: 0.2964 - val_decoder_loss: 0.1129 - val_clf_loss: 0.1804 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9348\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2675 - decoder_loss: 0.1122 - clf_loss: 0.1539 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9478 - val_loss: 0.2900 - val_decoder_loss: 0.1112 - val_clf_loss: 0.1781 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9343\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2625 - decoder_loss: 0.1114 - clf_loss: 0.1496 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9483 - val_loss: 0.3267 - val_decoder_loss: 0.1125 - val_clf_loss: 0.2123 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9214\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2582 - decoder_loss: 0.1100 - clf_loss: 0.1468 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9488 - val_loss: 0.2806 - val_decoder_loss: 0.1122 - val_clf_loss: 0.1670 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9416\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 122us/sample - loss: 0.2535 - decoder_loss: 0.1094 - clf_loss: 0.1427 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9525 - val_loss: 0.3202 - val_decoder_loss: 0.1097 - val_clf_loss: 0.2101 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9251\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2530 - decoder_loss: 0.1090 - clf_loss: 0.1426 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9511 - val_loss: 0.2785 - val_decoder_loss: 0.1083 - val_clf_loss: 0.1726 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9421\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2441 - decoder_loss: 0.1081 - clf_loss: 0.1345 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9545 - val_loss: 0.3000 - val_decoder_loss: 0.1082 - val_clf_loss: 0.1905 - val_decoder_accuracy: 0.0066 - val_clf_accuracy: 0.9321\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2456 - decoder_loss: 0.1072 - clf_loss: 0.1370 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9535 - val_loss: 0.2914 - val_decoder_loss: 0.1072 - val_clf_loss: 0.1811 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9324\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2425 - decoder_loss: 0.1067 - clf_loss: 0.1344 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9524 - val_loss: 0.2727 - val_decoder_loss: 0.1105 - val_clf_loss: 0.1594 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9438\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2381 - decoder_loss: 0.1061 - clf_loss: 0.1305 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9544 - val_loss: 0.2988 - val_decoder_loss: 0.1046 - val_clf_loss: 0.1904 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9295\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.2365 - decoder_loss: 0.1056 - clf_loss: 0.1295 - decoder_accuracy: 0.0073 - clf_accuracy: 0.9550 - val_loss: 0.3410 - val_decoder_loss: 0.1070 - val_clf_loss: 0.2306 - val_decoder_accuracy: 0.0067 - val_clf_accuracy: 0.9132\n",
      "Running sub 8, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[3.87  0.058 1.284 2.528 0.008 0.539 0.165]\n",
      "[6.601 0.036 1.092 5.473 0.008 0.6   0.138]\n",
      "[6.387 0.033 0.986 5.367 0.008 0.648 0.128]\n",
      "[6.986 0.031 0.926 6.028 0.008 0.672 0.135]\n",
      "[7.327 0.029 0.88  6.417 0.008 0.696 0.177]\n",
      "[7.501 0.028 0.883 6.589 0.008 0.698 0.176]\n",
      "[8.69  0.026 0.903 7.761 0.008 0.7   0.172]\n",
      "[8.41  0.025 0.94  7.444 0.008 0.697 0.16 ]\n",
      "[8.778 0.023 0.937 7.816 0.008 0.702 0.157]\n",
      "[9.019 0.022 0.894 8.102 0.008 0.724 0.166]\n",
      "[9.417 0.021 0.896 8.499 0.008 0.727 0.152]\n",
      "[9.725 0.021 0.903 8.801 0.008 0.726 0.152]\n",
      "[10.184  0.02   0.914  9.249  0.008  0.727  0.151]\n",
      "[11.183  0.019  0.9   10.263  0.008  0.732  0.148]\n",
      "[11.208  0.018  0.909 10.281  0.008  0.734  0.143]\n",
      "[1.728 0.045 0.905 0.779 0.008 0.732 0.135]\n",
      "[1.284 0.044 0.923 0.317 0.008 0.735 0.143]\n",
      "[1.155 0.044 0.953 0.158 0.008 0.73  0.142]\n",
      "[1.056 0.044 0.937 0.075 0.008 0.736 0.145]\n",
      "[1.    0.044 0.922 0.035 0.008 0.741 0.142]\n",
      "[0.968 0.044 0.908 0.017 0.008 0.741 0.153]\n",
      "[0.966 0.044 0.913 0.009 0.008 0.745 0.157]\n",
      "[0.931 0.044 0.882 0.006 0.008 0.754 0.164]\n",
      "[1.509 0.044 1.369 0.097 0.008 0.608 0.089]\n",
      "[0.864 0.043 0.817 0.003 0.008 0.745 0.173]\n",
      "[0.805 0.043 0.759 0.002 0.008 0.767 0.168]\n",
      "[0.803 0.044 0.757 0.002 0.008 0.765 0.168]\n",
      "[0.801 0.044 0.755 0.002 0.008 0.77  0.167]\n",
      "[0.81  0.043 0.765 0.002 0.008 0.772 0.165]\n",
      "[0.813 0.043 0.768 0.001 0.008 0.769 0.169]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 44.1919 - accuracy: 0.3198 - val_loss: 46.4465 - val_accuracy: 0.2386\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 33.1193 - accuracy: 0.4791 - val_loss: 33.8442 - val_accuracy: 0.4595\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 27.4158 - accuracy: 0.5675 - val_loss: 31.3657 - val_accuracy: 0.5030\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 23.4249 - accuracy: 0.6312 - val_loss: 28.3962 - val_accuracy: 0.5625\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 20.4191 - accuracy: 0.6763 - val_loss: 25.2784 - val_accuracy: 0.6256\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 18.1563 - accuracy: 0.7150 - val_loss: 24.8744 - val_accuracy: 0.6468\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 16.3245 - accuracy: 0.7487 - val_loss: 23.2993 - val_accuracy: 0.6751\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 84us/sample - loss: 14.5863 - accuracy: 0.7796 - val_loss: 24.1332 - val_accuracy: 0.6806\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 13.2589 - accuracy: 0.8008 - val_loss: 22.0000 - val_accuracy: 0.7027\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 12.2470 - accuracy: 0.8175 - val_loss: 21.1542 - val_accuracy: 0.7117\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 11.4503 - accuracy: 0.8295 - val_loss: 20.7934 - val_accuracy: 0.7278\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 10.8337 - accuracy: 0.8394 - val_loss: 21.4245 - val_accuracy: 0.7202\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 85us/sample - loss: 10.2040 - accuracy: 0.8499 - val_loss: 21.7624 - val_accuracy: 0.7276\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 93us/sample - loss: 9.7574 - accuracy: 0.8576 - val_loss: 22.0041 - val_accuracy: 0.7181\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 88us/sample - loss: 9.4215 - accuracy: 0.8608 - val_loss: 21.9765 - val_accuracy: 0.7233\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 85us/sample - loss: 9.1092 - accuracy: 0.8671 - val_loss: 22.7140 - val_accuracy: 0.7138\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 97us/sample - loss: 8.9680 - accuracy: 0.8670 - val_loss: 19.9647 - val_accuracy: 0.7527\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 90us/sample - loss: 8.5024 - accuracy: 0.8750 - val_loss: 20.1551 - val_accuracy: 0.7454\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 8.2575 - accuracy: 0.8788 - val_loss: 20.9672 - val_accuracy: 0.7414\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 7.8906 - accuracy: 0.8854 - val_loss: 20.1310 - val_accuracy: 0.7575\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 7.8068 - accuracy: 0.8858 - val_loss: 20.3809 - val_accuracy: 0.7559\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.5481 - accuracy: 0.8906 - val_loss: 21.4646 - val_accuracy: 0.7406\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 7.3612 - accuracy: 0.8933 - val_loss: 20.8645 - val_accuracy: 0.7565\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.3394 - accuracy: 0.8930 - val_loss: 20.9611 - val_accuracy: 0.7538\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.1696 - accuracy: 0.8957 - val_loss: 23.0744 - val_accuracy: 0.7289\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 7.0226 - accuracy: 0.8986 - val_loss: 21.7047 - val_accuracy: 0.7457\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.0030 - accuracy: 0.8984 - val_loss: 21.5082 - val_accuracy: 0.7446\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 6.8252 - accuracy: 0.9004 - val_loss: 23.9050 - val_accuracy: 0.7306\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 6.8920 - accuracy: 0.9005 - val_loss: 20.0832 - val_accuracy: 0.7679\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 6.4022 - accuracy: 0.9084 - val_loss: 21.3983 - val_accuracy: 0.7529\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 1.9912 - accuracy: 0.2003 - val_loss: 1.8677 - val_accuracy: 0.2670\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 1.6950 - accuracy: 0.3384 - val_loss: 1.6620 - val_accuracy: 0.3421\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 1.4873 - accuracy: 0.4235 - val_loss: 1.4761 - val_accuracy: 0.4329\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.3351 - accuracy: 0.4849 - val_loss: 1.3127 - val_accuracy: 0.4900\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.2323 - accuracy: 0.5207 - val_loss: 1.2678 - val_accuracy: 0.5205\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 1.1711 - accuracy: 0.5366 - val_loss: 1.2276 - val_accuracy: 0.5195\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 1.1272 - accuracy: 0.5508 - val_loss: 1.2073 - val_accuracy: 0.5278\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 1.0929 - accuracy: 0.5594 - val_loss: 1.1484 - val_accuracy: 0.5556\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.0593 - accuracy: 0.5677 - val_loss: 1.1491 - val_accuracy: 0.5444\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 1.0287 - accuracy: 0.5828 - val_loss: 1.1442 - val_accuracy: 0.5622\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 1.0089 - accuracy: 0.5885 - val_loss: 1.1208 - val_accuracy: 0.5583\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9886 - accuracy: 0.6001 - val_loss: 1.1020 - val_accuracy: 0.5617\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9710 - accuracy: 0.6056 - val_loss: 1.1217 - val_accuracy: 0.5590\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9585 - accuracy: 0.6109 - val_loss: 1.0859 - val_accuracy: 0.5697\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.9424 - accuracy: 0.6174 - val_loss: 1.0921 - val_accuracy: 0.5824\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.9282 - accuracy: 0.6217 - val_loss: 1.0636 - val_accuracy: 0.5943\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.9134 - accuracy: 0.6299 - val_loss: 1.0747 - val_accuracy: 0.6030\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.9073 - accuracy: 0.6335 - val_loss: 1.0615 - val_accuracy: 0.6035\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8977 - accuracy: 0.6365 - val_loss: 1.0574 - val_accuracy: 0.5986\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8898 - accuracy: 0.6394 - val_loss: 1.0317 - val_accuracy: 0.6103\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.8816 - accuracy: 0.6431 - val_loss: 0.9989 - val_accuracy: 0.6187\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.8735 - accuracy: 0.6467 - val_loss: 1.0139 - val_accuracy: 0.6170\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.8662 - accuracy: 0.6506 - val_loss: 1.0152 - val_accuracy: 0.6157\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8632 - accuracy: 0.6511 - val_loss: 1.0097 - val_accuracy: 0.6257\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8602 - accuracy: 0.6533 - val_loss: 0.9841 - val_accuracy: 0.6248\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.8537 - accuracy: 0.6576 - val_loss: 1.0106 - val_accuracy: 0.6235\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8439 - accuracy: 0.6558 - val_loss: 1.0004 - val_accuracy: 0.6257\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.8425 - accuracy: 0.6577 - val_loss: 1.0019 - val_accuracy: 0.6335\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8303 - accuracy: 0.6610 - val_loss: 1.0259 - val_accuracy: 0.6351\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.8362 - accuracy: 0.6650 - val_loss: 1.0154 - val_accuracy: 0.6273\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 1.4598 - accuracy: 0.4830 - val_loss: 1.8889 - val_accuracy: 0.2103\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 1.0560 - accuracy: 0.6504 - val_loss: 1.0916 - val_accuracy: 0.6335\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.8701 - accuracy: 0.7078 - val_loss: 0.9907 - val_accuracy: 0.6633\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.7439 - accuracy: 0.7515 - val_loss: 0.8612 - val_accuracy: 0.7046\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.6542 - accuracy: 0.7841 - val_loss: 0.8976 - val_accuracy: 0.6848\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5927 - accuracy: 0.7985 - val_loss: 0.8475 - val_accuracy: 0.7035\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.5414 - accuracy: 0.8136 - val_loss: 0.8181 - val_accuracy: 0.7221\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5019 - accuracy: 0.8262 - val_loss: 0.8658 - val_accuracy: 0.6997\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4718 - accuracy: 0.8346 - val_loss: 0.8181 - val_accuracy: 0.7216\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4482 - accuracy: 0.8440 - val_loss: 0.7863 - val_accuracy: 0.7351\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4258 - accuracy: 0.8521 - val_loss: 0.8502 - val_accuracy: 0.7200\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4172 - accuracy: 0.8554 - val_loss: 0.8338 - val_accuracy: 0.7206\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3906 - accuracy: 0.8620 - val_loss: 0.7834 - val_accuracy: 0.7468\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3789 - accuracy: 0.8662 - val_loss: 0.8113 - val_accuracy: 0.7459\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3670 - accuracy: 0.8712 - val_loss: 0.8522 - val_accuracy: 0.7211\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3607 - accuracy: 0.8725 - val_loss: 0.8630 - val_accuracy: 0.7321\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3513 - accuracy: 0.8769 - val_loss: 0.8466 - val_accuracy: 0.7254\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3439 - accuracy: 0.8794 - val_loss: 0.8412 - val_accuracy: 0.7387\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3367 - accuracy: 0.8818 - val_loss: 0.9201 - val_accuracy: 0.7186\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.3302 - accuracy: 0.8826 - val_loss: 0.8858 - val_accuracy: 0.7265\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3280 - accuracy: 0.8842 - val_loss: 0.9008 - val_accuracy: 0.7349\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3196 - accuracy: 0.8876 - val_loss: 0.9167 - val_accuracy: 0.7160\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3116 - accuracy: 0.8897 - val_loss: 0.8852 - val_accuracy: 0.7438\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.3042 - accuracy: 0.8922 - val_loss: 0.8609 - val_accuracy: 0.7525\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3002 - accuracy: 0.8944 - val_loss: 0.9033 - val_accuracy: 0.7389\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3009 - accuracy: 0.8937 - val_loss: 0.9253 - val_accuracy: 0.7521\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2929 - accuracy: 0.8950 - val_loss: 0.9824 - val_accuracy: 0.7132\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2902 - accuracy: 0.8967 - val_loss: 0.8817 - val_accuracy: 0.7471\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2886 - accuracy: 0.8965 - val_loss: 0.9416 - val_accuracy: 0.7329\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 47us/sample - loss: 0.2815 - accuracy: 0.9000 - val_loss: 0.9513 - val_accuracy: 0.7376\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 201us/sample - loss: 1.5110 - decoder_loss: 0.2416 - clf_loss: 1.2678 - decoder_accuracy: 0.0077 - clf_accuracy: 0.5463 - val_loss: 1.3905 - val_decoder_loss: 0.1957 - val_clf_loss: 1.1947 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.6124\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.9683 - decoder_loss: 0.1762 - clf_loss: 0.7905 - decoder_accuracy: 0.0079 - clf_accuracy: 0.7276 - val_loss: 1.0201 - val_decoder_loss: 0.1684 - val_clf_loss: 0.8531 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.6998\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.7953 - decoder_loss: 0.1685 - clf_loss: 0.6252 - decoder_accuracy: 0.0081 - clf_accuracy: 0.7790 - val_loss: 0.9260 - val_decoder_loss: 0.1626 - val_clf_loss: 0.7679 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7251\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.7059 - decoder_loss: 0.1628 - clf_loss: 0.5414 - decoder_accuracy: 0.0082 - clf_accuracy: 0.8050 - val_loss: 0.8850 - val_decoder_loss: 0.1592 - val_clf_loss: 0.7212 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7402\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.6379 - decoder_loss: 0.1562 - clf_loss: 0.4800 - decoder_accuracy: 0.0082 - clf_accuracy: 0.8274 - val_loss: 0.8885 - val_decoder_loss: 0.1540 - val_clf_loss: 0.7280 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7375\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.5996 - decoder_loss: 0.1487 - clf_loss: 0.4491 - decoder_accuracy: 0.0082 - clf_accuracy: 0.8390 - val_loss: 0.9100 - val_decoder_loss: 0.1487 - val_clf_loss: 0.7569 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7390\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.5659 - decoder_loss: 0.1438 - clf_loss: 0.4205 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8488 - val_loss: 0.8672 - val_decoder_loss: 0.1409 - val_clf_loss: 0.7236 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7446\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.5371 - decoder_loss: 0.1396 - clf_loss: 0.3959 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8579 - val_loss: 0.9193 - val_decoder_loss: 0.1398 - val_clf_loss: 0.7779 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7354\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.5189 - decoder_loss: 0.1370 - clf_loss: 0.3804 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8645 - val_loss: 0.8829 - val_decoder_loss: 0.1366 - val_clf_loss: 0.7456 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7495\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.5065 - decoder_loss: 0.1338 - clf_loss: 0.3711 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8644 - val_loss: 0.8140 - val_decoder_loss: 0.1347 - val_clf_loss: 0.6760 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7690\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.4920 - decoder_loss: 0.1317 - clf_loss: 0.3586 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8706 - val_loss: 0.8298 - val_decoder_loss: 0.1290 - val_clf_loss: 0.7041 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7602\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4728 - decoder_loss: 0.1290 - clf_loss: 0.3422 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8762 - val_loss: 0.8148 - val_decoder_loss: 0.1277 - val_clf_loss: 0.6806 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7651\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4601 - decoder_loss: 0.1277 - clf_loss: 0.3308 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8813 - val_loss: 0.8524 - val_decoder_loss: 0.1240 - val_clf_loss: 0.7196 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7544\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.4557 - decoder_loss: 0.1261 - clf_loss: 0.3280 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8838 - val_loss: 0.8537 - val_decoder_loss: 0.1226 - val_clf_loss: 0.7286 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7548\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.4424 - decoder_loss: 0.1248 - clf_loss: 0.3160 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8868 - val_loss: 0.8511 - val_decoder_loss: 0.1250 - val_clf_loss: 0.7247 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7662\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4356 - decoder_loss: 0.1238 - clf_loss: 0.3102 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8896 - val_loss: 0.7711 - val_decoder_loss: 0.1245 - val_clf_loss: 0.6503 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7717\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4256 - decoder_loss: 0.1227 - clf_loss: 0.3013 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8942 - val_loss: 0.8648 - val_decoder_loss: 0.1215 - val_clf_loss: 0.7390 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7557\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.4195 - decoder_loss: 0.1213 - clf_loss: 0.2965 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8939 - val_loss: 0.8782 - val_decoder_loss: 0.1195 - val_clf_loss: 0.7662 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7530\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4087 - decoder_loss: 0.1204 - clf_loss: 0.2868 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8970 - val_loss: 0.8063 - val_decoder_loss: 0.1218 - val_clf_loss: 0.6851 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7779\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.4062 - decoder_loss: 0.1195 - clf_loss: 0.2852 - decoder_accuracy: 0.0083 - clf_accuracy: 0.8982 - val_loss: 0.8181 - val_decoder_loss: 0.1217 - val_clf_loss: 0.6946 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7687\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.4002 - decoder_loss: 0.1180 - clf_loss: 0.2807 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9002 - val_loss: 0.8147 - val_decoder_loss: 0.1202 - val_clf_loss: 0.6890 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7722\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3962 - decoder_loss: 0.1180 - clf_loss: 0.2767 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9020 - val_loss: 0.8335 - val_decoder_loss: 0.1180 - val_clf_loss: 0.7107 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7637\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3850 - decoder_loss: 0.1166 - clf_loss: 0.2669 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9066 - val_loss: 0.8027 - val_decoder_loss: 0.1180 - val_clf_loss: 0.6874 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7721\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.3829 - decoder_loss: 0.1162 - clf_loss: 0.2652 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9059 - val_loss: 0.8813 - val_decoder_loss: 0.1177 - val_clf_loss: 0.7611 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7652\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.3799 - decoder_loss: 0.1155 - clf_loss: 0.2630 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9074 - val_loss: 0.8578 - val_decoder_loss: 0.1227 - val_clf_loss: 0.7390 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7727\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3724 - decoder_loss: 0.1145 - clf_loss: 0.2562 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9085 - val_loss: 0.7929 - val_decoder_loss: 0.1141 - val_clf_loss: 0.6814 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7748\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3668 - decoder_loss: 0.1142 - clf_loss: 0.2510 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9117 - val_loss: 0.8874 - val_decoder_loss: 0.1132 - val_clf_loss: 0.7759 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7687\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.3637 - decoder_loss: 0.1135 - clf_loss: 0.2486 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9119 - val_loss: 0.8564 - val_decoder_loss: 0.1149 - val_clf_loss: 0.7389 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7667\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.3615 - decoder_loss: 0.1127 - clf_loss: 0.2473 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9118 - val_loss: 0.8228 - val_decoder_loss: 0.1128 - val_clf_loss: 0.7080 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.7737\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 117us/sample - loss: 0.3605 - decoder_loss: 0.1125 - clf_loss: 0.2464 - decoder_accuracy: 0.0083 - clf_accuracy: 0.9116 - val_loss: 0.8371 - val_decoder_loss: 0.1120 - val_clf_loss: 0.7203 - val_decoder_accuracy: 0.0080 - val_clf_accuracy: 0.7721\n",
      "Running sub 9, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[3.585 0.061 1.142 2.381 0.007 0.647 0.228]\n",
      "[4.337 0.034 0.705 3.597 0.007 0.806 0.257]\n",
      "[4.528 0.031 0.528 3.968 0.007 0.846 0.265]\n",
      "[4.568 0.028 0.433 4.106 0.007 0.871 0.289]\n",
      "[4.877 0.027 0.367 4.483 0.007 0.887 0.272]\n",
      "[5.716 0.025 0.334 5.357 0.007 0.893 0.274]\n",
      "[5.77  0.024 0.341 5.405 0.007 0.886 0.262]\n",
      "[6.151 0.023 0.342 5.786 0.007 0.881 0.269]\n",
      "[6.735 0.021 0.319 6.394 0.007 0.889 0.261]\n",
      "[7.345 0.02  0.341 6.983 0.007 0.883 0.254]\n",
      "[8.027 0.019 0.339 7.668 0.007 0.886 0.249]\n",
      "[8.573 0.018 0.376 8.178 0.007 0.874 0.256]\n",
      "[8.881 0.018 0.357 8.505 0.007 0.879 0.24 ]\n",
      "[9.147 0.017 0.328 8.802 0.007 0.89  0.256]\n",
      "[9.504 0.017 0.349 9.138 0.007 0.88  0.239]\n",
      "[0.991 0.041 0.34  0.61  0.007 0.879 0.259]\n",
      "[0.618 0.04  0.302 0.276 0.007 0.896 0.253]\n",
      "[0.474 0.04  0.306 0.128 0.007 0.892 0.245]\n",
      "[0.43  0.041 0.338 0.051 0.007 0.883 0.249]\n",
      "[0.443 0.04  0.384 0.02  0.007 0.872 0.242]\n",
      "[0.497 0.04  0.449 0.008 0.007 0.856 0.22 ]\n",
      "[0.468 0.04  0.425 0.003 0.007 0.863 0.227]\n",
      "[0.451 0.04  0.409 0.002 0.007 0.871 0.239]\n",
      "[0.411 0.04  0.369 0.002 0.007 0.881 0.248]\n",
      "[0.411 0.04  0.369 0.001 0.007 0.883 0.256]\n",
      "[0.453 0.04  0.411 0.001 0.007 0.87  0.242]\n",
      "[0.439 0.04  0.397 0.002 0.007 0.877 0.244]\n",
      "[0.426 0.04  0.384 0.002 0.007 0.88  0.245]\n",
      "[0.391 0.04  0.35  0.002 0.007 0.887 0.247]\n",
      "[0.373 0.04  0.332 0.002 0.007 0.894 0.259]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 115us/sample - loss: 42.5789 - accuracy: 0.3685 - val_loss: 49.3477 - val_accuracy: 0.2232\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 28.1885 - accuracy: 0.6129 - val_loss: 28.6738 - val_accuracy: 0.5956\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 20.7866 - accuracy: 0.7315 - val_loss: 22.9177 - val_accuracy: 0.6746\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 15.9706 - accuracy: 0.7981 - val_loss: 21.0574 - val_accuracy: 0.6983\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 12.6490 - accuracy: 0.8412 - val_loss: 16.0631 - val_accuracy: 0.7794\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 10.5524 - accuracy: 0.8676 - val_loss: 15.8896 - val_accuracy: 0.7844\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 8.6576 - accuracy: 0.8919 - val_loss: 12.2743 - val_accuracy: 0.8322\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 7.7196 - accuracy: 0.9006 - val_loss: 11.1634 - val_accuracy: 0.8511\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.9118 - accuracy: 0.9111 - val_loss: 9.7913 - val_accuracy: 0.8602\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 6.1796 - accuracy: 0.9188 - val_loss: 12.8229 - val_accuracy: 0.8271\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 5.8416 - accuracy: 0.9230 - val_loss: 11.3399 - val_accuracy: 0.8405\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 5.3625 - accuracy: 0.9288 - val_loss: 9.2208 - val_accuracy: 0.8751\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.0128 - accuracy: 0.9325 - val_loss: 9.6898 - val_accuracy: 0.8679\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 4.7937 - accuracy: 0.9354 - val_loss: 10.0241 - val_accuracy: 0.8597\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.4742 - accuracy: 0.9400 - val_loss: 14.1514 - val_accuracy: 0.8149\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 4.3422 - accuracy: 0.9425 - val_loss: 8.6362 - val_accuracy: 0.8775\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.1970 - accuracy: 0.9427 - val_loss: 10.4236 - val_accuracy: 0.8578\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.0275 - accuracy: 0.9448 - val_loss: 8.7816 - val_accuracy: 0.8786\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.8034 - accuracy: 0.9475 - val_loss: 10.1888 - val_accuracy: 0.8619\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 3.7032 - accuracy: 0.9494 - val_loss: 9.9277 - val_accuracy: 0.8621\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 3.5708 - accuracy: 0.9521 - val_loss: 10.4566 - val_accuracy: 0.8594\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.4581 - accuracy: 0.9540 - val_loss: 8.6376 - val_accuracy: 0.8824\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.3793 - accuracy: 0.9538 - val_loss: 6.6710 - val_accuracy: 0.9078\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.3693 - accuracy: 0.9536 - val_loss: 8.0120 - val_accuracy: 0.8910\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 3.2032 - accuracy: 0.9549 - val_loss: 9.7564 - val_accuracy: 0.8684\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 3.1205 - accuracy: 0.9582 - val_loss: 8.5621 - val_accuracy: 0.8871\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.0678 - accuracy: 0.9591 - val_loss: 7.2141 - val_accuracy: 0.9030\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 3.1028 - accuracy: 0.9572 - val_loss: 7.3951 - val_accuracy: 0.8968\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 3.0346 - accuracy: 0.9582 - val_loss: 10.0790 - val_accuracy: 0.8646\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 2.9231 - accuracy: 0.9599 - val_loss: 11.8567 - val_accuracy: 0.8514\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 1.7900 - accuracy: 0.3416 - val_loss: 1.7551 - val_accuracy: 0.3033\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.3974 - accuracy: 0.5025 - val_loss: 1.3501 - val_accuracy: 0.4959\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 1.1924 - accuracy: 0.5604 - val_loss: 1.1817 - val_accuracy: 0.5548\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 1.0398 - accuracy: 0.6109 - val_loss: 1.0898 - val_accuracy: 0.5822\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.9356 - accuracy: 0.6481 - val_loss: 0.9928 - val_accuracy: 0.6100\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.8450 - accuracy: 0.6813 - val_loss: 0.9285 - val_accuracy: 0.6311\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.7623 - accuracy: 0.7156 - val_loss: 0.8602 - val_accuracy: 0.6568\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.6952 - accuracy: 0.7379 - val_loss: 0.8871 - val_accuracy: 0.6392\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.6409 - accuracy: 0.7597 - val_loss: 0.8207 - val_accuracy: 0.6660\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.6120 - accuracy: 0.7726 - val_loss: 0.8079 - val_accuracy: 0.6725\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.5789 - accuracy: 0.7829 - val_loss: 0.7237 - val_accuracy: 0.7116\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.5521 - accuracy: 0.7936 - val_loss: 0.7058 - val_accuracy: 0.7156\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5370 - accuracy: 0.7992 - val_loss: 0.7264 - val_accuracy: 0.7121\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5194 - accuracy: 0.8065 - val_loss: 0.7340 - val_accuracy: 0.7119\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5076 - accuracy: 0.8125 - val_loss: 0.6877 - val_accuracy: 0.7308\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4900 - accuracy: 0.8203 - val_loss: 0.7121 - val_accuracy: 0.7256\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.4742 - accuracy: 0.8265 - val_loss: 0.7424 - val_accuracy: 0.7195\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4669 - accuracy: 0.8297 - val_loss: 0.6978 - val_accuracy: 0.7292\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.4582 - accuracy: 0.8303 - val_loss: 0.6223 - val_accuracy: 0.7638\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 0.4551 - accuracy: 0.8336 - val_loss: 0.6496 - val_accuracy: 0.7498\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.4427 - accuracy: 0.8362 - val_loss: 0.6189 - val_accuracy: 0.7673\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4382 - accuracy: 0.8412 - val_loss: 0.6743 - val_accuracy: 0.7530\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4363 - accuracy: 0.8431 - val_loss: 0.7155 - val_accuracy: 0.7381\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.4271 - accuracy: 0.8437 - val_loss: 0.6609 - val_accuracy: 0.7521\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4239 - accuracy: 0.8463 - val_loss: 0.5953 - val_accuracy: 0.7756\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4187 - accuracy: 0.8463 - val_loss: 0.6506 - val_accuracy: 0.7524\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.4183 - accuracy: 0.8458 - val_loss: 0.6406 - val_accuracy: 0.7635\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4123 - accuracy: 0.8499 - val_loss: 0.6221 - val_accuracy: 0.7660\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.4049 - accuracy: 0.8515 - val_loss: 0.6137 - val_accuracy: 0.7757\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.4027 - accuracy: 0.8520 - val_loss: 0.5639 - val_accuracy: 0.7863\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 97us/sample - loss: 1.2565 - accuracy: 0.6513 - val_loss: 1.7077 - val_accuracy: 0.3587\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.7089 - accuracy: 0.8492 - val_loss: 0.7923 - val_accuracy: 0.7633\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.4640 - accuracy: 0.8880 - val_loss: 0.5447 - val_accuracy: 0.8410\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.3522 - accuracy: 0.9055 - val_loss: 0.5336 - val_accuracy: 0.7962\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.2844 - accuracy: 0.9183 - val_loss: 0.3971 - val_accuracy: 0.8610\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2459 - accuracy: 0.9263 - val_loss: 0.3554 - val_accuracy: 0.8787\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.2238 - accuracy: 0.9303 - val_loss: 0.3733 - val_accuracy: 0.8625\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.2099 - accuracy: 0.9334 - val_loss: 0.3409 - val_accuracy: 0.8694\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1946 - accuracy: 0.9373 - val_loss: 0.3127 - val_accuracy: 0.8895\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1834 - accuracy: 0.9389 - val_loss: 0.6052 - val_accuracy: 0.7933\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1724 - accuracy: 0.9436 - val_loss: 0.3077 - val_accuracy: 0.8878\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1660 - accuracy: 0.9454 - val_loss: 0.4062 - val_accuracy: 0.8543\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1605 - accuracy: 0.9462 - val_loss: 0.2810 - val_accuracy: 0.9006\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1537 - accuracy: 0.9481 - val_loss: 0.2909 - val_accuracy: 0.8956\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.1493 - accuracy: 0.9492 - val_loss: 0.2979 - val_accuracy: 0.8922\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1412 - accuracy: 0.9523 - val_loss: 0.3119 - val_accuracy: 0.8822\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.1375 - accuracy: 0.9526 - val_loss: 0.3126 - val_accuracy: 0.8843\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.1362 - accuracy: 0.9529 - val_loss: 0.2961 - val_accuracy: 0.8917\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1300 - accuracy: 0.9540 - val_loss: 0.3531 - val_accuracy: 0.8775\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.1278 - accuracy: 0.9558 - val_loss: 0.3436 - val_accuracy: 0.8817\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1249 - accuracy: 0.9563 - val_loss: 0.3493 - val_accuracy: 0.8781\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.1193 - accuracy: 0.9586 - val_loss: 0.2972 - val_accuracy: 0.9014\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1218 - accuracy: 0.9587 - val_loss: 0.3857 - val_accuracy: 0.8673\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.1203 - accuracy: 0.9579 - val_loss: 0.2576 - val_accuracy: 0.9106\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1150 - accuracy: 0.9597 - val_loss: 0.3819 - val_accuracy: 0.8717\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.1140 - accuracy: 0.9591 - val_loss: 0.3158 - val_accuracy: 0.8910\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1104 - accuracy: 0.9624 - val_loss: 0.3158 - val_accuracy: 0.8941\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1090 - accuracy: 0.9625 - val_loss: 0.2842 - val_accuracy: 0.9027\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.1090 - accuracy: 0.9621 - val_loss: 0.2574 - val_accuracy: 0.9149\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1038 - accuracy: 0.9638 - val_loss: 0.3281 - val_accuracy: 0.8856\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 182us/sample - loss: 1.2386 - decoder_loss: 0.2326 - clf_loss: 1.0044 - decoder_accuracy: 0.0066 - clf_accuracy: 0.6769 - val_loss: 1.1385 - val_decoder_loss: 0.1743 - val_clf_loss: 0.9637 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.7295\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.6299 - decoder_loss: 0.1613 - clf_loss: 0.4671 - decoder_accuracy: 0.0069 - clf_accuracy: 0.8717 - val_loss: 0.6681 - val_decoder_loss: 0.1442 - val_clf_loss: 0.5211 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8216\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.4743 - decoder_loss: 0.1539 - clf_loss: 0.3188 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9042 - val_loss: 0.6380 - val_decoder_loss: 0.1407 - val_clf_loss: 0.4931 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8171\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4102 - decoder_loss: 0.1482 - clf_loss: 0.2603 - decoder_accuracy: 0.0071 - clf_accuracy: 0.9178 - val_loss: 0.5275 - val_decoder_loss: 0.1409 - val_clf_loss: 0.3879 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8586\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3784 - decoder_loss: 0.1441 - clf_loss: 0.2328 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9248 - val_loss: 0.5116 - val_decoder_loss: 0.1321 - val_clf_loss: 0.3738 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8644\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3490 - decoder_loss: 0.1390 - clf_loss: 0.2083 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9319 - val_loss: 0.5127 - val_decoder_loss: 0.1272 - val_clf_loss: 0.3827 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8657\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3349 - decoder_loss: 0.1341 - clf_loss: 0.1993 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9342 - val_loss: 0.4497 - val_decoder_loss: 0.1238 - val_clf_loss: 0.3248 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8848\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3162 - decoder_loss: 0.1306 - clf_loss: 0.1840 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9398 - val_loss: 0.4619 - val_decoder_loss: 0.1252 - val_clf_loss: 0.3324 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8786\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3011 - decoder_loss: 0.1267 - clf_loss: 0.1729 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9413 - val_loss: 0.4661 - val_decoder_loss: 0.1204 - val_clf_loss: 0.3443 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8698\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.2930 - decoder_loss: 0.1238 - clf_loss: 0.1677 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9440 - val_loss: 0.4887 - val_decoder_loss: 0.1193 - val_clf_loss: 0.3716 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8606\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.2809 - decoder_loss: 0.1217 - clf_loss: 0.1577 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9485 - val_loss: 0.4178 - val_decoder_loss: 0.1161 - val_clf_loss: 0.3003 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8883\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.2771 - decoder_loss: 0.1192 - clf_loss: 0.1562 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9457 - val_loss: 0.5037 - val_decoder_loss: 0.1154 - val_clf_loss: 0.3908 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8581\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.2678 - decoder_loss: 0.1175 - clf_loss: 0.1488 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9519 - val_loss: 0.4271 - val_decoder_loss: 0.1123 - val_clf_loss: 0.3106 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8840\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.2654 - decoder_loss: 0.1156 - clf_loss: 0.1483 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9499 - val_loss: 0.4504 - val_decoder_loss: 0.1122 - val_clf_loss: 0.3356 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8787\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.2581 - decoder_loss: 0.1138 - clf_loss: 0.1428 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9504 - val_loss: 0.5523 - val_decoder_loss: 0.1128 - val_clf_loss: 0.4413 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8408\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.2546 - decoder_loss: 0.1126 - clf_loss: 0.1405 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9534 - val_loss: 0.5091 - val_decoder_loss: 0.1107 - val_clf_loss: 0.3969 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8644\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2495 - decoder_loss: 0.1116 - clf_loss: 0.1364 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9536 - val_loss: 0.4290 - val_decoder_loss: 0.1085 - val_clf_loss: 0.3206 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8824\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2450 - decoder_loss: 0.1102 - clf_loss: 0.1333 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9561 - val_loss: 0.4400 - val_decoder_loss: 0.1060 - val_clf_loss: 0.3373 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8786\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2371 - decoder_loss: 0.1094 - clf_loss: 0.1262 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9576 - val_loss: 0.4752 - val_decoder_loss: 0.1083 - val_clf_loss: 0.3628 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8697\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2378 - decoder_loss: 0.1083 - clf_loss: 0.1280 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9565 - val_loss: 0.5642 - val_decoder_loss: 0.1044 - val_clf_loss: 0.4525 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8387\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.2312 - decoder_loss: 0.1075 - clf_loss: 0.1223 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9589 - val_loss: 0.4334 - val_decoder_loss: 0.1066 - val_clf_loss: 0.3247 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8765\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.2309 - decoder_loss: 0.1065 - clf_loss: 0.1230 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9581 - val_loss: 0.4598 - val_decoder_loss: 0.1045 - val_clf_loss: 0.3609 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8706\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 115us/sample - loss: 0.2297 - decoder_loss: 0.1060 - clf_loss: 0.1222 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9592 - val_loss: 0.4776 - val_decoder_loss: 0.1063 - val_clf_loss: 0.3671 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8727\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.2254 - decoder_loss: 0.1052 - clf_loss: 0.1187 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9604 - val_loss: 0.4595 - val_decoder_loss: 0.1027 - val_clf_loss: 0.3573 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8759\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2205 - decoder_loss: 0.1043 - clf_loss: 0.1147 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9615 - val_loss: 0.4266 - val_decoder_loss: 0.1022 - val_clf_loss: 0.3197 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8775\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.2175 - decoder_loss: 0.1038 - clf_loss: 0.1121 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9613 - val_loss: 0.3835 - val_decoder_loss: 0.1010 - val_clf_loss: 0.2850 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8992\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.2179 - decoder_loss: 0.1028 - clf_loss: 0.1137 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9620 - val_loss: 0.3879 - val_decoder_loss: 0.1020 - val_clf_loss: 0.2846 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8995\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.2121 - decoder_loss: 0.1021 - clf_loss: 0.1085 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9628 - val_loss: 0.4650 - val_decoder_loss: 0.1031 - val_clf_loss: 0.3644 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8775\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.2104 - decoder_loss: 0.1017 - clf_loss: 0.1072 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9630 - val_loss: 0.4964 - val_decoder_loss: 0.1021 - val_clf_loss: 0.3911 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8649\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2097 - decoder_loss: 0.1013 - clf_loss: 0.1070 - decoder_accuracy: 0.0072 - clf_accuracy: 0.9632 - val_loss: 0.4291 - val_decoder_loss: 0.1012 - val_clf_loss: 0.3273 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8802\n",
      "Running sub 10, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.099 0.061 1.12  2.918 0.007 0.601 0.331]\n",
      "[5.151 0.038 0.761 4.351 0.007 0.734 0.311]\n",
      "[5.873 0.033 0.601 5.238 0.007 0.785 0.351]\n",
      "[5.475 0.03  0.407 5.037 0.007 0.861 0.313]\n",
      "[5.798 0.027 0.35  5.42  0.007 0.879 0.329]\n",
      "[6.203 0.025 0.328 5.849 0.007 0.882 0.308]\n",
      "[6.644 0.023 0.298 6.323 0.007 0.89  0.304]\n",
      "[6.952 0.022 0.273 6.656 0.007 0.895 0.296]\n",
      "[7.354 0.021 0.272 7.06  0.007 0.895 0.293]\n",
      "[7.634 0.02  0.259 7.354 0.007 0.904 0.287]\n",
      "[8.243 0.019 0.258 7.965 0.007 0.903 0.293]\n",
      "[9.144 0.018 0.258 8.868 0.007 0.903 0.298]\n",
      "[9.605 0.018 0.26  9.327 0.007 0.902 0.319]\n",
      "[10.102  0.017  0.255  9.829  0.007  0.903  0.305]\n",
      "[10.415  0.017  0.256 10.142  0.007  0.907  0.312]\n",
      "[0.888 0.046 0.252 0.59  0.007 0.905 0.282]\n",
      "[0.595 0.046 0.247 0.301 0.007 0.909 0.317]\n",
      "[0.443 0.046 0.239 0.158 0.007 0.916 0.318]\n",
      "[0.377 0.045 0.25  0.082 0.007 0.913 0.316]\n",
      "[0.335 0.046 0.247 0.041 0.007 0.914 0.313]\n",
      "[0.308 0.046 0.236 0.025 0.007 0.916 0.304]\n",
      "[0.295 0.045 0.231 0.019 0.007 0.92  0.3  ]\n",
      "[0.297 0.045 0.233 0.018 0.007 0.922 0.32 ]\n",
      "[0.285 0.045 0.226 0.014 0.007 0.923 0.325]\n",
      "[0.286 0.046 0.225 0.015 0.007 0.918 0.306]\n",
      "[0.285 0.045 0.226 0.013 0.007 0.922 0.311]\n",
      "[0.275 0.045 0.22  0.01  0.007 0.924 0.317]\n",
      "[0.283 0.045 0.228 0.01  0.007 0.923 0.307]\n",
      "[0.289 0.045 0.236 0.007 0.007 0.92  0.313]\n",
      "[0.276 0.045 0.224 0.007 0.007 0.926 0.311]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 40.5080 - accuracy: 0.3653 - val_loss: 43.5400 - val_accuracy: 0.2875\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 27.9492 - accuracy: 0.5600 - val_loss: 25.6131 - val_accuracy: 0.6119\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 21.4163 - accuracy: 0.6800 - val_loss: 18.5511 - val_accuracy: 0.7406\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 82us/sample - loss: 16.3218 - accuracy: 0.7666 - val_loss: 14.1929 - val_accuracy: 0.8040\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 12.8927 - accuracy: 0.8138 - val_loss: 10.8855 - val_accuracy: 0.8570\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 10.8236 - accuracy: 0.8406 - val_loss: 9.7955 - val_accuracy: 0.8643\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 9.4818 - accuracy: 0.8565 - val_loss: 9.4386 - val_accuracy: 0.8649\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 8.4932 - accuracy: 0.8709 - val_loss: 7.9476 - val_accuracy: 0.8946\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 7.9347 - accuracy: 0.8753 - val_loss: 7.8122 - val_accuracy: 0.8862\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.3320 - accuracy: 0.8865 - val_loss: 7.6377 - val_accuracy: 0.8903\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 6.9548 - accuracy: 0.8898 - val_loss: 7.1091 - val_accuracy: 0.8908\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 6.6192 - accuracy: 0.8944 - val_loss: 6.6835 - val_accuracy: 0.9044\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 6.3945 - accuracy: 0.8991 - val_loss: 6.4977 - val_accuracy: 0.8986\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.1482 - accuracy: 0.9023 - val_loss: 6.3766 - val_accuracy: 0.9048\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 5.9162 - accuracy: 0.9062 - val_loss: 7.0640 - val_accuracy: 0.9040\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.6622 - accuracy: 0.9111 - val_loss: 8.2827 - val_accuracy: 0.8829\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 5.3866 - accuracy: 0.9154 - val_loss: 7.2437 - val_accuracy: 0.8830\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 5.3747 - accuracy: 0.9160 - val_loss: 7.7474 - val_accuracy: 0.8892\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 5.2247 - accuracy: 0.9176 - val_loss: 5.9444 - val_accuracy: 0.9125\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 5.2251 - accuracy: 0.9170 - val_loss: 7.2767 - val_accuracy: 0.8971\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.0624 - accuracy: 0.9214 - val_loss: 7.2438 - val_accuracy: 0.9024\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.9758 - accuracy: 0.9226 - val_loss: 6.4094 - val_accuracy: 0.9165\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 4.9259 - accuracy: 0.9214 - val_loss: 6.2393 - val_accuracy: 0.9095\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.6841 - accuracy: 0.9270 - val_loss: 6.2646 - val_accuracy: 0.8992\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 4.5885 - accuracy: 0.9284 - val_loss: 5.7580 - val_accuracy: 0.9183\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.6129 - accuracy: 0.9268 - val_loss: 5.9091 - val_accuracy: 0.9095\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 4.5265 - accuracy: 0.9308 - val_loss: 7.2740 - val_accuracy: 0.8913\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 4.4056 - accuracy: 0.9308 - val_loss: 5.6823 - val_accuracy: 0.9194\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 4.3171 - accuracy: 0.9329 - val_loss: 5.5758 - val_accuracy: 0.9195\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.3239 - accuracy: 0.9332 - val_loss: 5.4134 - val_accuracy: 0.9197\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 1.9057 - accuracy: 0.2109 - val_loss: 1.8313 - val_accuracy: 0.2641\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.5484 - accuracy: 0.3652 - val_loss: 1.5263 - val_accuracy: 0.3502\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.3062 - accuracy: 0.4853 - val_loss: 1.2028 - val_accuracy: 0.5246\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0834 - accuracy: 0.5785 - val_loss: 1.0107 - val_accuracy: 0.6090\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9357 - accuracy: 0.6316 - val_loss: 0.8962 - val_accuracy: 0.6492\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8473 - accuracy: 0.6611 - val_loss: 0.8025 - val_accuracy: 0.6833\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.7797 - accuracy: 0.6873 - val_loss: 0.7717 - val_accuracy: 0.7032\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.7422 - accuracy: 0.6989 - val_loss: 0.7230 - val_accuracy: 0.7117\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.7051 - accuracy: 0.7144 - val_loss: 0.6940 - val_accuracy: 0.7357\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.6788 - accuracy: 0.7231 - val_loss: 0.6732 - val_accuracy: 0.7283\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.6573 - accuracy: 0.7346 - val_loss: 0.6282 - val_accuracy: 0.7525\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.6402 - accuracy: 0.7393 - val_loss: 0.6201 - val_accuracy: 0.7548\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.6258 - accuracy: 0.7446 - val_loss: 0.6399 - val_accuracy: 0.7357\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.6098 - accuracy: 0.7511 - val_loss: 0.5808 - val_accuracy: 0.7733\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 0.5987 - accuracy: 0.7554 - val_loss: 0.5897 - val_accuracy: 0.7705\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5962 - accuracy: 0.7569 - val_loss: 0.5649 - val_accuracy: 0.7837\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.5848 - accuracy: 0.7612 - val_loss: 0.5624 - val_accuracy: 0.7870\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.5688 - accuracy: 0.7644 - val_loss: 0.5621 - val_accuracy: 0.7770\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 0.5622 - accuracy: 0.7665 - val_loss: 0.5211 - val_accuracy: 0.8014\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.5550 - accuracy: 0.7723 - val_loss: 0.5322 - val_accuracy: 0.7948\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 0.5511 - accuracy: 0.7748 - val_loss: 0.5143 - val_accuracy: 0.8044\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.5443 - accuracy: 0.7783 - val_loss: 0.5181 - val_accuracy: 0.8024\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.5289 - accuracy: 0.7869 - val_loss: 0.5272 - val_accuracy: 0.7875\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.5253 - accuracy: 0.7861 - val_loss: 0.5064 - val_accuracy: 0.8076\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.5164 - accuracy: 0.7901 - val_loss: 0.4965 - val_accuracy: 0.8117\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.5071 - accuracy: 0.7934 - val_loss: 0.4856 - val_accuracy: 0.8125\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.4960 - accuracy: 0.7964 - val_loss: 0.4663 - val_accuracy: 0.8222\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4910 - accuracy: 0.8020 - val_loss: 0.4591 - val_accuracy: 0.8232\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4892 - accuracy: 0.8014 - val_loss: 0.4647 - val_accuracy: 0.8175\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.4801 - accuracy: 0.8054 - val_loss: 0.4563 - val_accuracy: 0.8271\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 1.3469 - accuracy: 0.5574 - val_loss: 1.7945 - val_accuracy: 0.2510\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.8120 - accuracy: 0.7742 - val_loss: 0.7854 - val_accuracy: 0.7357\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.5648 - accuracy: 0.8315 - val_loss: 0.4692 - val_accuracy: 0.8581\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.4416 - accuracy: 0.8523 - val_loss: 0.4266 - val_accuracy: 0.8408\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3712 - accuracy: 0.8691 - val_loss: 0.3417 - val_accuracy: 0.8817\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3319 - accuracy: 0.8787 - val_loss: 0.3543 - val_accuracy: 0.8652\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.3025 - accuracy: 0.8907 - val_loss: 0.3469 - val_accuracy: 0.8713\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2841 - accuracy: 0.8945 - val_loss: 0.3019 - val_accuracy: 0.8894\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2680 - accuracy: 0.8990 - val_loss: 0.3167 - val_accuracy: 0.8871\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2534 - accuracy: 0.9029 - val_loss: 0.2799 - val_accuracy: 0.8960\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2407 - accuracy: 0.9065 - val_loss: 0.2880 - val_accuracy: 0.8941\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2308 - accuracy: 0.9123 - val_loss: 0.3156 - val_accuracy: 0.8817\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2303 - accuracy: 0.9106 - val_loss: 0.2644 - val_accuracy: 0.9000\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2204 - accuracy: 0.9133 - val_loss: 0.2463 - val_accuracy: 0.9098\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2127 - accuracy: 0.9165 - val_loss: 0.2322 - val_accuracy: 0.9081\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2068 - accuracy: 0.9179 - val_loss: 0.2581 - val_accuracy: 0.9029\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2031 - accuracy: 0.9208 - val_loss: 0.2567 - val_accuracy: 0.9046\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.1951 - accuracy: 0.9229 - val_loss: 0.2378 - val_accuracy: 0.9138\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1959 - accuracy: 0.9234 - val_loss: 0.2471 - val_accuracy: 0.9098\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1902 - accuracy: 0.9271 - val_loss: 0.2318 - val_accuracy: 0.9129\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1845 - accuracy: 0.9279 - val_loss: 0.2551 - val_accuracy: 0.9008\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1838 - accuracy: 0.9279 - val_loss: 0.2740 - val_accuracy: 0.8957\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1788 - accuracy: 0.9308 - val_loss: 0.2566 - val_accuracy: 0.9111\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1718 - accuracy: 0.9331 - val_loss: 0.2548 - val_accuracy: 0.9051\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.1747 - accuracy: 0.9308 - val_loss: 0.2619 - val_accuracy: 0.9098\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.1701 - accuracy: 0.9331 - val_loss: 0.2721 - val_accuracy: 0.9038\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1689 - accuracy: 0.9343 - val_loss: 0.2484 - val_accuracy: 0.9056\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1622 - accuracy: 0.9366 - val_loss: 0.2307 - val_accuracy: 0.9195\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1597 - accuracy: 0.9366 - val_loss: 0.2375 - val_accuracy: 0.9132\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1605 - accuracy: 0.9386 - val_loss: 0.2305 - val_accuracy: 0.9192\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 193us/sample - loss: 1.3192 - decoder_loss: 0.2324 - clf_loss: 1.0853 - decoder_accuracy: 0.0069 - clf_accuracy: 0.6248 - val_loss: 1.2680 - val_decoder_loss: 0.2079 - val_clf_loss: 1.0570 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.6040\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.7414 - decoder_loss: 0.1743 - clf_loss: 0.5656 - decoder_accuracy: 0.0072 - clf_accuracy: 0.8098 - val_loss: 0.6891 - val_decoder_loss: 0.1878 - val_clf_loss: 0.4990 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8311\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.5814 - decoder_loss: 0.1679 - clf_loss: 0.4120 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8510 - val_loss: 0.5612 - val_decoder_loss: 0.1684 - val_clf_loss: 0.3931 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8578\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.5150 - decoder_loss: 0.1627 - clf_loss: 0.3508 - decoder_accuracy: 0.0075 - clf_accuracy: 0.8702 - val_loss: 0.5354 - val_decoder_loss: 0.1721 - val_clf_loss: 0.3662 - val_decoder_accuracy: 0.0070 - val_clf_accuracy: 0.8729\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.4758 - decoder_loss: 0.1572 - clf_loss: 0.3172 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8816 - val_loss: 0.4783 - val_decoder_loss: 0.1651 - val_clf_loss: 0.3162 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8930\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.4462 - decoder_loss: 0.1517 - clf_loss: 0.2930 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8892 - val_loss: 0.4837 - val_decoder_loss: 0.1602 - val_clf_loss: 0.3204 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8878\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4253 - decoder_loss: 0.1464 - clf_loss: 0.2774 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8929 - val_loss: 0.4362 - val_decoder_loss: 0.1461 - val_clf_loss: 0.2895 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.9011\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4105 - decoder_loss: 0.1429 - clf_loss: 0.2660 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8997 - val_loss: 0.4211 - val_decoder_loss: 0.1448 - val_clf_loss: 0.2761 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9006\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3966 - decoder_loss: 0.1393 - clf_loss: 0.2558 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8994 - val_loss: 0.4140 - val_decoder_loss: 0.1397 - val_clf_loss: 0.2729 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9038\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3855 - decoder_loss: 0.1362 - clf_loss: 0.2478 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9040 - val_loss: 0.3987 - val_decoder_loss: 0.1468 - val_clf_loss: 0.2481 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9063\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3714 - decoder_loss: 0.1339 - clf_loss: 0.2360 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9085 - val_loss: 0.3893 - val_decoder_loss: 0.1328 - val_clf_loss: 0.2531 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9092\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.3633 - decoder_loss: 0.1318 - clf_loss: 0.2300 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9124 - val_loss: 0.3769 - val_decoder_loss: 0.1294 - val_clf_loss: 0.2458 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9148\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3603 - decoder_loss: 0.1297 - clf_loss: 0.2292 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9100 - val_loss: 0.3726 - val_decoder_loss: 0.1314 - val_clf_loss: 0.2404 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9129\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3508 - decoder_loss: 0.1275 - clf_loss: 0.2218 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9134 - val_loss: 0.3574 - val_decoder_loss: 0.1279 - val_clf_loss: 0.2270 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9178\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.3420 - decoder_loss: 0.1255 - clf_loss: 0.2150 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9181 - val_loss: 0.3863 - val_decoder_loss: 0.1327 - val_clf_loss: 0.2522 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9124\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3354 - decoder_loss: 0.1242 - clf_loss: 0.2098 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9187 - val_loss: 0.3702 - val_decoder_loss: 0.1272 - val_clf_loss: 0.2408 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9092\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.3372 - decoder_loss: 0.1230 - clf_loss: 0.2127 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9183 - val_loss: 0.3639 - val_decoder_loss: 0.1242 - val_clf_loss: 0.2380 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9094\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3244 - decoder_loss: 0.1217 - clf_loss: 0.2013 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9227 - val_loss: 0.3655 - val_decoder_loss: 0.1200 - val_clf_loss: 0.2427 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9090\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3254 - decoder_loss: 0.1199 - clf_loss: 0.2040 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9212 - val_loss: 0.3578 - val_decoder_loss: 0.1193 - val_clf_loss: 0.2393 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9138\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3134 - decoder_loss: 0.1193 - clf_loss: 0.1926 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9281 - val_loss: 0.3445 - val_decoder_loss: 0.1201 - val_clf_loss: 0.2237 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9154\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.3104 - decoder_loss: 0.1186 - clf_loss: 0.1904 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9258 - val_loss: 0.3437 - val_decoder_loss: 0.1178 - val_clf_loss: 0.2233 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9121\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3077 - decoder_loss: 0.1171 - clf_loss: 0.1891 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9278 - val_loss: 0.3546 - val_decoder_loss: 0.1198 - val_clf_loss: 0.2313 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9119\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3037 - decoder_loss: 0.1163 - clf_loss: 0.1860 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9288 - val_loss: 0.4043 - val_decoder_loss: 0.1125 - val_clf_loss: 0.2922 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8841\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2999 - decoder_loss: 0.1151 - clf_loss: 0.1833 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9290 - val_loss: 0.3590 - val_decoder_loss: 0.1179 - val_clf_loss: 0.2396 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9140\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.2989 - decoder_loss: 0.1145 - clf_loss: 0.1829 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9302 - val_loss: 0.3325 - val_decoder_loss: 0.1163 - val_clf_loss: 0.2138 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9222\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 117us/sample - loss: 0.2929 - decoder_loss: 0.1140 - clf_loss: 0.1774 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9324 - val_loss: 0.3389 - val_decoder_loss: 0.1143 - val_clf_loss: 0.2253 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9198\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2889 - decoder_loss: 0.1131 - clf_loss: 0.1744 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9332 - val_loss: 0.3607 - val_decoder_loss: 0.1149 - val_clf_loss: 0.2442 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9105\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2913 - decoder_loss: 0.1123 - clf_loss: 0.1775 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9317 - val_loss: 0.3737 - val_decoder_loss: 0.1119 - val_clf_loss: 0.2597 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8911\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2851 - decoder_loss: 0.1117 - clf_loss: 0.1720 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9335 - val_loss: 0.3278 - val_decoder_loss: 0.1145 - val_clf_loss: 0.2105 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9208\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.2781 - decoder_loss: 0.1114 - clf_loss: 0.1652 - decoder_accuracy: 0.0076 - clf_accuracy: 0.9367 - val_loss: 0.3547 - val_decoder_loss: 0.1127 - val_clf_loss: 0.2407 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.9043\n",
      "Running sub 11, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.627 0.056 1.28  3.291 0.008 0.562 0.151]\n",
      "[6.518 0.038 1.106 5.374 0.008 0.635 0.177]\n",
      "[5.804 0.035 0.896 4.872 0.008 0.71  0.189]\n",
      "[6.132 0.031 0.768 5.331 0.008 0.747 0.187]\n",
      "[6.366 0.029 0.7   5.636 0.008 0.772 0.17 ]\n",
      "[6.547 0.028 0.652 5.866 0.008 0.775 0.176]\n",
      "[7.044 0.026 0.642 6.375 0.008 0.785 0.175]\n",
      "[7.318 0.025 0.576 6.716 0.008 0.797 0.173]\n",
      "[8.045 0.023 0.591 7.43  0.008 0.794 0.186]\n",
      "[8.558 0.022 0.572 7.964 0.008 0.803 0.171]\n",
      "[9.247 0.021 0.579 8.647 0.008 0.803 0.171]\n",
      "[9.852 0.019 0.55  9.282 0.008 0.818 0.17 ]\n",
      "[10.211  0.019  0.531  9.661  0.008  0.826  0.167]\n",
      "[10.701  0.018  0.553 10.13   0.008  0.822  0.173]\n",
      "[10.979  0.018  0.534 10.427  0.008  0.829  0.167]\n",
      "[1.56  0.046 0.588 0.926 0.008 0.821 0.135]\n",
      "[1.014 0.046 0.598 0.369 0.008 0.815 0.153]\n",
      "[0.82  0.047 0.572 0.201 0.008 0.821 0.148]\n",
      "[0.704 0.046 0.565 0.092 0.008 0.827 0.139]\n",
      "[0.614 0.046 0.526 0.042 0.008 0.841 0.145]\n",
      "[0.586 0.046 0.52  0.02  0.008 0.838 0.142]\n",
      "[0.589 0.046 0.532 0.011 0.008 0.833 0.151]\n",
      "[0.591 0.046 0.538 0.007 0.008 0.835 0.147]\n",
      "[0.606 0.046 0.555 0.005 0.008 0.834 0.145]\n",
      "[0.618 0.046 0.568 0.004 0.008 0.83  0.147]\n",
      "[0.607 0.046 0.558 0.003 0.008 0.836 0.144]\n",
      "[0.625 0.045 0.575 0.004 0.008 0.833 0.149]\n",
      "[0.636 0.046 0.588 0.003 0.008 0.835 0.15 ]\n",
      "[0.632 0.046 0.584 0.002 0.008 0.836 0.148]\n",
      "[0.665 0.046 0.617 0.002 0.008 0.833 0.149]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 130us/sample - loss: 38.2863 - accuracy: 0.3971 - val_loss: 50.7300 - val_accuracy: 0.2308\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 28.2459 - accuracy: 0.5644 - val_loss: 27.8780 - val_accuracy: 0.5821\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 23.0166 - accuracy: 0.6537 - val_loss: 22.3771 - val_accuracy: 0.6737\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 95us/sample - loss: 18.7008 - accuracy: 0.7275 - val_loss: 20.4804 - val_accuracy: 0.7154\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 15.5494 - accuracy: 0.7794 - val_loss: 20.0946 - val_accuracy: 0.7208\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 13.0478 - accuracy: 0.8273 - val_loss: 15.0274 - val_accuracy: 0.7900\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 11.1892 - accuracy: 0.8537 - val_loss: 13.6424 - val_accuracy: 0.8190\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 9.6069 - accuracy: 0.8751 - val_loss: 12.3865 - val_accuracy: 0.8329\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 8.5313 - accuracy: 0.8873 - val_loss: 14.0155 - val_accuracy: 0.8076\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.7049 - accuracy: 0.8972 - val_loss: 13.8796 - val_accuracy: 0.8211\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 7.2980 - accuracy: 0.9038 - val_loss: 13.9569 - val_accuracy: 0.8313\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 6.8252 - accuracy: 0.9084 - val_loss: 11.0952 - val_accuracy: 0.8527\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 6.4998 - accuracy: 0.9123 - val_loss: 11.5383 - val_accuracy: 0.8471\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 6.1383 - accuracy: 0.9174 - val_loss: 10.6346 - val_accuracy: 0.8568\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.7901 - accuracy: 0.9242 - val_loss: 12.2703 - val_accuracy: 0.8406\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 5.8185 - accuracy: 0.9213 - val_loss: 10.7593 - val_accuracy: 0.8603\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 5.4457 - accuracy: 0.9269 - val_loss: 13.3106 - val_accuracy: 0.8376\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 5.3005 - accuracy: 0.9296 - val_loss: 12.6020 - val_accuracy: 0.8527\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 5.1135 - accuracy: 0.9304 - val_loss: 13.5338 - val_accuracy: 0.8352\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 5.0982 - accuracy: 0.9303 - val_loss: 11.4291 - val_accuracy: 0.8517\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 4.9356 - accuracy: 0.9334 - val_loss: 10.3986 - val_accuracy: 0.8590\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 4.8099 - accuracy: 0.9349 - val_loss: 11.8665 - val_accuracy: 0.8586\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 4.7558 - accuracy: 0.9366 - val_loss: 10.8785 - val_accuracy: 0.8611\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 4.5555 - accuracy: 0.9383 - val_loss: 12.2973 - val_accuracy: 0.8586\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 4.5564 - accuracy: 0.9391 - val_loss: 11.4467 - val_accuracy: 0.8546\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 4.3864 - accuracy: 0.9418 - val_loss: 10.7547 - val_accuracy: 0.8568\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 4.4067 - accuracy: 0.9396 - val_loss: 11.7180 - val_accuracy: 0.8498\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 84us/sample - loss: 4.2987 - accuracy: 0.9414 - val_loss: 11.4994 - val_accuracy: 0.8610\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 4.0896 - accuracy: 0.9439 - val_loss: 13.0068 - val_accuracy: 0.8498\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 4.0639 - accuracy: 0.9456 - val_loss: 13.9892 - val_accuracy: 0.8325\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 94us/sample - loss: 1.7789 - accuracy: 0.3061 - val_loss: 1.6530 - val_accuracy: 0.3549\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 1.4746 - accuracy: 0.4251 - val_loss: 1.4365 - val_accuracy: 0.4637\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.3122 - accuracy: 0.4999 - val_loss: 1.3077 - val_accuracy: 0.5097\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.1828 - accuracy: 0.5421 - val_loss: 1.2194 - val_accuracy: 0.5521\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0931 - accuracy: 0.5735 - val_loss: 1.1899 - val_accuracy: 0.5624\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 1.0441 - accuracy: 0.5877 - val_loss: 1.2741 - val_accuracy: 0.5114\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.0068 - accuracy: 0.6052 - val_loss: 1.1533 - val_accuracy: 0.5824\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.9766 - accuracy: 0.6184 - val_loss: 1.2174 - val_accuracy: 0.5560\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 0.9461 - accuracy: 0.6289 - val_loss: 1.0654 - val_accuracy: 0.5997\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.9172 - accuracy: 0.6444 - val_loss: 1.0743 - val_accuracy: 0.6092\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8911 - accuracy: 0.6536 - val_loss: 1.0697 - val_accuracy: 0.5771\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8627 - accuracy: 0.6662 - val_loss: 1.0374 - val_accuracy: 0.6165\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.8396 - accuracy: 0.6731 - val_loss: 1.0736 - val_accuracy: 0.6065\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.8070 - accuracy: 0.6886 - val_loss: 0.9794 - val_accuracy: 0.6417\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.7821 - accuracy: 0.7013 - val_loss: 0.9624 - val_accuracy: 0.6548\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.7581 - accuracy: 0.7124 - val_loss: 1.0556 - val_accuracy: 0.6478\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.7455 - accuracy: 0.7176 - val_loss: 0.9779 - val_accuracy: 0.6690\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.7301 - accuracy: 0.7241 - val_loss: 0.9730 - val_accuracy: 0.6721\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.7101 - accuracy: 0.7334 - val_loss: 0.9560 - val_accuracy: 0.6894\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.6947 - accuracy: 0.7379 - val_loss: 0.8962 - val_accuracy: 0.6995\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.6827 - accuracy: 0.7443 - val_loss: 1.0264 - val_accuracy: 0.6817\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6674 - accuracy: 0.7508 - val_loss: 1.0102 - val_accuracy: 0.6884\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.6548 - accuracy: 0.7588 - val_loss: 1.0099 - val_accuracy: 0.6573\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.6478 - accuracy: 0.7610 - val_loss: 0.8912 - val_accuracy: 0.7100\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.6330 - accuracy: 0.7660 - val_loss: 0.8930 - val_accuracy: 0.7070\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6136 - accuracy: 0.7741 - val_loss: 0.9044 - val_accuracy: 0.7106\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.6061 - accuracy: 0.7788 - val_loss: 0.8824 - val_accuracy: 0.7198\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5898 - accuracy: 0.7832 - val_loss: 0.8534 - val_accuracy: 0.7197\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.5746 - accuracy: 0.7904 - val_loss: 0.7799 - val_accuracy: 0.7359\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.5719 - accuracy: 0.7921 - val_loss: 0.8895 - val_accuracy: 0.7244\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 88us/sample - loss: 1.3842 - accuracy: 0.5048 - val_loss: 2.2524 - val_accuracy: 0.2346\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.8880 - accuracy: 0.7178 - val_loss: 0.9178 - val_accuracy: 0.6892\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.6551 - accuracy: 0.8064 - val_loss: 0.7258 - val_accuracy: 0.7657\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5192 - accuracy: 0.8412 - val_loss: 0.5577 - val_accuracy: 0.8176\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.4181 - accuracy: 0.8677 - val_loss: 0.4797 - val_accuracy: 0.8344\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3563 - accuracy: 0.8877 - val_loss: 0.4838 - val_accuracy: 0.8303\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3184 - accuracy: 0.8985 - val_loss: 0.6287 - val_accuracy: 0.7871\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2888 - accuracy: 0.9057 - val_loss: 0.4477 - val_accuracy: 0.8441\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.2710 - accuracy: 0.9098 - val_loss: 0.5136 - val_accuracy: 0.8230\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2530 - accuracy: 0.9167 - val_loss: 0.5254 - val_accuracy: 0.8205\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.2440 - accuracy: 0.9194 - val_loss: 0.5295 - val_accuracy: 0.8319\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.2297 - accuracy: 0.9240 - val_loss: 0.4145 - val_accuracy: 0.8602\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2204 - accuracy: 0.9261 - val_loss: 0.5549 - val_accuracy: 0.8387\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.2120 - accuracy: 0.9279 - val_loss: 0.5435 - val_accuracy: 0.8313\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.2081 - accuracy: 0.9290 - val_loss: 0.4891 - val_accuracy: 0.8438\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1994 - accuracy: 0.9319 - val_loss: 0.4814 - val_accuracy: 0.8454\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.1900 - accuracy: 0.9368 - val_loss: 0.5115 - val_accuracy: 0.8421\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1908 - accuracy: 0.9352 - val_loss: 0.4537 - val_accuracy: 0.8583\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1908 - accuracy: 0.9354 - val_loss: 0.4180 - val_accuracy: 0.8671\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.1781 - accuracy: 0.9391 - val_loss: 0.6073 - val_accuracy: 0.8130\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1817 - accuracy: 0.9394 - val_loss: 0.4732 - val_accuracy: 0.8625\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.1683 - accuracy: 0.9416 - val_loss: 0.4766 - val_accuracy: 0.8568\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.1678 - accuracy: 0.9422 - val_loss: 0.5616 - val_accuracy: 0.8378\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.1641 - accuracy: 0.9447 - val_loss: 0.6769 - val_accuracy: 0.8359\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.1698 - accuracy: 0.9420 - val_loss: 0.5229 - val_accuracy: 0.8602\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1659 - accuracy: 0.9428 - val_loss: 0.6012 - val_accuracy: 0.8394\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1636 - accuracy: 0.9451 - val_loss: 0.5135 - val_accuracy: 0.8567\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.1545 - accuracy: 0.9479 - val_loss: 0.4792 - val_accuracy: 0.8592\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.1499 - accuracy: 0.9485 - val_loss: 0.4777 - val_accuracy: 0.8638\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1499 - accuracy: 0.9477 - val_loss: 0.4795 - val_accuracy: 0.8624\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 200us/sample - loss: 1.4897 - decoder_loss: 0.2321 - clf_loss: 1.2562 - decoder_accuracy: 0.0073 - clf_accuracy: 0.5757 - val_loss: 1.2798 - val_decoder_loss: 0.1745 - val_clf_loss: 1.1045 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.6598\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.8435 - decoder_loss: 0.1770 - clf_loss: 0.6650 - decoder_accuracy: 0.0076 - clf_accuracy: 0.7932 - val_loss: 0.8045 - val_decoder_loss: 0.1929 - val_clf_loss: 0.6099 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.7865\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.6564 - decoder_loss: 0.1705 - clf_loss: 0.4844 - decoder_accuracy: 0.0079 - clf_accuracy: 0.8423 - val_loss: 0.6990 - val_decoder_loss: 0.1831 - val_clf_loss: 0.5109 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8159\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.5699 - decoder_loss: 0.1664 - clf_loss: 0.4019 - decoder_accuracy: 0.0081 - clf_accuracy: 0.8658 - val_loss: 0.6497 - val_decoder_loss: 0.1668 - val_clf_loss: 0.4760 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8286\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.5160 - decoder_loss: 0.1627 - clf_loss: 0.3517 - decoder_accuracy: 0.0081 - clf_accuracy: 0.8820 - val_loss: 0.6139 - val_decoder_loss: 0.1662 - val_clf_loss: 0.4457 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8370\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.4813 - decoder_loss: 0.1583 - clf_loss: 0.3213 - decoder_accuracy: 0.0081 - clf_accuracy: 0.8909 - val_loss: 0.5975 - val_decoder_loss: 0.1619 - val_clf_loss: 0.4372 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8378\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.4505 - decoder_loss: 0.1538 - clf_loss: 0.2951 - decoder_accuracy: 0.0081 - clf_accuracy: 0.9005 - val_loss: 0.5749 - val_decoder_loss: 0.1586 - val_clf_loss: 0.4115 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.8470\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4287 - decoder_loss: 0.1493 - clf_loss: 0.2777 - decoder_accuracy: 0.0081 - clf_accuracy: 0.9063 - val_loss: 0.5895 - val_decoder_loss: 0.1567 - val_clf_loss: 0.4270 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8417\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 117us/sample - loss: 0.4079 - decoder_loss: 0.1452 - clf_loss: 0.2610 - decoder_accuracy: 0.0081 - clf_accuracy: 0.9138 - val_loss: 0.5965 - val_decoder_loss: 0.1451 - val_clf_loss: 0.4499 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8440\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3956 - decoder_loss: 0.1412 - clf_loss: 0.2528 - decoder_accuracy: 0.0081 - clf_accuracy: 0.9154 - val_loss: 0.5385 - val_decoder_loss: 0.1385 - val_clf_loss: 0.3964 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8552\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.3745 - decoder_loss: 0.1376 - clf_loss: 0.2354 - decoder_accuracy: 0.0081 - clf_accuracy: 0.9203 - val_loss: 0.5560 - val_decoder_loss: 0.1400 - val_clf_loss: 0.4167 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8537\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3683 - decoder_loss: 0.1349 - clf_loss: 0.2318 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9211 - val_loss: 0.5211 - val_decoder_loss: 0.1352 - val_clf_loss: 0.3836 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8654\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.3534 - decoder_loss: 0.1324 - clf_loss: 0.2193 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9267 - val_loss: 0.5293 - val_decoder_loss: 0.1302 - val_clf_loss: 0.3954 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8548\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3450 - decoder_loss: 0.1302 - clf_loss: 0.2132 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9263 - val_loss: 0.5656 - val_decoder_loss: 0.1343 - val_clf_loss: 0.4252 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8559\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.3419 - decoder_loss: 0.1280 - clf_loss: 0.2123 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9267 - val_loss: 0.6210 - val_decoder_loss: 0.1314 - val_clf_loss: 0.4864 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8381\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.3321 - decoder_loss: 0.1264 - clf_loss: 0.2041 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9307 - val_loss: 0.5269 - val_decoder_loss: 0.1273 - val_clf_loss: 0.3989 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8621\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3247 - decoder_loss: 0.1249 - clf_loss: 0.1982 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9327 - val_loss: 0.5474 - val_decoder_loss: 0.1273 - val_clf_loss: 0.4144 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8605\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3149 - decoder_loss: 0.1230 - clf_loss: 0.1903 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9356 - val_loss: 0.5537 - val_decoder_loss: 0.1259 - val_clf_loss: 0.4261 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8535\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 102us/sample - loss: 0.3146 - decoder_loss: 0.1218 - clf_loss: 0.1912 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9355 - val_loss: 0.5309 - val_decoder_loss: 0.1199 - val_clf_loss: 0.4109 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8713\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.3084 - decoder_loss: 0.1204 - clf_loss: 0.1863 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9364 - val_loss: 0.5915 - val_decoder_loss: 0.1194 - val_clf_loss: 0.4720 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.8522\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3027 - decoder_loss: 0.1191 - clf_loss: 0.1820 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9385 - val_loss: 0.5667 - val_decoder_loss: 0.1206 - val_clf_loss: 0.4472 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8660\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 115us/sample - loss: 0.2990 - decoder_loss: 0.1181 - clf_loss: 0.1794 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9401 - val_loss: 0.5648 - val_decoder_loss: 0.1209 - val_clf_loss: 0.4452 - val_decoder_accuracy: 0.0077 - val_clf_accuracy: 0.8586\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.2944 - decoder_loss: 0.1170 - clf_loss: 0.1757 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9411 - val_loss: 0.6052 - val_decoder_loss: 0.1158 - val_clf_loss: 0.4852 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8576\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2879 - decoder_loss: 0.1162 - clf_loss: 0.1701 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9431 - val_loss: 0.4633 - val_decoder_loss: 0.1151 - val_clf_loss: 0.3507 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8825\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2872 - decoder_loss: 0.1155 - clf_loss: 0.1701 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9425 - val_loss: 0.5796 - val_decoder_loss: 0.1162 - val_clf_loss: 0.4581 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8551\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 122us/sample - loss: 0.2827 - decoder_loss: 0.1148 - clf_loss: 0.1663 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9434 - val_loss: 0.5668 - val_decoder_loss: 0.1153 - val_clf_loss: 0.4528 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8502\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.2777 - decoder_loss: 0.1142 - clf_loss: 0.1619 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9457 - val_loss: 0.5278 - val_decoder_loss: 0.1145 - val_clf_loss: 0.4117 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8602\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.2719 - decoder_loss: 0.1133 - clf_loss: 0.1570 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9464 - val_loss: 0.5708 - val_decoder_loss: 0.1135 - val_clf_loss: 0.4553 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8544\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.2700 - decoder_loss: 0.1127 - clf_loss: 0.1557 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9477 - val_loss: 0.5106 - val_decoder_loss: 0.1126 - val_clf_loss: 0.3991 - val_decoder_accuracy: 0.0079 - val_clf_accuracy: 0.8621\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2712 - decoder_loss: 0.1119 - clf_loss: 0.1577 - decoder_accuracy: 0.0082 - clf_accuracy: 0.9469 - val_loss: 0.5399 - val_decoder_loss: 0.1112 - val_clf_loss: 0.4248 - val_decoder_accuracy: 0.0078 - val_clf_accuracy: 0.8610\n",
      "Running sub 12, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.204 0.055 1.281 2.867 0.008 0.485 0.183]\n",
      "[8.781 0.039 1.072 7.67  0.007 0.571 0.226]\n",
      "[9.48  0.035 0.986 8.458 0.008 0.599 0.351]\n",
      "[10.081  0.033  0.866  9.182  0.008  0.638  0.341]\n",
      "[10.868  0.03   0.819 10.017  0.008  0.656  0.334]\n",
      "[9.319 0.028 0.796 8.494 0.008 0.679 0.327]\n",
      "[8.272 0.027 0.779 7.466 0.008 0.702 0.317]\n",
      "[8.273 0.026 0.791 7.455 0.008 0.714 0.28 ]\n",
      "[8.267 0.025 0.785 7.455 0.008 0.712 0.246]\n",
      "[8.936 0.024 0.772 8.139 0.008 0.723 0.273]\n",
      "[8.798 0.024 0.781 7.992 0.008 0.723 0.274]\n",
      "[8.943 0.023 0.825 8.095 0.008 0.716 0.304]\n",
      "[9.337 0.022 0.849 8.466 0.008 0.711 0.301]\n",
      "[9.747 0.021 0.893 8.833 0.008 0.706 0.315]\n",
      "[9.991 0.021 0.928 9.041 0.008 0.701 0.319]\n",
      "[1.52  0.045 0.918 0.557 0.008 0.694 0.307]\n",
      "[1.162 0.044 0.899 0.22  0.008 0.693 0.342]\n",
      "[1.061 0.044 0.904 0.114 0.008 0.69  0.34 ]\n",
      "[1.001 0.044 0.903 0.053 0.008 0.695 0.342]\n",
      "[0.975 0.044 0.901 0.03  0.008 0.696 0.332]\n",
      "[0.998 0.044 0.938 0.016 0.008 0.693 0.321]\n",
      "[1.395 0.045 0.877 0.474 0.008 0.705 0.329]\n",
      "[1.757 0.044 0.93  0.783 0.008 0.7   0.331]\n",
      "[8.753 0.044 0.926 7.783 0.008 0.701 0.328]\n",
      "[23.261  0.045  0.912 22.304  0.008  0.707  0.325]\n",
      "[1.01  0.044 0.961 0.005 0.008 0.699 0.294]\n",
      "[0.908 0.044 0.862 0.002 0.008 0.706 0.37 ]\n",
      "[0.911 0.044 0.866 0.001 0.008 0.715 0.367]\n",
      "[0.966 0.044 0.921 0.001 0.008 0.714 0.367]\n",
      "[0.931 0.044 0.887 0.001 0.008 0.72  0.355]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 122us/sample - loss: 41.1133 - accuracy: 0.3488 - val_loss: 51.3415 - val_accuracy: 0.1997\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 31.1415 - accuracy: 0.5252 - val_loss: 32.3432 - val_accuracy: 0.4951\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 25.4001 - accuracy: 0.6183 - val_loss: 25.6451 - val_accuracy: 0.5937\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 21.3101 - accuracy: 0.6796 - val_loss: 24.1217 - val_accuracy: 0.6151\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 18.1245 - accuracy: 0.7302 - val_loss: 23.8719 - val_accuracy: 0.6330\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 16.2085 - accuracy: 0.7582 - val_loss: 20.1314 - val_accuracy: 0.6771\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 14.6536 - accuracy: 0.7812 - val_loss: 19.2155 - val_accuracy: 0.6976\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 13.7933 - accuracy: 0.7938 - val_loss: 18.4666 - val_accuracy: 0.7010\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 12.9224 - accuracy: 0.8025 - val_loss: 18.9616 - val_accuracy: 0.6960\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 12.3399 - accuracy: 0.8145 - val_loss: 18.3220 - val_accuracy: 0.7032\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 11.6978 - accuracy: 0.8215 - val_loss: 20.3646 - val_accuracy: 0.6790\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 11.4101 - accuracy: 0.8241 - val_loss: 20.3150 - val_accuracy: 0.6844\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 11.0636 - accuracy: 0.8333 - val_loss: 20.5163 - val_accuracy: 0.6824\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 10.7086 - accuracy: 0.8350 - val_loss: 18.0767 - val_accuracy: 0.7094\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 10.3270 - accuracy: 0.8429 - val_loss: 18.1933 - val_accuracy: 0.7103\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 10.0814 - accuracy: 0.8447 - val_loss: 18.7806 - val_accuracy: 0.7137\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 9.7419 - accuracy: 0.8505 - val_loss: 18.0308 - val_accuracy: 0.7160\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 84us/sample - loss: 9.5740 - accuracy: 0.8531 - val_loss: 16.7575 - val_accuracy: 0.7310\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 75us/sample - loss: 9.3129 - accuracy: 0.8552 - val_loss: 17.2984 - val_accuracy: 0.7327\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 8.9009 - accuracy: 0.8636 - val_loss: 20.2935 - val_accuracy: 0.6952\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.9765 - accuracy: 0.8623 - val_loss: 19.9080 - val_accuracy: 0.7044\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 8.8451 - accuracy: 0.8631 - val_loss: 18.7284 - val_accuracy: 0.7154\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.5403 - accuracy: 0.8679 - val_loss: 19.6477 - val_accuracy: 0.7041\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 8.4758 - accuracy: 0.8715 - val_loss: 19.0713 - val_accuracy: 0.7027\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 8.3464 - accuracy: 0.8722 - val_loss: 18.0732 - val_accuracy: 0.7192\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 8.2488 - accuracy: 0.8724 - val_loss: 19.3695 - val_accuracy: 0.7211\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 8.0243 - accuracy: 0.8790 - val_loss: 19.4224 - val_accuracy: 0.7065\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 7.8825 - accuracy: 0.8802 - val_loss: 19.7955 - val_accuracy: 0.7100\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 7.9964 - accuracy: 0.8785 - val_loss: 19.3094 - val_accuracy: 0.7144\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 7.7951 - accuracy: 0.8805 - val_loss: 18.1941 - val_accuracy: 0.7386\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 1.9110 - accuracy: 0.2456 - val_loss: 1.8804 - val_accuracy: 0.2451\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.5871 - accuracy: 0.3739 - val_loss: 1.6939 - val_accuracy: 0.3540\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.4013 - accuracy: 0.4477 - val_loss: 1.4918 - val_accuracy: 0.3903\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.2851 - accuracy: 0.4856 - val_loss: 1.4148 - val_accuracy: 0.4127\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 1.1962 - accuracy: 0.5257 - val_loss: 1.3630 - val_accuracy: 0.4273\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.1161 - accuracy: 0.5590 - val_loss: 1.2785 - val_accuracy: 0.4721\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 1.0604 - accuracy: 0.5825 - val_loss: 1.2784 - val_accuracy: 0.4921\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.0103 - accuracy: 0.6031 - val_loss: 1.2272 - val_accuracy: 0.5103\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.9740 - accuracy: 0.6181 - val_loss: 1.1834 - val_accuracy: 0.5254\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.9455 - accuracy: 0.6317 - val_loss: 1.1659 - val_accuracy: 0.5319\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.9240 - accuracy: 0.6344 - val_loss: 1.1619 - val_accuracy: 0.5416\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.9170 - accuracy: 0.6410 - val_loss: 1.1324 - val_accuracy: 0.5503\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9086 - accuracy: 0.6430 - val_loss: 1.1103 - val_accuracy: 0.5652\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.8894 - accuracy: 0.6496 - val_loss: 1.1310 - val_accuracy: 0.5452\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.8793 - accuracy: 0.6550 - val_loss: 1.1144 - val_accuracy: 0.5508\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.8799 - accuracy: 0.6555 - val_loss: 1.1192 - val_accuracy: 0.5549\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.8702 - accuracy: 0.6589 - val_loss: 1.2272 - val_accuracy: 0.5360\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.8579 - accuracy: 0.6656 - val_loss: 1.0937 - val_accuracy: 0.5622\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.8562 - accuracy: 0.6635 - val_loss: 1.1482 - val_accuracy: 0.5494\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.8568 - accuracy: 0.6640 - val_loss: 1.0870 - val_accuracy: 0.5786\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.8492 - accuracy: 0.6667 - val_loss: 1.0755 - val_accuracy: 0.5781\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.8471 - accuracy: 0.6679 - val_loss: 1.0894 - val_accuracy: 0.5752\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.8416 - accuracy: 0.6670 - val_loss: 1.1188 - val_accuracy: 0.5598\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8378 - accuracy: 0.6695 - val_loss: 1.1030 - val_accuracy: 0.5605\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.8338 - accuracy: 0.6722 - val_loss: 1.1395 - val_accuracy: 0.5668\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.8296 - accuracy: 0.6771 - val_loss: 1.1692 - val_accuracy: 0.5613\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8286 - accuracy: 0.6770 - val_loss: 1.0964 - val_accuracy: 0.5687\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.8252 - accuracy: 0.6786 - val_loss: 1.1660 - val_accuracy: 0.5557\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.8247 - accuracy: 0.6775 - val_loss: 1.1202 - val_accuracy: 0.5665\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.8144 - accuracy: 0.6817 - val_loss: 1.0900 - val_accuracy: 0.5833\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 96us/sample - loss: 1.5322 - accuracy: 0.4108 - val_loss: 1.8323 - val_accuracy: 0.2435\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 1.0834 - accuracy: 0.6354 - val_loss: 1.2418 - val_accuracy: 0.5263\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8244 - accuracy: 0.7313 - val_loss: 0.9721 - val_accuracy: 0.6530\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.6819 - accuracy: 0.7704 - val_loss: 0.9288 - val_accuracy: 0.6421\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.6040 - accuracy: 0.7910 - val_loss: 0.9862 - val_accuracy: 0.6303\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.5531 - accuracy: 0.8035 - val_loss: 0.9236 - val_accuracy: 0.6614\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.5175 - accuracy: 0.8166 - val_loss: 0.9262 - val_accuracy: 0.6651\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4989 - accuracy: 0.8196 - val_loss: 0.7935 - val_accuracy: 0.7014\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4822 - accuracy: 0.8257 - val_loss: 0.7459 - val_accuracy: 0.7243\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4575 - accuracy: 0.8335 - val_loss: 0.7926 - val_accuracy: 0.7183\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4330 - accuracy: 0.8437 - val_loss: 0.8506 - val_accuracy: 0.6959\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4294 - accuracy: 0.8424 - val_loss: 0.7842 - val_accuracy: 0.7144\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4199 - accuracy: 0.8463 - val_loss: 0.7556 - val_accuracy: 0.7235\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4053 - accuracy: 0.8502 - val_loss: 0.7985 - val_accuracy: 0.7110\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3956 - accuracy: 0.8561 - val_loss: 0.8077 - val_accuracy: 0.7260\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3818 - accuracy: 0.8618 - val_loss: 0.7311 - val_accuracy: 0.7337\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.3791 - accuracy: 0.8605 - val_loss: 0.7853 - val_accuracy: 0.7279\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3668 - accuracy: 0.8660 - val_loss: 0.7781 - val_accuracy: 0.7227\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3594 - accuracy: 0.8674 - val_loss: 0.7593 - val_accuracy: 0.7387\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3626 - accuracy: 0.8654 - val_loss: 0.7857 - val_accuracy: 0.7330\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3512 - accuracy: 0.8677 - val_loss: 0.8381 - val_accuracy: 0.7165\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3509 - accuracy: 0.8698 - val_loss: 0.8021 - val_accuracy: 0.7263\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3427 - accuracy: 0.8737 - val_loss: 0.8287 - val_accuracy: 0.7335\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.3303 - accuracy: 0.8781 - val_loss: 0.8260 - val_accuracy: 0.7303\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 48us/sample - loss: 0.3281 - accuracy: 0.8790 - val_loss: 0.8689 - val_accuracy: 0.7103\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3222 - accuracy: 0.8827 - val_loss: 0.7587 - val_accuracy: 0.7490\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.3239 - accuracy: 0.8804 - val_loss: 0.7880 - val_accuracy: 0.7414\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3186 - accuracy: 0.8826 - val_loss: 0.8457 - val_accuracy: 0.7217\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.3095 - accuracy: 0.8871 - val_loss: 0.8508 - val_accuracy: 0.7230\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3126 - accuracy: 0.8853 - val_loss: 0.8677 - val_accuracy: 0.7237\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 200us/sample - loss: 1.5152 - decoder_loss: 0.2465 - clf_loss: 1.2671 - decoder_accuracy: 0.0071 - clf_accuracy: 0.5313 - val_loss: 1.5517 - val_decoder_loss: 0.2080 - val_clf_loss: 1.3402 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.5222\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.9628 - decoder_loss: 0.1731 - clf_loss: 0.7883 - decoder_accuracy: 0.0073 - clf_accuracy: 0.7243 - val_loss: 1.1016 - val_decoder_loss: 0.1778 - val_clf_loss: 0.9215 - val_decoder_accuracy: 0.0074 - val_clf_accuracy: 0.6316\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.7828 - decoder_loss: 0.1604 - clf_loss: 0.6208 - decoder_accuracy: 0.0075 - clf_accuracy: 0.7783 - val_loss: 0.9590 - val_decoder_loss: 0.1679 - val_clf_loss: 0.7861 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.6738\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.6852 - decoder_loss: 0.1545 - clf_loss: 0.5291 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8093 - val_loss: 0.8747 - val_decoder_loss: 0.1543 - val_clf_loss: 0.7162 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.6992\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.6419 - decoder_loss: 0.1502 - clf_loss: 0.4903 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8214 - val_loss: 0.9276 - val_decoder_loss: 0.1562 - val_clf_loss: 0.7703 - val_decoder_accuracy: 0.0074 - val_clf_accuracy: 0.6960\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.6006 - decoder_loss: 0.1469 - clf_loss: 0.4523 - decoder_accuracy: 0.0076 - clf_accuracy: 0.8326 - val_loss: 0.8807 - val_decoder_loss: 0.1567 - val_clf_loss: 0.7168 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7086\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.5790 - decoder_loss: 0.1441 - clf_loss: 0.4334 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8392 - val_loss: 0.8060 - val_decoder_loss: 0.1436 - val_clf_loss: 0.6550 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.7335\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.5539 - decoder_loss: 0.1411 - clf_loss: 0.4114 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8485 - val_loss: 0.7933 - val_decoder_loss: 0.1419 - val_clf_loss: 0.6516 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.7357\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.5295 - decoder_loss: 0.1387 - clf_loss: 0.3894 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8579 - val_loss: 0.7956 - val_decoder_loss: 0.1419 - val_clf_loss: 0.6542 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7332\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.5167 - decoder_loss: 0.1370 - clf_loss: 0.3783 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8605 - val_loss: 0.8440 - val_decoder_loss: 0.1377 - val_clf_loss: 0.7005 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7171\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.5027 - decoder_loss: 0.1352 - clf_loss: 0.3661 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8646 - val_loss: 0.7683 - val_decoder_loss: 0.1403 - val_clf_loss: 0.6365 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7435\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.4880 - decoder_loss: 0.1339 - clf_loss: 0.3527 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8703 - val_loss: 0.7672 - val_decoder_loss: 0.1329 - val_clf_loss: 0.6337 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7459\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.4789 - decoder_loss: 0.1326 - clf_loss: 0.3449 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8702 - val_loss: 0.7768 - val_decoder_loss: 0.1330 - val_clf_loss: 0.6420 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7389\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.4716 - decoder_loss: 0.1312 - clf_loss: 0.3391 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8737 - val_loss: 0.7983 - val_decoder_loss: 0.1350 - val_clf_loss: 0.6596 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7360\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.4615 - decoder_loss: 0.1294 - clf_loss: 0.3308 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8783 - val_loss: 0.7415 - val_decoder_loss: 0.1336 - val_clf_loss: 0.6050 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7622\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.4503 - decoder_loss: 0.1283 - clf_loss: 0.3208 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8803 - val_loss: 0.7410 - val_decoder_loss: 0.1313 - val_clf_loss: 0.6182 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7648\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.4418 - decoder_loss: 0.1272 - clf_loss: 0.3132 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8843 - val_loss: 0.8436 - val_decoder_loss: 0.1276 - val_clf_loss: 0.7214 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7408\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4349 - decoder_loss: 0.1260 - clf_loss: 0.3075 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8874 - val_loss: 0.7464 - val_decoder_loss: 0.1310 - val_clf_loss: 0.6188 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7673\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4277 - decoder_loss: 0.1252 - clf_loss: 0.3013 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8881 - val_loss: 0.8048 - val_decoder_loss: 0.1296 - val_clf_loss: 0.6760 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7465\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4274 - decoder_loss: 0.1241 - clf_loss: 0.3019 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8879 - val_loss: 0.8191 - val_decoder_loss: 0.1205 - val_clf_loss: 0.6978 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7371\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4162 - decoder_loss: 0.1228 - clf_loss: 0.2921 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8930 - val_loss: 0.7794 - val_decoder_loss: 0.1275 - val_clf_loss: 0.6488 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7568\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.4107 - decoder_loss: 0.1221 - clf_loss: 0.2873 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8955 - val_loss: 0.7846 - val_decoder_loss: 0.1219 - val_clf_loss: 0.6596 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7554\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.4038 - decoder_loss: 0.1220 - clf_loss: 0.2805 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8969 - val_loss: 0.7600 - val_decoder_loss: 0.1232 - val_clf_loss: 0.6293 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7624\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.3991 - decoder_loss: 0.1208 - clf_loss: 0.2769 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8983 - val_loss: 0.8296 - val_decoder_loss: 0.1220 - val_clf_loss: 0.7164 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.7424\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.3940 - decoder_loss: 0.1204 - clf_loss: 0.2724 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8996 - val_loss: 0.8242 - val_decoder_loss: 0.1223 - val_clf_loss: 0.6979 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7546\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3930 - decoder_loss: 0.1198 - clf_loss: 0.2719 - decoder_accuracy: 0.0077 - clf_accuracy: 0.8998 - val_loss: 0.8471 - val_decoder_loss: 0.1215 - val_clf_loss: 0.7240 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7435\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.3818 - decoder_loss: 0.1186 - clf_loss: 0.2620 - decoder_accuracy: 0.0077 - clf_accuracy: 0.9041 - val_loss: 0.8022 - val_decoder_loss: 0.1248 - val_clf_loss: 0.6793 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.7611\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3794 - decoder_loss: 0.1186 - clf_loss: 0.2596 - decoder_accuracy: 0.0077 - clf_accuracy: 0.9054 - val_loss: 0.7457 - val_decoder_loss: 0.1206 - val_clf_loss: 0.6281 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7749\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.3752 - decoder_loss: 0.1176 - clf_loss: 0.2563 - decoder_accuracy: 0.0077 - clf_accuracy: 0.9064 - val_loss: 0.8173 - val_decoder_loss: 0.1165 - val_clf_loss: 0.6991 - val_decoder_accuracy: 0.0075 - val_clf_accuracy: 0.7470\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.3733 - decoder_loss: 0.1175 - clf_loss: 0.2545 - decoder_accuracy: 0.0077 - clf_accuracy: 0.9059 - val_loss: 0.8297 - val_decoder_loss: 0.1189 - val_clf_loss: 0.7173 - val_decoder_accuracy: 0.0076 - val_clf_accuracy: 0.7500\n",
      "Running sub 13, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[4.494 0.057 1.197 3.24  0.007 0.581 0.16 ]\n",
      "[7.455 0.039 0.894 6.522 0.007 0.682 0.213]\n",
      "[8.16  0.033 0.728 7.398 0.007 0.738 0.232]\n",
      "[8.467 0.03  0.656 7.78  0.007 0.76  0.239]\n",
      "[8.371 0.028 0.619 7.723 0.007 0.777 0.247]\n",
      "[7.785 0.027 0.583 7.174 0.007 0.794 0.242]\n",
      "[8.495 0.026 0.563 7.905 0.007 0.805 0.24 ]\n",
      "[8.78  0.025 0.58  8.175 0.007 0.796 0.225]\n",
      "[8.597 0.023 0.557 8.016 0.007 0.8   0.225]\n",
      "[8.763 0.022 0.558 8.183 0.007 0.799 0.207]\n",
      "[9.209 0.022 0.54  8.646 0.007 0.809 0.205]\n",
      "[9.629 0.021 0.546 9.062 0.007 0.812 0.213]\n",
      "[9.928 0.02  0.528 9.379 0.007 0.818 0.214]\n",
      "[10.337  0.02   0.517  9.799  0.007  0.822  0.217]\n",
      "[10.677  0.02   0.513 10.145  0.007  0.826  0.225]\n",
      "[1.262 0.045 0.535 0.682 0.007 0.812 0.197]\n",
      "[0.946 0.046 0.523 0.377 0.007 0.82  0.207]\n",
      "[0.768 0.045 0.523 0.199 0.007 0.816 0.211]\n",
      "[0.663 0.045 0.52  0.097 0.007 0.821 0.223]\n",
      "[0.611 0.045 0.521 0.045 0.007 0.822 0.23 ]\n",
      "[0.59  0.045 0.526 0.02  0.007 0.821 0.224]\n",
      "[0.586 0.045 0.534 0.008 0.007 0.823 0.207]\n",
      "[0.605 0.044 0.556 0.004 0.007 0.816 0.202]\n",
      "[0.634 0.044 0.588 0.002 0.007 0.808 0.216]\n",
      "[0.628 0.045 0.581 0.002 0.007 0.807 0.206]\n",
      "[0.631 0.045 0.585 0.002 0.007 0.805 0.225]\n",
      "[0.627 0.044 0.581 0.002 0.007 0.808 0.217]\n",
      "[0.651 0.045 0.605 0.002 0.007 0.808 0.219]\n",
      "[0.657 0.045 0.611 0.001 0.007 0.805 0.211]\n",
      "[0.65  0.045 0.603 0.002 0.007 0.807 0.219]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 44.2259 - accuracy: 0.3069 - val_loss: 45.1420 - val_accuracy: 0.2341\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 99us/sample - loss: 34.3110 - accuracy: 0.4645 - val_loss: 31.6649 - val_accuracy: 0.5090\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 27.7186 - accuracy: 0.5876 - val_loss: 24.6104 - val_accuracy: 0.6254\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 22.2945 - accuracy: 0.6752 - val_loss: 21.1138 - val_accuracy: 0.6903\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 18.9595 - accuracy: 0.7283 - val_loss: 20.1170 - val_accuracy: 0.7043\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 87us/sample - loss: 16.5292 - accuracy: 0.7619 - val_loss: 18.9878 - val_accuracy: 0.7287\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 14.7658 - accuracy: 0.7866 - val_loss: 18.2528 - val_accuracy: 0.7262\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 13.7034 - accuracy: 0.8035 - val_loss: 16.0867 - val_accuracy: 0.7651\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 12.7126 - accuracy: 0.8154 - val_loss: 16.0908 - val_accuracy: 0.7765\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 11.9603 - accuracy: 0.8291 - val_loss: 16.4979 - val_accuracy: 0.7737\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 11.1421 - accuracy: 0.8387 - val_loss: 15.0348 - val_accuracy: 0.7830\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 77us/sample - loss: 10.7321 - accuracy: 0.8433 - val_loss: 16.4288 - val_accuracy: 0.7744\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 10.4217 - accuracy: 0.8479 - val_loss: 16.5609 - val_accuracy: 0.7576\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 9.7532 - accuracy: 0.8575 - val_loss: 16.3706 - val_accuracy: 0.7940\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 9.5104 - accuracy: 0.8627 - val_loss: 17.8385 - val_accuracy: 0.7763\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 9.2286 - accuracy: 0.8659 - val_loss: 16.4050 - val_accuracy: 0.7830\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 8.9647 - accuracy: 0.8699 - val_loss: 15.6528 - val_accuracy: 0.7903\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 8.7190 - accuracy: 0.8727 - val_loss: 16.1687 - val_accuracy: 0.7927\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.4007 - accuracy: 0.8798 - val_loss: 16.9535 - val_accuracy: 0.7717\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 8.2602 - accuracy: 0.8808 - val_loss: 16.5253 - val_accuracy: 0.7949\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 8.1281 - accuracy: 0.8815 - val_loss: 17.6221 - val_accuracy: 0.7640\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 7.8450 - accuracy: 0.8860 - val_loss: 16.6121 - val_accuracy: 0.7911\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 7.6870 - accuracy: 0.8881 - val_loss: 15.6969 - val_accuracy: 0.7905\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 7.4706 - accuracy: 0.8916 - val_loss: 17.5004 - val_accuracy: 0.7798\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.4122 - accuracy: 0.8931 - val_loss: 16.8562 - val_accuracy: 0.7832\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 7.3792 - accuracy: 0.8929 - val_loss: 16.3648 - val_accuracy: 0.7932\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 7.2247 - accuracy: 0.8958 - val_loss: 15.1273 - val_accuracy: 0.8038\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 7.0726 - accuracy: 0.8992 - val_loss: 16.9047 - val_accuracy: 0.7797\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 6.9333 - accuracy: 0.9016 - val_loss: 14.9952 - val_accuracy: 0.8065\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 6.9621 - accuracy: 0.9001 - val_loss: 18.2060 - val_accuracy: 0.7775\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 1.9035 - accuracy: 0.2452 - val_loss: 1.8081 - val_accuracy: 0.2632\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.6915 - accuracy: 0.3251 - val_loss: 1.5994 - val_accuracy: 0.3462\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.5882 - accuracy: 0.3656 - val_loss: 1.5232 - val_accuracy: 0.3787\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 1.5153 - accuracy: 0.3948 - val_loss: 1.4437 - val_accuracy: 0.4130\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.4412 - accuracy: 0.4315 - val_loss: 1.3661 - val_accuracy: 0.4486\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.3795 - accuracy: 0.4621 - val_loss: 1.3047 - val_accuracy: 0.4757\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.3244 - accuracy: 0.4869 - val_loss: 1.2673 - val_accuracy: 0.5013\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.2721 - accuracy: 0.5127 - val_loss: 1.2068 - val_accuracy: 0.5187\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.2291 - accuracy: 0.5306 - val_loss: 1.1591 - val_accuracy: 0.5416\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 1.1860 - accuracy: 0.5448 - val_loss: 1.1315 - val_accuracy: 0.5514\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 1.1529 - accuracy: 0.5619 - val_loss: 1.1153 - val_accuracy: 0.5678\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 1.1070 - accuracy: 0.5806 - val_loss: 1.0551 - val_accuracy: 0.5883\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 1.0702 - accuracy: 0.5935 - val_loss: 1.0265 - val_accuracy: 0.5994\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 1.0348 - accuracy: 0.6057 - val_loss: 1.0019 - val_accuracy: 0.6122\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 1.0081 - accuracy: 0.6110 - val_loss: 0.9583 - val_accuracy: 0.6213\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9840 - accuracy: 0.6231 - val_loss: 0.9572 - val_accuracy: 0.6179\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.9600 - accuracy: 0.6315 - val_loss: 0.9436 - val_accuracy: 0.6292\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9400 - accuracy: 0.6413 - val_loss: 0.9330 - val_accuracy: 0.6279\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.9226 - accuracy: 0.6481 - val_loss: 0.9309 - val_accuracy: 0.6305\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.9112 - accuracy: 0.6473 - val_loss: 0.8758 - val_accuracy: 0.6521\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.8963 - accuracy: 0.6541 - val_loss: 0.8706 - val_accuracy: 0.6517\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.8876 - accuracy: 0.6590 - val_loss: 0.8533 - val_accuracy: 0.6648\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8689 - accuracy: 0.6696 - val_loss: 0.8471 - val_accuracy: 0.6600\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.8605 - accuracy: 0.6712 - val_loss: 0.8705 - val_accuracy: 0.6616\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.8546 - accuracy: 0.6747 - val_loss: 0.8703 - val_accuracy: 0.6586\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.8404 - accuracy: 0.6779 - val_loss: 0.8764 - val_accuracy: 0.6530\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.8334 - accuracy: 0.6831 - val_loss: 0.8976 - val_accuracy: 0.6373\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.8284 - accuracy: 0.6832 - val_loss: 0.8536 - val_accuracy: 0.6656\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 0.8128 - accuracy: 0.6869 - val_loss: 0.8589 - val_accuracy: 0.6692\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.8106 - accuracy: 0.6881 - val_loss: 0.8346 - val_accuracy: 0.6751\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 83us/sample - loss: 1.4988 - accuracy: 0.4284 - val_loss: 1.9146 - val_accuracy: 0.2865\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 1.0904 - accuracy: 0.6324 - val_loss: 1.0060 - val_accuracy: 0.6508\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 0.8516 - accuracy: 0.7181 - val_loss: 0.8311 - val_accuracy: 0.7063\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.7072 - accuracy: 0.7612 - val_loss: 0.7369 - val_accuracy: 0.7329\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.6248 - accuracy: 0.7850 - val_loss: 0.6804 - val_accuracy: 0.7610\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.5578 - accuracy: 0.8087 - val_loss: 0.6525 - val_accuracy: 0.7630\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.5199 - accuracy: 0.8208 - val_loss: 0.6265 - val_accuracy: 0.7692\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.4863 - accuracy: 0.8286 - val_loss: 0.6167 - val_accuracy: 0.7865\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4657 - accuracy: 0.8367 - val_loss: 0.6864 - val_accuracy: 0.7575\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.4384 - accuracy: 0.8461 - val_loss: 0.6154 - val_accuracy: 0.7937\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.4188 - accuracy: 0.8538 - val_loss: 0.6624 - val_accuracy: 0.7610\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4073 - accuracy: 0.8556 - val_loss: 0.5982 - val_accuracy: 0.7906\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3926 - accuracy: 0.8608 - val_loss: 0.7068 - val_accuracy: 0.7833\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.3816 - accuracy: 0.8638 - val_loss: 0.5852 - val_accuracy: 0.8075\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3644 - accuracy: 0.8682 - val_loss: 0.5765 - val_accuracy: 0.8049\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.3550 - accuracy: 0.8735 - val_loss: 0.6165 - val_accuracy: 0.7771\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3458 - accuracy: 0.8772 - val_loss: 0.6210 - val_accuracy: 0.7906\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.3441 - accuracy: 0.8758 - val_loss: 0.6174 - val_accuracy: 0.7933\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.3365 - accuracy: 0.8801 - val_loss: 0.6183 - val_accuracy: 0.8022\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.3320 - accuracy: 0.8804 - val_loss: 0.5932 - val_accuracy: 0.8035\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 1s 49us/sample - loss: 0.3295 - accuracy: 0.8817 - val_loss: 0.6366 - val_accuracy: 0.7975\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3208 - accuracy: 0.8852 - val_loss: 0.5499 - val_accuracy: 0.8151\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.3120 - accuracy: 0.8886 - val_loss: 0.5908 - val_accuracy: 0.8040\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.3114 - accuracy: 0.8896 - val_loss: 0.6337 - val_accuracy: 0.7821\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.3094 - accuracy: 0.8903 - val_loss: 0.6408 - val_accuracy: 0.7903\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.2959 - accuracy: 0.8940 - val_loss: 0.6099 - val_accuracy: 0.8052\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.2983 - accuracy: 0.8939 - val_loss: 0.5902 - val_accuracy: 0.8057\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.2934 - accuracy: 0.8947 - val_loss: 0.6327 - val_accuracy: 0.7824\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.2894 - accuracy: 0.8973 - val_loss: 0.6096 - val_accuracy: 0.8030\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.2844 - accuracy: 0.8974 - val_loss: 0.5795 - val_accuracy: 0.8102\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 192us/sample - loss: 1.6066 - decoder_loss: 0.2363 - clf_loss: 1.3688 - decoder_accuracy: 0.0067 - clf_accuracy: 0.5066 - val_loss: 1.4091 - val_decoder_loss: 0.2094 - val_clf_loss: 1.1953 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.5863\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.9937 - decoder_loss: 0.1644 - clf_loss: 0.8280 - decoder_accuracy: 0.0069 - clf_accuracy: 0.7177 - val_loss: 0.9440 - val_decoder_loss: 0.1855 - val_clf_loss: 0.7571 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7263\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.8029 - decoder_loss: 0.1583 - clf_loss: 0.6432 - decoder_accuracy: 0.0072 - clf_accuracy: 0.7809 - val_loss: 0.8601 - val_decoder_loss: 0.1758 - val_clf_loss: 0.6855 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7475\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 111us/sample - loss: 0.7008 - decoder_loss: 0.1535 - clf_loss: 0.5459 - decoder_accuracy: 0.0072 - clf_accuracy: 0.8105 - val_loss: 0.7779 - val_decoder_loss: 0.1602 - val_clf_loss: 0.6148 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7873\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.6404 - decoder_loss: 0.1498 - clf_loss: 0.4891 - decoder_accuracy: 0.0073 - clf_accuracy: 0.8296 - val_loss: 0.7739 - val_decoder_loss: 0.1610 - val_clf_loss: 0.6103 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7951\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 128us/sample - loss: 0.5998 - decoder_loss: 0.1467 - clf_loss: 0.4516 - decoder_accuracy: 0.0073 - clf_accuracy: 0.8415 - val_loss: 0.7758 - val_decoder_loss: 0.1477 - val_clf_loss: 0.6223 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.7856\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.5730 - decoder_loss: 0.1444 - clf_loss: 0.4271 - decoder_accuracy: 0.0073 - clf_accuracy: 0.8498 - val_loss: 0.7706 - val_decoder_loss: 0.1400 - val_clf_loss: 0.6284 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7879\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.5462 - decoder_loss: 0.1421 - clf_loss: 0.4025 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8609 - val_loss: 0.6952 - val_decoder_loss: 0.1431 - val_clf_loss: 0.5544 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8019\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.5305 - decoder_loss: 0.1401 - clf_loss: 0.3888 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8619 - val_loss: 0.7000 - val_decoder_loss: 0.1373 - val_clf_loss: 0.5577 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8094\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.5108 - decoder_loss: 0.1377 - clf_loss: 0.3715 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8673 - val_loss: 0.7565 - val_decoder_loss: 0.1357 - val_clf_loss: 0.6173 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7976\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.4973 - decoder_loss: 0.1361 - clf_loss: 0.3597 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8740 - val_loss: 0.6935 - val_decoder_loss: 0.1387 - val_clf_loss: 0.5522 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8133\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.4880 - decoder_loss: 0.1340 - clf_loss: 0.3523 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8769 - val_loss: 0.6625 - val_decoder_loss: 0.1348 - val_clf_loss: 0.5256 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8176\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.4721 - decoder_loss: 0.1326 - clf_loss: 0.3380 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8806 - val_loss: 0.6877 - val_decoder_loss: 0.1327 - val_clf_loss: 0.5549 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8132\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.4647 - decoder_loss: 0.1311 - clf_loss: 0.3321 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8808 - val_loss: 0.7298 - val_decoder_loss: 0.1318 - val_clf_loss: 0.6001 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8035\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.4502 - decoder_loss: 0.1296 - clf_loss: 0.3190 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8886 - val_loss: 0.6714 - val_decoder_loss: 0.1334 - val_clf_loss: 0.5341 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8197\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 118us/sample - loss: 0.4383 - decoder_loss: 0.1289 - clf_loss: 0.3077 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8912 - val_loss: 0.6755 - val_decoder_loss: 0.1274 - val_clf_loss: 0.5489 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8138\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 103us/sample - loss: 0.4314 - decoder_loss: 0.1278 - clf_loss: 0.3021 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8942 - val_loss: 0.6470 - val_decoder_loss: 0.1252 - val_clf_loss: 0.5184 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8184\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.4258 - decoder_loss: 0.1266 - clf_loss: 0.2977 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8944 - val_loss: 0.7066 - val_decoder_loss: 0.1228 - val_clf_loss: 0.5822 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.7994\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.4190 - decoder_loss: 0.1249 - clf_loss: 0.2924 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8958 - val_loss: 0.6989 - val_decoder_loss: 0.1266 - val_clf_loss: 0.5735 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8086\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.4141 - decoder_loss: 0.1249 - clf_loss: 0.2876 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8980 - val_loss: 0.6203 - val_decoder_loss: 0.1252 - val_clf_loss: 0.4943 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8316\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.4058 - decoder_loss: 0.1231 - clf_loss: 0.2811 - decoder_accuracy: 0.0074 - clf_accuracy: 0.8990 - val_loss: 0.6880 - val_decoder_loss: 0.1243 - val_clf_loss: 0.5599 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8195\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 101us/sample - loss: 0.3973 - decoder_loss: 0.1226 - clf_loss: 0.2732 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9040 - val_loss: 0.6875 - val_decoder_loss: 0.1224 - val_clf_loss: 0.5639 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8138\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.3911 - decoder_loss: 0.1219 - clf_loss: 0.2677 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9056 - val_loss: 0.6778 - val_decoder_loss: 0.1242 - val_clf_loss: 0.5465 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8216\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.3899 - decoder_loss: 0.1214 - clf_loss: 0.2671 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9054 - val_loss: 0.6750 - val_decoder_loss: 0.1247 - val_clf_loss: 0.5519 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8224\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 104us/sample - loss: 0.3815 - decoder_loss: 0.1206 - clf_loss: 0.2594 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9066 - val_loss: 0.6859 - val_decoder_loss: 0.1230 - val_clf_loss: 0.5612 - val_decoder_accuracy: 0.0071 - val_clf_accuracy: 0.8048\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 109us/sample - loss: 0.3795 - decoder_loss: 0.1197 - clf_loss: 0.2583 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9076 - val_loss: 0.6424 - val_decoder_loss: 0.1215 - val_clf_loss: 0.5157 - val_decoder_accuracy: 0.0073 - val_clf_accuracy: 0.8297\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 105us/sample - loss: 0.3738 - decoder_loss: 0.1189 - clf_loss: 0.2534 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9108 - val_loss: 0.6723 - val_decoder_loss: 0.1217 - val_clf_loss: 0.5505 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8187\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3723 - decoder_loss: 0.1181 - clf_loss: 0.2526 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9100 - val_loss: 0.6310 - val_decoder_loss: 0.1209 - val_clf_loss: 0.5078 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8322\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 4s 145us/sample - loss: 0.3630 - decoder_loss: 0.1176 - clf_loss: 0.2439 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9123 - val_loss: 0.6417 - val_decoder_loss: 0.1202 - val_clf_loss: 0.5226 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8240\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.3611 - decoder_loss: 0.1176 - clf_loss: 0.2420 - decoder_accuracy: 0.0074 - clf_accuracy: 0.9131 - val_loss: 0.6703 - val_decoder_loss: 0.1196 - val_clf_loss: 0.5518 - val_decoder_accuracy: 0.0072 - val_clf_accuracy: 0.8229\n",
      "Running sub 14, model 2, latent dim 4, cv 1\n",
      "['loss', 'decoder_loss', 'clf_loss', 'clf_1_loss', 'decoder_accuracy', 'clf_accuracy', 'clf_1_accuracy']\n",
      "[3.478 0.068 0.915 2.495 0.006 0.699 0.222]\n",
      "[7.274 0.038 0.641 6.594 0.006 0.763 0.198]\n",
      "[7.026 0.033 0.575 6.418 0.006 0.798 0.195]\n",
      "[5.958 0.03  0.416 5.511 0.006 0.856 0.197]\n",
      "[5.724 0.028 0.346 5.349 0.006 0.883 0.194]\n",
      "[5.685 0.027 0.32  5.337 0.006 0.891 0.183]\n",
      "[5.644 0.026 0.321 5.296 0.006 0.89  0.196]\n",
      "[5.821 0.025 0.326 5.469 0.006 0.883 0.183]\n",
      "[6.363 0.023 0.291 6.048 0.006 0.899 0.191]\n",
      "[6.783 0.023 0.323 6.436 0.006 0.885 0.189]\n",
      "[7.095 0.022 0.314 6.759 0.006 0.892 0.181]\n",
      "[8.126 0.021 0.347 7.758 0.006 0.88  0.193]\n",
      "[8.004 0.02  0.311 7.672 0.006 0.896 0.196]\n",
      "[8.392 0.02  0.328 8.044 0.006 0.887 0.199]\n",
      "[8.83  0.019 0.31  8.501 0.006 0.895 0.208]\n",
      "[0.76  0.041 0.335 0.383 0.006 0.884 0.211]\n",
      "[0.613 0.041 0.375 0.196 0.006 0.876 0.225]\n",
      "[0.466 0.041 0.322 0.103 0.006 0.897 0.237]\n",
      "[0.375 0.041 0.286 0.048 0.006 0.908 0.237]\n",
      "[0.368 0.041 0.296 0.031 0.006 0.904 0.235]\n",
      "[0.38  0.041 0.317 0.022 0.006 0.897 0.231]\n",
      "[0.324 0.04  0.266 0.017 0.006 0.915 0.24 ]\n",
      "[0.364 0.04  0.31  0.014 0.006 0.904 0.229]\n",
      "[0.381 0.04  0.328 0.013 0.006 0.892 0.224]\n",
      "[0.373 0.04  0.319 0.014 0.006 0.897 0.227]\n",
      "[0.314 0.041 0.266 0.008 0.006 0.913 0.229]\n",
      "[0.293 0.04  0.244 0.009 0.006 0.92  0.229]\n",
      "[0.332 0.04  0.284 0.008 0.006 0.91  0.228]\n",
      "[0.312 0.04  0.265 0.006 0.006 0.917 0.235]\n",
      "[0.315 0.04  0.268 0.006 0.006 0.918 0.239]\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 40.7255 - accuracy: 0.3948 - val_loss: 52.8204 - val_accuracy: 0.2144\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 2s 85us/sample - loss: 28.1648 - accuracy: 0.5919 - val_loss: 26.5411 - val_accuracy: 0.6230\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 21.9071 - accuracy: 0.6966 - val_loss: 19.1580 - val_accuracy: 0.7386\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 17.5390 - accuracy: 0.7631 - val_loss: 16.0307 - val_accuracy: 0.7729\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 2s 86us/sample - loss: 14.6706 - accuracy: 0.8015 - val_loss: 14.1181 - val_accuracy: 0.8024\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 12.9269 - accuracy: 0.8220 - val_loss: 14.0205 - val_accuracy: 0.8100\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 11.5125 - accuracy: 0.8410 - val_loss: 12.8632 - val_accuracy: 0.8260\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 72us/sample - loss: 10.7006 - accuracy: 0.8489 - val_loss: 10.9614 - val_accuracy: 0.8390\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 69us/sample - loss: 9.9571 - accuracy: 0.8601 - val_loss: 12.4846 - val_accuracy: 0.8235\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 9.2385 - accuracy: 0.8701 - val_loss: 13.0468 - val_accuracy: 0.8238\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 73us/sample - loss: 8.5975 - accuracy: 0.8789 - val_loss: 13.6701 - val_accuracy: 0.8273\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 7.9221 - accuracy: 0.8886 - val_loss: 10.4719 - val_accuracy: 0.8648\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 88us/sample - loss: 7.5059 - accuracy: 0.8944 - val_loss: 10.2748 - val_accuracy: 0.8627\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 7.2894 - accuracy: 0.8980 - val_loss: 11.6108 - val_accuracy: 0.8533\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 6.9033 - accuracy: 0.9028 - val_loss: 9.7010 - val_accuracy: 0.8695\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 6.5324 - accuracy: 0.9060 - val_loss: 11.7980 - val_accuracy: 0.8538\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 6.2607 - accuracy: 0.9099 - val_loss: 10.1341 - val_accuracy: 0.8732\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.9368 - accuracy: 0.9165 - val_loss: 9.9076 - val_accuracy: 0.8735\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 5.7552 - accuracy: 0.9187 - val_loss: 8.9265 - val_accuracy: 0.8781\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.6830 - accuracy: 0.9171 - val_loss: 10.6072 - val_accuracy: 0.8652\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 5.3841 - accuracy: 0.9226 - val_loss: 10.1129 - val_accuracy: 0.8729\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 2s 79us/sample - loss: 5.3270 - accuracy: 0.9225 - val_loss: 9.5778 - val_accuracy: 0.8865\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 74us/sample - loss: 5.1386 - accuracy: 0.9258 - val_loss: 9.3142 - val_accuracy: 0.8821\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 4.9838 - accuracy: 0.9282 - val_loss: 10.1030 - val_accuracy: 0.8851\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 94us/sample - loss: 4.9605 - accuracy: 0.9287 - val_loss: 11.8680 - val_accuracy: 0.8751\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 78us/sample - loss: 4.7642 - accuracy: 0.9303 - val_loss: 9.7248 - val_accuracy: 0.8673\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 2s 84us/sample - loss: 4.6197 - accuracy: 0.9315 - val_loss: 11.5988 - val_accuracy: 0.8671\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 2s 87us/sample - loss: 4.5390 - accuracy: 0.9325 - val_loss: 10.9268 - val_accuracy: 0.8686\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 2s 76us/sample - loss: 4.5025 - accuracy: 0.9334 - val_loss: 11.0754 - val_accuracy: 0.8838\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 81us/sample - loss: 4.3617 - accuracy: 0.9369 - val_loss: 9.8654 - val_accuracy: 0.8884\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 91us/sample - loss: 1.8504 - accuracy: 0.2620 - val_loss: 1.6841 - val_accuracy: 0.3573\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 1.4574 - accuracy: 0.4624 - val_loss: 1.3050 - val_accuracy: 0.4822\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 1.2334 - accuracy: 0.5510 - val_loss: 1.1203 - val_accuracy: 0.5695\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 1.0687 - accuracy: 0.6219 - val_loss: 0.9743 - val_accuracy: 0.6356\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.9365 - accuracy: 0.6679 - val_loss: 0.8530 - val_accuracy: 0.6748\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.8442 - accuracy: 0.6934 - val_loss: 0.7780 - val_accuracy: 0.7005\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.7792 - accuracy: 0.7106 - val_loss: 0.7465 - val_accuracy: 0.7105\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.7270 - accuracy: 0.7340 - val_loss: 0.7228 - val_accuracy: 0.7138\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.6942 - accuracy: 0.7446 - val_loss: 0.6930 - val_accuracy: 0.7197\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.6594 - accuracy: 0.7557 - val_loss: 0.6865 - val_accuracy: 0.7202\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.6381 - accuracy: 0.7606 - val_loss: 0.5903 - val_accuracy: 0.7533\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.6160 - accuracy: 0.7650 - val_loss: 0.6173 - val_accuracy: 0.7403\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 0.6039 - accuracy: 0.7698 - val_loss: 0.5655 - val_accuracy: 0.7624\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 71us/sample - loss: 0.5917 - accuracy: 0.7744 - val_loss: 0.5398 - val_accuracy: 0.7790\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.5827 - accuracy: 0.7767 - val_loss: 0.6447 - val_accuracy: 0.7324\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.5707 - accuracy: 0.7844 - val_loss: 0.5338 - val_accuracy: 0.7811\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 2s 64us/sample - loss: 0.5605 - accuracy: 0.7872 - val_loss: 0.5367 - val_accuracy: 0.7856\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 67us/sample - loss: 0.5500 - accuracy: 0.7925 - val_loss: 0.5186 - val_accuracy: 0.7913\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 2s 70us/sample - loss: 0.5441 - accuracy: 0.7929 - val_loss: 0.5877 - val_accuracy: 0.7722\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.5316 - accuracy: 0.8005 - val_loss: 0.5073 - val_accuracy: 0.7949\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.5234 - accuracy: 0.8019 - val_loss: 0.5197 - val_accuracy: 0.7883\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5176 - accuracy: 0.8002 - val_loss: 0.4869 - val_accuracy: 0.8098\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.5143 - accuracy: 0.8068 - val_loss: 0.4938 - val_accuracy: 0.8056\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.5114 - accuracy: 0.8064 - val_loss: 0.5171 - val_accuracy: 0.8057\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 2s 63us/sample - loss: 0.5058 - accuracy: 0.8083 - val_loss: 0.4855 - val_accuracy: 0.8113\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.4917 - accuracy: 0.8142 - val_loss: 0.4870 - val_accuracy: 0.8098\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.4893 - accuracy: 0.8133 - val_loss: 0.4717 - val_accuracy: 0.8152\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4936 - accuracy: 0.8126 - val_loss: 0.5319 - val_accuracy: 0.7910\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 58us/sample - loss: 0.4858 - accuracy: 0.8146 - val_loss: 0.4957 - val_accuracy: 0.8194\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 65us/sample - loss: 0.4843 - accuracy: 0.8170 - val_loss: 0.4680 - val_accuracy: 0.8202\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 2s 80us/sample - loss: 1.2083 - accuracy: 0.6020 - val_loss: 1.5581 - val_accuracy: 0.3470\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.7593 - accuracy: 0.7641 - val_loss: 0.7101 - val_accuracy: 0.7519\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.5826 - accuracy: 0.8092 - val_loss: 0.4890 - val_accuracy: 0.8484\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.4827 - accuracy: 0.8421 - val_loss: 0.4604 - val_accuracy: 0.8448\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.4182 - accuracy: 0.8579 - val_loss: 0.4193 - val_accuracy: 0.8630\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 2s 62us/sample - loss: 0.3721 - accuracy: 0.8708 - val_loss: 0.4200 - val_accuracy: 0.8354\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 1s 51us/sample - loss: 0.3436 - accuracy: 0.8790 - val_loss: 0.3150 - val_accuracy: 0.8938\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.3173 - accuracy: 0.8873 - val_loss: 0.3322 - val_accuracy: 0.8790\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.3052 - accuracy: 0.8910 - val_loss: 0.3707 - val_accuracy: 0.8613\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 1s 56us/sample - loss: 0.2843 - accuracy: 0.8990 - val_loss: 0.3555 - val_accuracy: 0.8667\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.2682 - accuracy: 0.9030 - val_loss: 0.2575 - val_accuracy: 0.9140\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.2563 - accuracy: 0.9075 - val_loss: 0.3047 - val_accuracy: 0.8981\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.2476 - accuracy: 0.9105 - val_loss: 0.3097 - val_accuracy: 0.8852\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.2336 - accuracy: 0.9173 - val_loss: 0.3775 - val_accuracy: 0.8633\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.2274 - accuracy: 0.9185 - val_loss: 0.3414 - val_accuracy: 0.8787\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.2177 - accuracy: 0.9197 - val_loss: 0.2699 - val_accuracy: 0.9008\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 1s 52us/sample - loss: 0.2129 - accuracy: 0.9247 - val_loss: 0.3065 - val_accuracy: 0.8916\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 2s 60us/sample - loss: 0.2046 - accuracy: 0.9263 - val_loss: 0.2780 - val_accuracy: 0.8946\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.1925 - accuracy: 0.9290 - val_loss: 0.2799 - val_accuracy: 0.8983\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.1912 - accuracy: 0.9294 - val_loss: 0.2632 - val_accuracy: 0.9070\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 2s 66us/sample - loss: 0.1867 - accuracy: 0.9322 - val_loss: 0.2801 - val_accuracy: 0.8976\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1827 - accuracy: 0.9332 - val_loss: 0.1975 - val_accuracy: 0.9303\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 1s 50us/sample - loss: 0.1738 - accuracy: 0.9383 - val_loss: 0.2560 - val_accuracy: 0.9086\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 2s 68us/sample - loss: 0.1723 - accuracy: 0.9387 - val_loss: 0.2329 - val_accuracy: 0.9094\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 1s 53us/sample - loss: 0.1702 - accuracy: 0.9383 - val_loss: 0.2755 - val_accuracy: 0.9040\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 1s 59us/sample - loss: 0.1655 - accuracy: 0.9391 - val_loss: 0.2724 - val_accuracy: 0.9060\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 1s 54us/sample - loss: 0.1648 - accuracy: 0.9386 - val_loss: 0.3731 - val_accuracy: 0.8776\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 1s 57us/sample - loss: 0.1650 - accuracy: 0.9402 - val_loss: 0.2732 - val_accuracy: 0.9013\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 1s 55us/sample - loss: 0.1578 - accuracy: 0.9429 - val_loss: 0.2641 - val_accuracy: 0.9100\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 2s 61us/sample - loss: 0.1524 - accuracy: 0.9446 - val_loss: 0.2626 - val_accuracy: 0.9116\n",
      "Train on 25200 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "25200/25200 [==============================] - 5s 192us/sample - loss: 1.2474 - decoder_loss: 0.2353 - clf_loss: 1.0105 - decoder_accuracy: 0.0060 - clf_accuracy: 0.6805 - val_loss: 1.0959 - val_decoder_loss: 0.2000 - val_clf_loss: 0.8925 - val_decoder_accuracy: 0.0063 - val_clf_accuracy: 0.7075\n",
      "Epoch 2/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.7277 - decoder_loss: 0.1624 - clf_loss: 0.5638 - decoder_accuracy: 0.0064 - clf_accuracy: 0.8165 - val_loss: 0.5942 - val_decoder_loss: 0.1557 - val_clf_loss: 0.4348 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8789\n",
      "Epoch 3/30\n",
      "25200/25200 [==============================] - 3s 108us/sample - loss: 0.5915 - decoder_loss: 0.1559 - clf_loss: 0.4341 - decoder_accuracy: 0.0066 - clf_accuracy: 0.8540 - val_loss: 0.5571 - val_decoder_loss: 0.1559 - val_clf_loss: 0.4004 - val_decoder_accuracy: 0.0060 - val_clf_accuracy: 0.8559\n",
      "Epoch 4/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.5151 - decoder_loss: 0.1529 - clf_loss: 0.3607 - decoder_accuracy: 0.0067 - clf_accuracy: 0.8757 - val_loss: 0.4613 - val_decoder_loss: 0.1514 - val_clf_loss: 0.3076 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.9062\n",
      "Epoch 5/30\n",
      "25200/25200 [==============================] - 3s 116us/sample - loss: 0.4718 - decoder_loss: 0.1496 - clf_loss: 0.3207 - decoder_accuracy: 0.0067 - clf_accuracy: 0.8875 - val_loss: 0.5411 - val_decoder_loss: 0.1542 - val_clf_loss: 0.3828 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.8519\n",
      "Epoch 6/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.4378 - decoder_loss: 0.1460 - clf_loss: 0.2903 - decoder_accuracy: 0.0068 - clf_accuracy: 0.8992 - val_loss: 0.3767 - val_decoder_loss: 0.1414 - val_clf_loss: 0.2320 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.9210\n",
      "Epoch 7/30\n",
      "25200/25200 [==============================] - 3s 134us/sample - loss: 0.4159 - decoder_loss: 0.1423 - clf_loss: 0.2722 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9025 - val_loss: 0.3822 - val_decoder_loss: 0.1404 - val_clf_loss: 0.2423 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9189\n",
      "Epoch 8/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3954 - decoder_loss: 0.1388 - clf_loss: 0.2552 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9098 - val_loss: 0.4020 - val_decoder_loss: 0.1380 - val_clf_loss: 0.2607 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.9086\n",
      "Epoch 9/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3789 - decoder_loss: 0.1358 - clf_loss: 0.2416 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9142 - val_loss: 0.3703 - val_decoder_loss: 0.1386 - val_clf_loss: 0.2323 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.9160\n",
      "Epoch 10/30\n",
      "25200/25200 [==============================] - 3s 122us/sample - loss: 0.3639 - decoder_loss: 0.1338 - clf_loss: 0.2286 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9186 - val_loss: 0.3629 - val_decoder_loss: 0.1333 - val_clf_loss: 0.2283 - val_decoder_accuracy: 0.0063 - val_clf_accuracy: 0.9211\n",
      "Epoch 11/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.3529 - decoder_loss: 0.1311 - clf_loss: 0.2204 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9215 - val_loss: 0.3674 - val_decoder_loss: 0.1290 - val_clf_loss: 0.2377 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9190\n",
      "Epoch 12/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.3420 - decoder_loss: 0.1292 - clf_loss: 0.2113 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9244 - val_loss: 0.4531 - val_decoder_loss: 0.1346 - val_clf_loss: 0.3163 - val_decoder_accuracy: 0.0061 - val_clf_accuracy: 0.8778\n",
      "Epoch 13/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.3337 - decoder_loss: 0.1273 - clf_loss: 0.2050 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9281 - val_loss: 0.3600 - val_decoder_loss: 0.1279 - val_clf_loss: 0.2309 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9176\n",
      "Epoch 14/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.3290 - decoder_loss: 0.1259 - clf_loss: 0.2017 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9279 - val_loss: 0.3807 - val_decoder_loss: 0.1280 - val_clf_loss: 0.2497 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9051\n",
      "Epoch 15/30\n",
      "25200/25200 [==============================] - 3s 119us/sample - loss: 0.3202 - decoder_loss: 0.1240 - clf_loss: 0.1947 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9307 - val_loss: 0.4485 - val_decoder_loss: 0.1267 - val_clf_loss: 0.3195 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8783\n",
      "Epoch 16/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3157 - decoder_loss: 0.1224 - clf_loss: 0.1919 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9312 - val_loss: 0.5266 - val_decoder_loss: 0.1281 - val_clf_loss: 0.4020 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8578\n",
      "Epoch 17/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.3096 - decoder_loss: 0.1202 - clf_loss: 0.1879 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9334 - val_loss: 0.4188 - val_decoder_loss: 0.1209 - val_clf_loss: 0.2961 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8844\n",
      "Epoch 18/30\n",
      "25200/25200 [==============================] - 3s 106us/sample - loss: 0.3038 - decoder_loss: 0.1190 - clf_loss: 0.1834 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9335 - val_loss: 0.3855 - val_decoder_loss: 0.1178 - val_clf_loss: 0.2696 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9035\n",
      "Epoch 19/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2982 - decoder_loss: 0.1178 - clf_loss: 0.1790 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9351 - val_loss: 0.3710 - val_decoder_loss: 0.1185 - val_clf_loss: 0.2542 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9103\n",
      "Epoch 20/30\n",
      "25200/25200 [==============================] - 3s 120us/sample - loss: 0.2926 - decoder_loss: 0.1168 - clf_loss: 0.1744 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9375 - val_loss: 0.3784 - val_decoder_loss: 0.1177 - val_clf_loss: 0.2613 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9081\n",
      "Epoch 21/30\n",
      "25200/25200 [==============================] - 3s 107us/sample - loss: 0.2879 - decoder_loss: 0.1158 - clf_loss: 0.1707 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9379 - val_loss: 0.4157 - val_decoder_loss: 0.1186 - val_clf_loss: 0.2945 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8938\n",
      "Epoch 22/30\n",
      "25200/25200 [==============================] - 3s 132us/sample - loss: 0.2818 - decoder_loss: 0.1147 - clf_loss: 0.1658 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9407 - val_loss: 0.4226 - val_decoder_loss: 0.1131 - val_clf_loss: 0.3097 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8863\n",
      "Epoch 23/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.2810 - decoder_loss: 0.1134 - clf_loss: 0.1663 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9379 - val_loss: 0.3777 - val_decoder_loss: 0.1144 - val_clf_loss: 0.2596 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9090\n",
      "Epoch 24/30\n",
      "25200/25200 [==============================] - 3s 114us/sample - loss: 0.2735 - decoder_loss: 0.1126 - clf_loss: 0.1595 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9440 - val_loss: 0.3953 - val_decoder_loss: 0.1151 - val_clf_loss: 0.2756 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9022\n",
      "Epoch 25/30\n",
      "25200/25200 [==============================] - 3s 112us/sample - loss: 0.2718 - decoder_loss: 0.1121 - clf_loss: 0.1584 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9426 - val_loss: 0.3305 - val_decoder_loss: 0.1134 - val_clf_loss: 0.2132 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9194\n",
      "Epoch 26/30\n",
      "25200/25200 [==============================] - 3s 124us/sample - loss: 0.2660 - decoder_loss: 0.1108 - clf_loss: 0.1538 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9443 - val_loss: 0.3595 - val_decoder_loss: 0.1139 - val_clf_loss: 0.2427 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9124\n",
      "Epoch 27/30\n",
      "25200/25200 [==============================] - 3s 125us/sample - loss: 0.2633 - decoder_loss: 0.1107 - clf_loss: 0.1512 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9452 - val_loss: 0.3419 - val_decoder_loss: 0.1093 - val_clf_loss: 0.2342 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9205\n",
      "Epoch 28/30\n",
      "25200/25200 [==============================] - 3s 121us/sample - loss: 0.2631 - decoder_loss: 0.1096 - clf_loss: 0.1520 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9434 - val_loss: 0.3912 - val_decoder_loss: 0.1110 - val_clf_loss: 0.2817 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.8963\n",
      "Epoch 29/30\n",
      "25200/25200 [==============================] - 3s 113us/sample - loss: 0.2614 - decoder_loss: 0.1090 - clf_loss: 0.1511 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9445 - val_loss: 0.3127 - val_decoder_loss: 0.1096 - val_clf_loss: 0.2015 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9260\n",
      "Epoch 30/30\n",
      "25200/25200 [==============================] - 3s 110us/sample - loss: 0.2584 - decoder_loss: 0.1082 - clf_loss: 0.1489 - decoder_accuracy: 0.0068 - clf_accuracy: 0.9459 - val_loss: 0.3183 - val_decoder_loss: 0.1099 - val_clf_loss: 0.2075 - val_decoder_accuracy: 0.0062 - val_clf_accuracy: 0.9216\n"
     ]
    }
   ],
   "source": [
    "# Loop through training\n",
    "train_dict = {'sub_type':sub_type,'n_train':'fullsomemix4', 'load':False, 'train_scale':5, 'epochs': 30, 'batch_size' : 128, 'sparsity':True,'dt':'manual','feat_type':'feat','noise':True, 'latent_dim':4,'mod':['lda'],'gens':50, 'mod_dt':'emgscaleall','train_grp':2}\n",
    "train_sess = session.Session(**train_dict)\n",
    "\n",
    "# loop through subjects\n",
    "for sub_i in range(1,np.max(params[:,0])+1):\n",
    "    for lat in range(4,5):\n",
    "        train_sess.latent_dim = lat\n",
    "        train_out = train_sess.loop_cv(raw,params,sub=sub_i,mod='all')\n",
    "        # for key,val in train_out.items():\n",
    "        #     exec(key + '=val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'TR'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)\n",
    "\n",
    "train_dict = {'sub_type':sub_type,'n_train':'fullallmix4', 'load':False, 'train_scale':5, 'epochs': 30, 'batch_size' : 128, 'sparsity':True,'dt':'manual','feat_type':'feat','noise':True, 'latent_dim':4,'mod':['lda'],'gens':50, 'mod_dt':'1028','train_grp':2}\n",
    "train_sess = session.Session(**train_dict)\n",
    "\n",
    "# loop through subjects\n",
    "for sub_i in range(1,np.max(params[:,0])+1):\n",
    "    for lat in range(1,11):\n",
    "        train_sess.latent_dim = lat\n",
    "        train_out = train_sess.loop_cv(raw,params,sub=sub_i,mod='all')\n",
    "        # for key,val in train_out.items():\n",
    "        #     exec(key + '=val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:25: RuntimeWarning: Mean of empty slice\n",
      "  # mean_acc = np.nanmean(np.nanmean(all_acc,axis=2),axis=0)\n",
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:26: RuntimeWarning: Mean of empty slice\n",
      "  # mean_val = np.nanmean(np.nanmean(all_val,axis=2),axis=0)\n"
     ]
    }
   ],
   "source": [
    "train_dict = {'sub_type':sub_type,'n_train':'fullallmix4', 'load':False, 'train_scale':5, 'epochs': 30, 'batch_size' : 128, 'sparsity':True,'dt':'cv','feat_type':'feat','noise':True, 'latent_dim':4,'mod':['lda'],'gens':50, 'mod_dt':'1028','train_grp':2}\n",
    "train_sess = session.Session(**train_dict)\n",
    "svae, cnn,vcnn,ecnn = plot_utils.plot_latent_dim(params,train_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions of inputs\n",
    "sub = 2\n",
    "test_dict = {'sub_type':sub_type,'dt':'manual', 'mod_dt':'1012','sparsity':True, 'load':True, 'batch_size':128, 'latent_dim':5, 'epochs':30,'train_scale':5, 'n_train':'fullgaussflat4', 'n_test':'partgauss4','feat_type':'feat', 'noise':True,'train_grp':2,'test_dt': 'noisescale'}\n",
    "test_sess = session.Session(**test_dict)\n",
    "ntype = 'gauss'\n",
    "addon = True\n",
    "\n",
    "if not addon:\n",
    "    x_clean_lda = np.array([]).reshape(0,6)\n",
    "    x_clean_noise = np.array([]).reshape(0,6)\n",
    "    x_clean_sae = np.array([]).reshape(0,5)\n",
    "    x_clean_cnn = np.array([]).reshape(0,5)\n",
    "    x_clean_vcnn = np.array([]).reshape(0,5)\n",
    "    y_clean = np.array([]).reshape(0,1)\n",
    "\n",
    "    x_noisy_lda = np.array([]).reshape(0,6)\n",
    "    x_noisy_noise = np.array([]).reshape(0,6)\n",
    "    x_noisy_sae = np.array([]).reshape(0,5)\n",
    "    x_noisy_cnn = np.array([]).reshape(0,5)\n",
    "    x_noisy_vcnn = np.array([]).reshape(0,5)\n",
    "    y_noisy = np.array([]).reshape(0,1)\n",
    "\n",
    "if ntype == 'flat':\n",
    "    test_max = 2\n",
    "else:\n",
    "    test_max = 6\n",
    "\n",
    "for i in range(1,5):\n",
    "    test_sess.n_test = 'part' + ntype + str(i)\n",
    "    for test_scale in range(1,test_max):\n",
    "        red_out = test_sess.reduce_latent(raw, params, sub, cv=1,test_scale=1)\n",
    "        for key,val in red_out.items():\n",
    "            exec(key + '=val')\n",
    "        x_clean_lda = np.vstack([x_test_lda_red[:clean_size,:], x_clean_lda])\n",
    "        x_clean_noise = np.vstack([x_test_noise_red[:clean_size,:], x_clean_noise])\n",
    "        x_clean_sae = np.vstack([x_test_sae_red[:clean_size,:], x_clean_sae])\n",
    "        x_clean_cnn = np.vstack([x_test_cnn_red[:clean_size,:], x_clean_cnn])\n",
    "        x_clean_vcnn = np.vstack([x_test_vcnn_red[:clean_size,:], x_clean_vcnn])\n",
    "        y_clean = np.vstack([y_test[:clean_size,:], y_clean])\n",
    "\n",
    "        x_noisy_lda = np.vstack([x_test_lda_red[clean_size:,:], x_noisy_lda])\n",
    "        x_noisy_noise = np.vstack([x_test_noise_red[clean_size:,:], x_noisy_noise])\n",
    "        x_noisy_sae = np.vstack([x_test_sae_red[clean_size:,:], x_noisy_sae])\n",
    "        x_noisy_cnn = np.vstack([x_test_cnn_red[clean_size:,:], x_noisy_cnn])\n",
    "        x_noisy_vcnn = np.vstack([x_test_vcnn_red[clean_size:,:], x_noisy_vcnn])\n",
    "        y_noisy = np.vstack([y_test[clean_size:,:], y_noisy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reduced dimensions\n",
    "plot_utils.plot_latent_rep(x_clean_lda, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_noise, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_sae, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_cnn, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_vcnn, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_latent_rep(x_noisy_lda, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_noise, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_sae, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_cnn, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_vcnn, y_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_lda = np.vstack([x_clean_lda,x_noisy_lda])\n",
    "x_all_noise = np.vstack([x_clean_noise,x_noisy_noise])\n",
    "x_all_sae = np.vstack([x_clean_sae,x_noisy_sae])\n",
    "x_all_cnn = np.vstack([x_clean_cnn,x_noisy_cnn])\n",
    "x_all_vcnn = np.vstack([x_clean_vcnn,x_noisy_vcnn])\n",
    "y_all = np.vstack([y_clean, y_noisy])\n",
    "plot_utils.plot_latent_rep(x_all_lda, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_noise, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_sae, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_cnn, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_vcnn, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1,2,3],[-2.5,3,1],[2,1,4],[2,1,4]])\n",
    "test2 = np.array([[-3.3,2,3],[-2.5,3,.2],[2,1,4],[2,1,4]])\n",
    "test3 = np.stack([test,test2])\n",
    "\n",
    "emg_scale = np.ones((4,1))\n",
    "for i in range(np.shape(test3)[1]):\n",
    "    print(i)\n",
    "    emg_scale[i] = 5/np.max(np.abs(test3[:,i,:]))\n",
    "\n",
    "test4 = test3*emg_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results_2_manual_noisescaleall/AB1_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB2_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB3_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB4_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB5_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB6_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB7_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB8_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB9_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB10_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB11_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB12_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB13_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB1_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB2_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB3_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB4_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB5_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB6_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB7_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB8_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB9_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB10_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB11_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB12_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB13_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB1_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB2_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB3_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB4_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB5_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB6_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB7_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB8_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB9_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB10_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB11_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB12_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB13_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB1_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB2_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB3_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB4_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB5_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB6_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB7_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB8_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB9_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB10_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB11_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB12_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n",
      "results_2_manual_noisescaleall/AB13_feat_dim_4_ep_30_bat_128_fullallmix4_5_lr_10_sparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:51: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:52: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:53: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy as cp\n",
    "test_dict = {'sub_type':sub_type,'dt':'manual', 'mod_dt':'emgscaleall','sparsity':True, 'load':True, 'batch_size':128, 'latent_dim':4, 'epochs':30,'train_scale':5, 'n_train':'fullallmix4', 'n_test':'partgauss4','feat_type':'feat', 'noise':True,'train_grp':2,'test_dt':'noisescaleall'}\n",
    "test_sess = session.Session(**test_dict) \n",
    "\n",
    "load = False\n",
    "for posi in range(1,2):#5):\n",
    "    ntype = 'real'# + str(posi)\n",
    "\n",
    "    if ntype[:3] == 'pos' and not load:\n",
    "        i_start = 4\n",
    "        i_end = 5\n",
    "        acc_all = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "        acc_clean = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "        acc_noise = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "    elif ntype == 'flat' or 'mix' in ntype or 'real' in ntype:\n",
    "        i_start = 1\n",
    "        i_end = 5\n",
    "        acc_all = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "        acc_clean = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "        acc_noise = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "    else:\n",
    "        i_start = 1\n",
    "        i_end = 5\n",
    "        acc_all = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "        acc_clean = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "        acc_noise = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "\n",
    "    for i in range(i_start,i_end):\n",
    "        if load and 'pos' in ntype:\n",
    "            test_sess.n_test = 'part' + ntype + str(i) + '4'\n",
    "        else:\n",
    "            test_sess.n_test = 'part' + ntype + str(i)    \n",
    "\n",
    "        if load:\n",
    "            for sub in range(1,np.max(params[:,0])):\n",
    "                foldername = test_sess.create_foldername(ftype='results')\n",
    "                filename = test_sess.create_filename(foldername,0,sub)\n",
    "                print(filename)\n",
    "                if os.path.isfile(filename + '_' + test_sess.n_test + '_cvresults.p'):\n",
    "                    with open(filename + '_' + test_sess.n_test + '_cvresults.p', 'rb') as f:\n",
    "                        temp_all, temp_noise, temp_clean = pickle.load(f)\n",
    "                    acc_all[sub-1,i-i_start,:,:], acc_clean[sub-1,i-i_start,:,:], acc_noise[sub-1,i-i_start,:,:] = np.squeeze(temp_all),np.squeeze(temp_clean),np.squeeze(temp_noise)\n",
    "        else:\n",
    "            test_out = test_sess.loop_test(raw, params)\n",
    "            for key,val in test_out.items():\n",
    "                exec(key + '=val')\n",
    "\n",
    "    ave_pos_noise= np.nanmean(acc_noise,axis=0)\n",
    "    ave_pos_clean = np.nanmean(acc_clean,axis=0)\n",
    "    ave_noise= np.nanmean(acc_noise,axis=2)\n",
    "    ave_clean = np.nanmean(acc_clean,axis=2)\n",
    "    ave_gauss_noise = np.nanmean(ave_noise,axis=0)\n",
    "    ave_gauss_clean = np.nanmean(ave_clean,axis=0)\n",
    "\n",
    "    if 'gauss' in ntype:\n",
    "        ave_gauss= cp.deepcopy(ave_noise)\n",
    "    elif 'flat' in ntype:\n",
    "        ave_flat = cp.deepcopy(ave_noise)\n",
    "    elif '60hz' in ntype:\n",
    "        ave_60hz = cp.deepcopy(ave_noise)\n",
    "    elif 'mix' in ntype:\n",
    "        ave_mix = cp.deepcopy(ave_noise)\n",
    "\n",
    "# out_dict['ave_' + ntype] = cp.deepcopy(ave_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_dict = {}\n",
    "ave_gauss = out_dict['ave_posrealbreak']\n",
    "ave_mix = out_dict['ave_posrealmix']\n",
    "ave_flat = out_dict['ave_posrealmove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_posrealbreak\n",
      "ave_posrealcontactbig\n",
      "ave_posrealmove\n",
      "ave_posrealmix\n"
     ]
    }
   ],
   "source": [
    "for key in out_dict:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:206: RuntimeWarning: Mean of empty slice\n",
      "  ave_diff_gauss = np.nanmean(np.nanmean(all_gauss_diff,axis=1),axis=0)\n",
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:207: RuntimeWarning: Mean of empty slice\n",
      "  ave_diff_60hz = np.nanmean(np.nanmean(all_60hz_diff,axis=1),axis=0)\n",
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:208: RuntimeWarning: Mean of empty slice\n",
      "  ave_diff_flat = np.nanmean(np.nanmean(all_flat_diff,axis=1),axis=0)\n",
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:209: RuntimeWarning: Mean of empty slice\n",
      "  ave_diff_clean = np.nanmean(np.nanmean(all_clean_diff,axis=1),axis=0)\n",
      "c:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\plot_utils.py:213: RuntimeWarning: Mean of empty slice\n",
      "  diff_clean = np.nanmean(all_clean_diff,axis=0)\n"
     ]
    }
   ],
   "source": [
    "plot_utils.plot_summary(ave_clean,ave_gauss,ave_mix,ave_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs # noisy electrodes\n",
    "plot_utils.plot_electrode_results(ave_gauss_noise,ave_gauss_clean,test_sess.n_train,ntype,test_sess.sub_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs. position\n",
    "plot_utils.plot_pos_results(ave_pos_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "    real_noise_temp, _ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.ndarray.flatten(real_noise_temp[0,:,:])\n",
    "out = np.squeeze(np.zeros((200*1000//8,5)))\n",
    "t = np.linspace(0,200//8,200*1000//8)\n",
    "fig, ax = plt.subplots(5,1)\n",
    "for ch in range(5):\n",
    "    it = 0\n",
    "    for i in range(0,1000,8):\n",
    "        out[it*200:it*200+200,ch] = real_noise_temp[ch,i,:]\n",
    "        it += 1\n",
    "    ax[ch].plot(t,out[:,ch])\n",
    "    ax[ch].set_ylim([-5,5])\n",
    "    ax[ch].set_xlim([0,200//8])\n",
    "    if ch < 4:\n",
    "        ax[ch].set_xticks([])\n",
    "    # ax[ch].tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/TR4_traindata_4.p\n"
     ]
    }
   ],
   "source": [
    "import process_data as prd\n",
    "foldername = test_sess.create_foldername()\n",
    "filename = test_sess.create_filename(foldername, 1, 1)\n",
    "test_sess.n_test = 'partrealbreaknm3'\n",
    "\n",
    "with open(filename + '.p', 'rb') as f:\n",
    "    scaler, svae_w, svae_enc_w, svae_dec_w, svae_clf_w, sae_w, sae_enc_w, sae_clf_w, cnn_w, cnn_enc_w, cnn_clf_w, vcnn_w, vcnn_enc_w, vcnn_clf_w, ecnn_w, ecnn_enc_w, ecnn_clf_w, w_svae, c_svae, w_sae, c_sae, w_cnn, c_cnn, w_vcnn, c_vcnn, w_ecnn, c_ecnn, w, c, w_noise, c_noise, mu, C, qda, qda_noise,emg_scale = pickle.load(f)   \n",
    "\n",
    "_, x_test, _, _, p_test, _ = prd.train_data_split(raw,params,4,test_sess.sub_type,dt=test_sess.dt,train_grp=4)\n",
    "x_test = x_test * emg_scale\n",
    "x_test_noise, x_test_clean, y_test_clean = prd.add_noise(x_test, p_test, 4, test_sess.n_test, 1, real_noise=real_noise_temp, emg_scale = emg_scale)\n",
    "x_test_noise[x_test_noise > 5] = 5\n",
    "x_test_noise[x_test_noise < -5] = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2821\n"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(6,1)\n",
    "fig2, ax2 = plt.subplots(6,1)\n",
    "ind = np.random.randint(np.size(x_test,0),np.size(x_test_noise,0))\n",
    "print(ind)\n",
    "for ch in range(6):\n",
    "    ax[ch].plot(x_test_clean[ind,ch,:])\n",
    "    ax2[ch].plot(x_test_noise[ind,ch,:])\n",
    "    ax[ch].set_ylim([-5,5])\n",
    "    ax2[ch].set_ylim([-5,5])\n",
    "    ax[0].set_title(str(np.argmax(y_test_clean[ind,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models_2_cv/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_cv_1_sparse_hist.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-14404b3f90b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoldername\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msub_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeat_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_dim_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_ep_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_bat_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_lr_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'_cv_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_sparse'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_hist.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0msvae_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msae_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvcnn_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models_2_cv/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_cv_1_sparse_hist.p'"
     ]
    }
   ],
   "source": [
    "## Plot training metrics trajectories\n",
    "# initialize parameters\n",
    "train_grp = 2\n",
    "dt = 'cv'\n",
    "feat_type = 'feat'\n",
    "latent_dim = 5\n",
    "epochs = 30\n",
    "n_train = 'fullgaussflat4'\n",
    "train_scale = 5\n",
    "foldername = 'models' + '_' + str(train_grp) + '_' + dt\n",
    "batch_size = 128\n",
    "n_test = 0\n",
    "lr = 0.001\n",
    "\n",
    "# initialize loss and accuracy matrices\n",
    "loss = np.full([4,4,epochs],np.nan)\n",
    "val_loss = np.full([4,4,epochs],np.nan)\n",
    "acc = np.full([4,4,epochs],np.nan)\n",
    "val_acc = np.full([4,4,epochs],np.nan)\n",
    "\n",
    "# loop through subjects\n",
    "for sub in range(1,2):#6):\n",
    "    # loop through cross validations\n",
    "    for cv in range(1,5):\n",
    "        # load data\n",
    "        filename = foldername + '/' + sub_type + str(sub) + '_' + feat_type + '_dim_' + str(latent_dim) + '_ep_' + str(epochs) + '_bat_' + str(batch_size) + '_' + n_train + '_' + str(train_scale) + '_lr_' + str(int(lr*10000)) \n",
    "        filename += '_cv_'+ str(cv) + '_sparse'\n",
    "        with open(filename + '_hist.p', 'rb') as f:\n",
    "            svae_hist, sae_hist, cnn_hist, vcnn_hist = pickle.load(f)\n",
    "\n",
    "        svae_hist = np.transpose(svae_hist)\n",
    "\n",
    "        # compile losses and accuracies (uncomment back if all models have same # epochs)\n",
    "        loss[cv-1,:2,:] = np.array([svae_hist[2,:], sae_hist['loss']])#, cnn_hist['loss'], vcnn_hist['loss']])\n",
    "        val_loss[cv-1,:2,:] = np.array([svae_hist[9,:], sae_hist['val_loss']])#, cnn_hist['val_loss'], vcnn_hist['val_loss']])\n",
    "        acc[cv-1,:2,:] = np.array([svae_hist[5,:], sae_hist['accuracy']])#, cnn_hist['accuracy'], vcnn_hist['accuracy']])\n",
    "        val_acc[cv-1,:2,:] = np.array([svae_hist[-2,:], sae_hist['val_accuracy']])#, cnn_hist['val_accuracy'], vcnn_hist['val_accuracy']])\n",
    "\n",
    "        # 0-30 for 30 epochs\n",
    "        loss[cv-1,2:,0:30] = np.array([cnn_hist['loss'], vcnn_hist['loss']])\n",
    "        val_loss[cv-1,2:,0:30] = np.array([cnn_hist['val_loss'], vcnn_hist['val_loss']])\n",
    "        acc[cv-1,2:,0:30] = np.array([cnn_hist['accuracy'], vcnn_hist['clf_accuracy']])\n",
    "        val_acc[cv-1,2:,0:30] = np.array([cnn_hist['val_accuracy'], vcnn_hist['val_clf_accuracy']])\n",
    "\n",
    "    # load results\n",
    "    resultsfile = filename\n",
    "    # with open(resultsfile + '_results.p', 'rb') as f:\n",
    "    #     acc_all, acc_clean, acc_noise = pickle.load(f)\n",
    "\n",
    "    # average metrics over cvs\n",
    "    ave_loss = np.mean(loss,axis=0)\n",
    "    ave_val_loss = np.mean(val_loss,axis=0)\n",
    "    ave_acc = np.mean(acc,axis=0)\n",
    "    ave_val_acc = np.mean(val_acc,axis=0)\n",
    "\n",
    "    # plot metrics over epochs\n",
    "    plt.figure(sub)\n",
    "    for i in range(0,4):\n",
    "        ax = plt.subplot(2,2,i+1)\n",
    "        ax.plot(ave_loss[i])\n",
    "        ax.plot(ave_val_loss[i])\n",
    "        # ax.set_ylim(0,5)\n",
    "    plt.figure(sub+1)\n",
    "    for i in range(0,4):\n",
    "        ax2 = plt.subplot(2,2,i+1)\n",
    "        ax2.plot(ave_acc[i])\n",
    "        ax2.plot(ave_val_acc[i])\n",
    "        ax2.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generated or reconstructed features\n",
    "col = ['k','b','r','g','c','y','m']\n",
    "fig = plt.figure()\n",
    "\n",
    "# number of channels\n",
    "ch_max = x_noise.shape[1]\n",
    "\n",
    "# number of classes\n",
    "cl_max = y_train.shape[1]\n",
    "\n",
    "# loop through channels\n",
    "for i in range(0,ch_max-1):\n",
    "    ax = plt.subplot(6,1,i+1)\n",
    "\n",
    "    # loop through classes\n",
    "    for cl in range(2,3):\n",
    "        # index inputs from current class\n",
    "        x_noise_cl = x_noise[y_train[:,cl]==1,i,:]\n",
    "        x_clean_cl = x_clean[y_train[:,cl]==1,i,:]\n",
    "        dec_ind = gen_clf == cl\n",
    "\n",
    "        ## plot all noisy features\n",
    "        # for x_all in range(0,x_noise_cl.shape[0]):\n",
    "        #     ax.plot(x_noise_cl[x_all,:,0],col[cl],linewidth=1)\n",
    "\n",
    "        ## plot all clean features\n",
    "        for x_all in range(0,x_clean_cl.shape[0]):\n",
    "            ax.plot(x_clean_cl[x_all,:,0],col[cl],linewidth=.5,linestyle='-')\n",
    "\n",
    "        ## plot all decoder output\n",
    "        max_gen = 100\n",
    "        ax.plot(np.transpose(dec_out[dec_ind,i,:,0][:max_gen,:]),col[cl+1],linewidth=.5,linestyle='--')\n",
    "        \n",
    "        ## plot mean of noisy features\n",
    "        # ax.plot(np.mean(x_cl[:x_cl.shape[0],:,0],axis=0),col[cl],linewidth=1)\n",
    "\n",
    "        ## plot mean of clean features\n",
    "        # ax.plot(np.mean(x_clean_cl[:x_clean_cl.shape[0],:,0],axis=0),col[cl],linewidth=1,linestyle=':')\n",
    "\n",
    "        ## plot mean of reconstructed output\n",
    "        # rec_cl = dec_out[y_train[:,cl]==1,i,:]\n",
    "        # ax.plot(np.mean(rec_cl[:,:,0],axis=0),col[cl],linewidth=1,linestyle='--')\n",
    "        \n",
    "        ## plot mean of decoder output\n",
    "        # ax.plot(np.mean(np.transpose(dec_out[dec_ind,i,:,0]),axis=1),col[cl+1],linewidth=.5,linestyle='--')\n",
    "\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noise results\n",
    "sub_all, sub_noise, sub_clean, ave_all, ave_noise, ave_clean = loop.ave_results(params, sub_type, train_grp=2, feat_type='feat',epochs=30,n_train='fullgaussflat4',train_scale=5,n_test='partgauss2', latent_dim=4,loop_i='noise', dt='cv')\n",
    "sub_all, sub_noise, sub_clean, flat_ave_all, flat_ave_noise, flat_ave_clean = loop.ave_results(params, sub_type, train_grp=2, feat_type='feat',epochs=30,n_train='fullgaussflat4',train_scale=5,n_test='partflat2', latent_dim=4,loop_i='noise',dt='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_ave_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f80215fd5ceb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_ave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_ave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flat_ave_noise' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy vs. noise\n",
    "fig,ax = plt.subplots(1,3)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'-o')\n",
    "for i in range(4,9):    \n",
    "    ax[1].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'-o')\n",
    "for i in range(9,14):    \n",
    "    ax[2].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'--o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0, 'Type of Noise', ha='center')\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])\n",
    "ax[1].legend(['svcnn-lda','sae-lda','cnn-lda','vcnn-lda','rec-lda'])\n",
    "ax[2].legend(['LDA','LDA-corrupt','QDA','QDA-corrupt','ch'])\n",
    "ax[1].set_yticks([])\n",
    "ax[2].set_yticks([])\n",
    "for i in range(0,3):\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_xticks(range(0,6))\n",
    "    ax[i].set_xticklabels(['Flat','1','2','3','4','5'])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy vs. noise\n",
    "fig,ax = plt.subplots(1,3)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'-o')\n",
    "for i in range(4,9):    \n",
    "    ax[1].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'-o')\n",
    "for i in range(9,14):    \n",
    "    ax[2].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'--o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0, 'Type of Noise', ha='center')\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])\n",
    "ax[1].legend(['svcnn-lda','sae-lda','cnn-lda','vcnn-lda','rec-lda'])\n",
    "ax[2].legend(['LDA','LDA-corrupt','QDA','QDA-corrupt','ch'])\n",
    "ax[1].set_yticks([])\n",
    "ax[2].set_yticks([])\n",
    "for i in range(0,3):\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_xticks(range(0,6))\n",
    "    ax[i].set_xticklabels(['Flat','1','2','3','4','5'])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 - clf_loss: 0.7969 - decoder_accuracy: 0.0039 - clf_accuracy: 0.6803 - val_loss: 1.0422 - val_decoder_loss: 0.0800 - val_clf_loss: 0.9572 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6190\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 2s 126us/sample - loss: 0.8524 - decoder_loss: 0.0848 - clf_loss: 0.7650 - decoder_accuracy: 0.0039 - clf_accuracy: 0.6941 - val_loss: 1.0159 - val_decoder_loss: 0.0789 - val_clf_loss: 0.9367 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6186\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 2s 115us/sample - loss: 0.8229 - decoder_loss: 0.0882 - clf_loss: 0.7323 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7081 - val_loss: 0.9926 - val_decoder_loss: 0.0857 - val_clf_loss: 0.9000 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6271\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.8014 - decoder_loss: 0.0913 - clf_loss: 0.7074 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7258 - val_loss: 1.0692 - val_decoder_loss: 0.0987 - val_clf_loss: 0.9689 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6473\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.7771 - decoder_loss: 0.0939 - clf_loss: 0.6808 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7356 - val_loss: 0.9612 - val_decoder_loss: 0.0850 - val_clf_loss: 0.8723 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6579\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.7592 - decoder_loss: 0.0960 - clf_loss: 0.6605 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7415 - val_loss: 1.0534 - val_decoder_loss: 0.0971 - val_clf_loss: 0.9582 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6359\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 2s 113us/sample - loss: 0.7400 - decoder_loss: 0.0978 - clf_loss: 0.6395 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7510 - val_loss: 0.9241 - val_decoder_loss: 0.0879 - val_clf_loss: 0.8316 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6870\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 2s 115us/sample - loss: 0.7153 - decoder_loss: 0.0994 - clf_loss: 0.6134 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7596 - val_loss: 0.9645 - val_decoder_loss: 0.0872 - val_clf_loss: 0.8736 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6748\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.7093 - decoder_loss: 0.1006 - clf_loss: 0.6063 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7679 - val_loss: 0.9345 - val_decoder_loss: 0.0939 - val_clf_loss: 0.8378 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6786\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.6957 - decoder_loss: 0.1015 - clf_loss: 0.5913 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7700 - val_loss: 1.0018 - val_decoder_loss: 0.0965 - val_clf_loss: 0.9047 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6683\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 2s 119us/sample - loss: 0.6823 - decoder_loss: 0.1022 - clf_loss: 0.5776 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7784 - val_loss: 0.8892 - val_decoder_loss: 0.1014 - val_clf_loss: 0.7889 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7013\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 2s 123us/sample - loss: 0.6701 - decoder_loss: 0.1026 - clf_loss: 0.5647 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7856 - val_loss: 0.9618 - val_decoder_loss: 0.0983 - val_clf_loss: 0.8625 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6625\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 2s 127us/sample - loss: 0.6686 - decoder_loss: 0.1027 - clf_loss: 0.5633 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7844 - val_loss: 0.9086 - val_decoder_loss: 0.0949 - val_clf_loss: 0.8090 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6879\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 2s 121us/sample - loss: 0.6506 - decoder_loss: 0.1029 - clf_loss: 0.5451 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7887 - val_loss: 0.9867 - val_decoder_loss: 0.1028 - val_clf_loss: 0.8810 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6881\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.6377 - decoder_loss: 0.1028 - clf_loss: 0.5323 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7957 - val_loss: 0.9662 - val_decoder_loss: 0.0973 - val_clf_loss: 0.8656 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6989\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 2s 122us/sample - loss: 0.6410 - decoder_loss: 0.1027 - clf_loss: 0.5353 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7954 - val_loss: 0.8722 - val_decoder_loss: 0.0925 - val_clf_loss: 0.7788 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7068\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 2s 121us/sample - loss: 0.6246 - decoder_loss: 0.1024 - clf_loss: 0.5198 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8031 - val_loss: 0.9064 - val_decoder_loss: 0.0979 - val_clf_loss: 0.8021 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7100\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 2s 106us/sample - loss: 0.6225 - decoder_loss: 0.1021 - clf_loss: 0.5176 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8027 - val_loss: 0.9816 - val_decoder_loss: 0.1066 - val_clf_loss: 0.8788 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6984\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 2s 107us/sample - loss: 0.6135 - decoder_loss: 0.1018 - clf_loss: 0.5088 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8022 - val_loss: 0.8649 - val_decoder_loss: 0.0989 - val_clf_loss: 0.7607 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7168\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 2s 109us/sample - loss: 0.6048 - decoder_loss: 0.1014 - clf_loss: 0.5007 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8070 - val_loss: 0.9282 - val_decoder_loss: 0.0952 - val_clf_loss: 0.8287 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7043\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 2s 109us/sample - loss: 0.5918 - decoder_loss: 0.1010 - clf_loss: 0.4884 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8131 - val_loss: 0.9987 - val_decoder_loss: 0.0995 - val_clf_loss: 0.8918 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6973\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 90us/sample - loss: 2.0452 - accuracy: 0.1662 - val_loss: 1.9161 - val_accuracy: 0.2060\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 1.8632 - accuracy: 0.2239 - val_loss: 1.7475 - val_accuracy: 0.2765\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 1.6901 - accuracy: 0.3037 - val_loss: 1.5965 - val_accuracy: 0.3405\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.5500 - accuracy: 0.3784 - val_loss: 1.4882 - val_accuracy: 0.3752\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.4397 - accuracy: 0.4257 - val_loss: 1.4160 - val_accuracy: 0.4010\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.3500 - accuracy: 0.4588 - val_loss: 1.3419 - val_accuracy: 0.4441\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.2722 - accuracy: 0.4868 - val_loss: 1.3386 - val_accuracy: 0.4517\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.2249 - accuracy: 0.5035 - val_loss: 1.2498 - val_accuracy: 0.4789\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.1881 - accuracy: 0.5169 - val_loss: 1.3074 - val_accuracy: 0.4468\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.1456 - accuracy: 0.5278 - val_loss: 1.2494 - val_accuracy: 0.4940\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.1241 - accuracy: 0.5399 - val_loss: 1.2053 - val_accuracy: 0.4921\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 59us/sample - loss: 1.1041 - accuracy: 0.5448 - val_loss: 1.2224 - val_accuracy: 0.4925\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.0819 - accuracy: 0.5511 - val_loss: 1.1890 - val_accuracy: 0.5054\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0698 - accuracy: 0.5538 - val_loss: 1.1664 - val_accuracy: 0.5048\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0640 - accuracy: 0.5659 - val_loss: 1.2042 - val_accuracy: 0.4943\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0506 - accuracy: 0.5635 - val_loss: 1.2515 - val_accuracy: 0.4827\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.0426 - accuracy: 0.5713 - val_loss: 1.1926 - val_accuracy: 0.5060\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0357 - accuracy: 0.5733 - val_loss: 1.1857 - val_accuracy: 0.4978\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0275 - accuracy: 0.5798 - val_loss: 1.1746 - val_accuracy: 0.5159\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0249 - accuracy: 0.5763 - val_loss: 1.1662 - val_accuracy: 0.5121\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0182 - accuracy: 0.5802 - val_loss: 1.1726 - val_accuracy: 0.5071\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0110 - accuracy: 0.5818 - val_loss: 1.2502 - val_accuracy: 0.5006\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0005 - accuracy: 0.5875 - val_loss: 1.1660 - val_accuracy: 0.5143\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 0.9975 - accuracy: 0.5879 - val_loss: 1.1488 - val_accuracy: 0.5149\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9887 - accuracy: 0.5910 - val_loss: 1.1389 - val_accuracy: 0.5175\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 0.9891 - accuracy: 0.5895 - val_loss: 1.1643 - val_accuracy: 0.5098\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 61us/sample - loss: 0.9788 - accuracy: 0.5955 - val_loss: 1.1305 - val_accuracy: 0.5273\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 65us/sample - loss: 0.9813 - accuracy: 0.5941 - val_loss: 1.2040 - val_accuracy: 0.5097\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9716 - accuracy: 0.5968 - val_loss: 1.1121 - val_accuracy: 0.5254\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9666 - accuracy: 0.5965 - val_loss: 1.1255 - val_accuracy: 0.5267\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 89us/sample - loss: 1.6775 - accuracy: 0.3281 - val_loss: 1.9610 - val_accuracy: 0.1538\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 52us/sample - loss: 1.3230 - accuracy: 0.4839 - val_loss: 1.3440 - val_accuracy: 0.4860\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 1.1476 - accuracy: 0.5527 - val_loss: 1.1524 - val_accuracy: 0.5581\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 1.0100 - accuracy: 0.6061 - val_loss: 1.0886 - val_accuracy: 0.5870\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.9111 - accuracy: 0.6429 - val_loss: 0.9760 - val_accuracy: 0.6143\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.8521 - accuracy: 0.6617 - val_loss: 0.8976 - val_accuracy: 0.6594\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.8019 - accuracy: 0.6763 - val_loss: 0.8646 - val_accuracy: 0.6659\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.7555 - accuracy: 0.6914 - val_loss: 0.8689 - val_accuracy: 0.6524\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.7245 - accuracy: 0.7047 - val_loss: 0.8760 - val_accuracy: 0.6614\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.7045 - accuracy: 0.7173 - val_loss: 0.8957 - val_accuracy: 0.6563\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6735 - accuracy: 0.7317 - val_loss: 0.8447 - val_accuracy: 0.6738\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6526 - accuracy: 0.7435 - val_loss: 0.9342 - val_accuracy: 0.6554\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6371 - accuracy: 0.7534 - val_loss: 0.8494 - val_accuracy: 0.6838\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 51us/sample - loss: 0.6186 - accuracy: 0.7587 - val_loss: 0.8839 - val_accuracy: 0.6838\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 50us/sample - loss: 0.6085 - accuracy: 0.7623 - val_loss: 0.9472 - val_accuracy: 0.6600\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5867 - accuracy: 0.7740 - val_loss: 0.8320 - val_accuracy: 0.6911\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5762 - accuracy: 0.7780 - val_loss: 0.8080 - val_accuracy: 0.7032\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5667 - accuracy: 0.7794 - val_loss: 0.7982 - val_accuracy: 0.6952\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5558 - accuracy: 0.7869 - val_loss: 0.8498 - val_accuracy: 0.6967\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 49us/sample - loss: 0.5539 - accuracy: 0.7866 - val_loss: 0.7998 - val_accuracy: 0.7084\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5414 - accuracy: 0.7928 - val_loss: 0.7803 - val_accuracy: 0.7173\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5390 - accuracy: 0.7937 - val_loss: 0.7976 - val_accuracy: 0.7108\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5285 - accuracy: 0.7958 - val_loss: 0.7989 - val_accuracy: 0.7178\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.5201 - accuracy: 0.7971 - val_loss: 0.7965 - val_accuracy: 0.7110\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5206 - accuracy: 0.7980 - val_loss: 0.7861 - val_accuracy: 0.7035\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.5158 - accuracy: 0.8025 - val_loss: 0.7738 - val_accuracy: 0.7076\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.4999 - accuracy: 0.8040 - val_loss: 0.7668 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 46us/sample - loss: 0.5034 - accuracy: 0.8053 - val_loss: 0.8087 - val_accuracy: 0.7011\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.4942 - accuracy: 0.8090 - val_loss: 0.9211 - val_accuracy: 0.6771\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.4908 - accuracy: 0.8116 - val_loss: 0.7583 - val_accuracy: 0.7241\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.5095 - accuracy: 0.2328 - val_loss: 0.4812 - val_accuracy: 0.2057\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.4240 - accuracy: 0.3496 - val_loss: 0.4153 - val_accuracy: 0.3584\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3843 - accuracy: 0.4206 - val_loss: 0.3910 - val_accuracy: 0.3819\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3542 - accuracy: 0.4760 - val_loss: 0.3744 - val_accuracy: 0.3992\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3293 - accuracy: 0.5207 - val_loss: 0.3395 - val_accuracy: 0.4692\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.3120 - accuracy: 0.5492 - val_loss: 0.3215 - val_accuracy: 0.5054\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.3000 - accuracy: 0.5707 - val_loss: 0.3261 - val_accuracy: 0.5073\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2897 - accuracy: 0.5913 - val_loss: 0.3152 - val_accuracy: 0.5273\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2799 - accuracy: 0.6065 - val_loss: 0.3053 - val_accuracy: 0.5476\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2735 - accuracy: 0.6206 - val_loss: 0.3049 - val_accuracy: 0.5617\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2648 - accuracy: 0.6347 - val_loss: 0.3034 - val_accuracy: 0.5713\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2603 - accuracy: 0.6458 - val_loss: 0.2934 - val_accuracy: 0.5776\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2575 - accuracy: 0.6497 - val_loss: 0.2974 - val_accuracy: 0.5857\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2520 - accuracy: 0.6616 - val_loss: 0.2931 - val_accuracy: 0.5900\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2472 - accuracy: 0.6751 - val_loss: 0.3072 - val_accuracy: 0.5817\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2440 - accuracy: 0.6779 - val_loss: 0.2874 - val_accuracy: 0.6021\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 65us/sample - loss: 0.2402 - accuracy: 0.6923 - val_loss: 0.2886 - val_accuracy: 0.6121\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2360 - accuracy: 0.6984 - val_loss: 0.2876 - val_accuracy: 0.6183\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2317 - accuracy: 0.7051 - val_loss: 0.2863 - val_accuracy: 0.6184\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2309 - accuracy: 0.7067 - val_loss: 0.2898 - val_accuracy: 0.6224\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2262 - accuracy: 0.7174 - val_loss: 0.2742 - val_accuracy: 0.6432\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2235 - accuracy: 0.7241 - val_loss: 0.2766 - val_accuracy: 0.6454\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2217 - accuracy: 0.7274 - val_loss: 0.2789 - val_accuracy: 0.6463\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2210 - accuracy: 0.7271 - val_loss: 0.2596 - val_accuracy: 0.6684\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2167 - accuracy: 0.7336 - val_loss: 0.2770 - val_accuracy: 0.6490\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2140 - accuracy: 0.7444 - val_loss: 0.2644 - val_accuracy: 0.6713\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2133 - accuracy: 0.7457 - val_loss: 0.2730 - val_accuracy: 0.6516\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2112 - accuracy: 0.7465 - val_loss: 0.2732 - val_accuracy: 0.6590\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 72us/sample - loss: 0.2097 - accuracy: 0.7504 - val_loss: 0.2687 - val_accuracy: 0.6424\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 72us/sample - loss: 0.2103 - accuracy: 0.7489 - val_loss: 0.2610 - val_accuracy: 0.6662\n"
     ]
    }
   ],
   "source": [
    "# Loop through noise\n",
    "acc_all, acc_noise, acc_clean, file_name = loop.loop_noise(raw, params, sub_type, load=False, n_train='fullgaussflat4', train_scale=5, n_test=0,epochs=30, batch_size = 128, sparsity=True,dt='cv',feat_type='feat',noise=True, latent_dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dimension Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through latent dimensions\n",
    "sub_all, sub_noise, sub_clean, file_name = loop.loop_alldim(raw, params, sub_type, load=False, n_train='gaussflat', train_scale=3, n_test='gauss', test_scale=1,epochs=30, sparsity=True, dt='0414')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from latent dimension loop, sparse vs. not sparse\n",
    "sub_all, sub_noise, sub_clean, sparse_all, sparse_noise, sparse_clean = loop.load_results(params, sub_type=sub_type,sparsity=True, dt='0414')\n",
    "sub_all, sub_noise, sub_clean, ave_all, ave_noise, ave_clean = loop.load_results(params, sub_type=sub_type,sparsity=False, dt='0414')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. latent dimension\n",
    "fig,ax = plt.subplots(1,2)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(ave_noise[:,i],'-o')\n",
    "    ax[1].plot(sparse_noise[:,i],'-o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0.04, 'Latent Dimension', ha='center')\n",
    "ax[0].set_ylim(0.5,1)\n",
    "ax[1].set_ylim(0.5,1)\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
