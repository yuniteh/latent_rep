{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "from matplotlib import pyplot as plt\n",
    "import loop\n",
    "import session\n",
    "import plot_utils \n",
    "set_gpu()\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'TR'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sub 2, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.834 5.411 1.421 0.059 0.474]\n",
      "[9.044 8.197 0.845 0.057 0.678]\n",
      "[7.644 6.91  0.732 0.055 0.712]\n",
      "[8.898 8.227 0.669 0.055 0.728]\n",
      "[9.12  8.525 0.593 0.056 0.758]\n",
      "[9.922 9.376 0.544 0.056 0.78 ]\n",
      "[10.28   9.781  0.496  0.055  0.797]\n",
      "[10.966 10.495  0.468  0.056  0.806]\n",
      "[12.152 11.68   0.47   0.056  0.813]\n",
      "[12.298 11.864  0.432  0.056  0.826]\n",
      "[11.972 11.549  0.421  0.056  0.831]\n",
      "[13.207 12.784  0.421  0.057  0.83 ]\n",
      "[13.092 12.692  0.398  0.057  0.84 ]\n",
      "[13.278 12.885  0.391  0.057  0.838]\n",
      "[13.126 12.734  0.39   0.057  0.833]\n",
      "[5.24  4.679 0.56  0.057 0.798]\n",
      "[4.225 3.67  0.554 0.056 0.818]\n",
      "[3.211 2.663 0.546 0.056 0.839]\n",
      "[2.929 2.393 0.534 0.056 0.836]\n",
      "[2.76  2.237 0.521 0.056 0.843]\n",
      "[2.653 2.133 0.518 0.055 0.837]\n",
      "[2.594 2.087 0.505 0.056 0.837]\n",
      "[2.539 2.039 0.499 0.056 0.838]\n",
      "[2.515 2.    0.514 0.055 0.828]\n",
      "[2.486 1.965 0.519 0.055 0.824]\n",
      "[2.477 1.961 0.514 0.056 0.823]\n",
      "[2.438 1.932 0.504 0.056 0.826]\n",
      "[2.403 1.922 0.479 0.056 0.833]\n",
      "[2.376 1.914 0.461 0.056 0.839]\n",
      "[2.362 1.895 0.465 0.056 0.836]\n",
      "[2.367 1.89  0.476 0.057 0.833]\n",
      "[2.334 1.883 0.45  0.057 0.837]\n",
      "[2.311 1.873 0.437 0.057 0.841]\n",
      "[2.303 1.874 0.428 0.056 0.846]\n",
      "[2.292 1.873 0.417 0.057 0.848]\n",
      "[2.287 1.868 0.418 0.057 0.847]\n",
      "[2.269 1.858 0.409 0.057 0.85 ]\n",
      "[2.26  1.85  0.408 0.057 0.85 ]\n",
      "[2.253 1.853 0.399 0.057 0.853]\n",
      "[2.241 1.843 0.397 0.057 0.855]\n",
      "[2.23  1.837 0.393 0.057 0.857]\n",
      "[2.235 1.831 0.403 0.057 0.854]\n",
      "[2.225 1.827 0.396 0.057 0.857]\n",
      "[2.225 1.827 0.396 0.057 0.854]\n",
      "[2.231 1.811 0.419 0.057 0.847]\n",
      "[2.213 1.803 0.409 0.057 0.851]\n",
      "[2.204 1.801 0.401 0.057 0.855]\n",
      "[2.206 1.798 0.406 0.057 0.854]\n",
      "[2.212 1.793 0.417 0.057 0.848]\n",
      "[2.22  1.784 0.435 0.057 0.846]\n",
      "Running sub 3, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[5.899 4.405 1.493 0.06  0.24 ]\n",
      "[8.528 7.162 1.364 0.06  0.437]\n",
      "[9.446 7.97  1.475 0.057 0.416]\n",
      "[9.123 7.662 1.459 0.053 0.44 ]\n",
      "[9.848 8.364 1.482 0.051 0.444]\n",
      "[10.132  8.733  1.397  0.053  0.498]\n",
      "[9.903 8.595 1.305 0.054 0.537]\n",
      "[9.892 8.587 1.304 0.053 0.54 ]\n",
      "[10.129  8.841  1.286  0.054  0.561]\n",
      "[10.271  8.971  1.298  0.054  0.548]\n",
      "[9.592 8.332 1.258 0.055 0.562]\n",
      "[10.227  8.934  1.292  0.053  0.571]\n",
      "[10.451  9.157  1.292  0.052  0.569]\n",
      "[10.071  8.784  1.285  0.052  0.571]\n",
      "[10.189  8.893  1.294  0.053  0.57 ]\n",
      "[6.099 4.82  1.276 0.055 0.559]\n",
      "[4.755 3.533 1.219 0.055 0.546]\n",
      "[4.046 2.907 1.137 0.056 0.568]\n",
      "[3.739 2.633 1.105 0.055 0.571]\n",
      "[3.606 2.484 1.12  0.055 0.558]\n",
      "[3.551 2.442 1.108 0.055 0.572]\n",
      "[3.524 2.396 1.126 0.055 0.569]\n",
      "[3.48  2.345 1.133 0.055 0.559]\n",
      "[3.444 2.295 1.148 0.055 0.56 ]\n",
      "[3.466 2.301 1.164 0.056 0.568]\n",
      "[3.48  2.279 1.2   0.054 0.553]\n",
      "[3.487 2.271 1.214 0.055 0.549]\n",
      "[3.505 2.248 1.255 0.055 0.541]\n",
      "[3.485 2.234 1.25  0.054 0.546]\n",
      "[3.48  2.21  1.269 0.054 0.546]\n",
      "[3.504 2.226 1.277 0.055 0.556]\n",
      "[3.478 2.196 1.28  0.054 0.539]\n",
      "[3.481 2.195 1.285 0.055 0.533]\n",
      "[3.45  2.178 1.271 0.054 0.543]\n",
      "[3.479 2.181 1.297 0.056 0.542]\n",
      "[3.477 2.175 1.3   0.056 0.532]\n",
      "[3.445 2.163 1.28  0.056 0.54 ]\n",
      "[3.467 2.167 1.299 0.056 0.541]\n",
      "[3.508 2.168 1.339 0.056 0.541]\n",
      "[3.471 2.154 1.316 0.055 0.551]\n",
      "[3.489 2.151 1.337 0.054 0.564]\n",
      "[3.525 2.156 1.368 0.055 0.554]\n",
      "[3.538 2.143 1.394 0.054 0.549]\n",
      "[3.524 2.143 1.38  0.055 0.554]\n",
      "[3.513 2.138 1.374 0.054 0.556]\n",
      "[3.556 2.15  1.404 0.054 0.547]\n",
      "[3.541 2.136 1.404 0.054 0.546]\n",
      "[3.597 2.146 1.45  0.056 0.543]\n",
      "[3.628 2.141 1.486 0.055 0.548]\n",
      "[3.633 2.152 1.479 0.055 0.567]\n",
      "Running sub 4, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[5.976 4.463 1.512 0.06  0.35 ]\n",
      "[111.875 110.75    1.124   0.06    0.515]\n",
      "[30.977 30.078  0.898  0.055  0.585]\n",
      "[13.931 13.22   0.71   0.048  0.655]\n",
      "[12.629 11.999  0.628  0.045  0.698]\n",
      "[14.664 13.997  0.666  0.043  0.68 ]\n",
      "[13.243 12.601  0.64   0.043  0.698]\n",
      "[13.883 13.247  0.635  0.042  0.718]\n",
      "[13.604 12.993  0.609  0.042  0.732]\n",
      "[17.194 16.573  0.619  0.043  0.73 ]\n",
      "[15.645 15.028  0.615  0.043  0.734]\n",
      "[17.52  16.903  0.615  0.043  0.734]\n",
      "[15.406 14.812  0.593  0.043  0.743]\n",
      "[16.417 15.827  0.588  0.042  0.741]\n",
      "[14.724 14.16   0.561  0.043  0.752]\n",
      "[5.241 4.556 0.684 0.042 0.712]\n",
      "[3.929 3.292 0.637 0.043 0.738]\n",
      "[3.452 2.853 0.599 0.042 0.75 ]\n",
      "[3.143 2.546 0.596 0.041 0.752]\n",
      "[3.001 2.408 0.592 0.042 0.751]\n",
      "[2.89  2.293 0.596 0.042 0.75 ]\n",
      "[2.835 2.233 0.601 0.042 0.748]\n",
      "[2.801 2.195 0.605 0.041 0.74 ]\n",
      "[2.772 2.163 0.609 0.042 0.741]\n",
      "[2.76  2.137 0.622 0.042 0.738]\n",
      "[2.745 2.12  0.623 0.042 0.733]\n",
      "[2.727 2.092 0.635 0.042 0.726]\n",
      "[2.706 2.067 0.638 0.042 0.73 ]\n",
      "[2.701 2.058 0.642 0.041 0.729]\n",
      "[2.695 2.045 0.648 0.041 0.726]\n",
      "[2.689 2.031 0.657 0.041 0.725]\n",
      "[2.672 2.022 0.649 0.041 0.729]\n",
      "[2.646 2.015 0.629 0.041 0.736]\n",
      "[2.63  2.007 0.623 0.042 0.738]\n",
      "[2.62  2.003 0.615 0.041 0.738]\n",
      "[2.63  2.012 0.617 0.041 0.739]\n",
      "[2.616 2.006 0.609 0.041 0.74 ]\n",
      "[2.609 2.007 0.601 0.041 0.74 ]\n",
      "[2.603 2.009 0.592 0.041 0.747]\n",
      "[2.59  2.004 0.585 0.041 0.747]\n",
      "[2.593 2.006 0.586 0.041 0.747]\n",
      "[2.604 2.007 0.597 0.041 0.752]\n",
      "[2.583 1.992 0.591 0.041 0.755]\n",
      "[2.572 1.996 0.576 0.041 0.76 ]\n",
      "[2.579 1.99  0.587 0.041 0.756]\n",
      "[2.562 1.98  0.581 0.041 0.759]\n",
      "[2.566 1.981 0.584 0.041 0.759]\n",
      "[2.53  1.953 0.576 0.042 0.761]\n",
      "[2.544 1.957 0.587 0.041 0.757]\n",
      "[2.532 1.956 0.576 0.042 0.758]\n",
      "Running sub 5, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[5.827 4.315 1.511 0.06  0.316]\n",
      "[7.914 7.004 0.908 0.06  0.633]\n",
      "[8.318 7.643 0.674 0.059 0.735]\n",
      "[8.63  8.034 0.594 0.059 0.772]\n",
      "[9.2   8.644 0.555 0.058 0.785]\n",
      "[9.525 8.982 0.542 0.058 0.787]\n",
      "[9.446 8.907 0.537 0.059 0.782]\n",
      "[9.315 8.776 0.538 0.059 0.789]\n",
      "[9.927 9.392 0.533 0.059 0.798]\n",
      "[9.64  9.118 0.52  0.059 0.804]\n",
      "[9.835 9.304 0.529 0.06  0.8  ]\n",
      "[10.134  9.616  0.516  0.06   0.804]\n",
      "[10.899 10.374  0.523  0.06   0.806]\n",
      "[11.312 10.784  0.527  0.06   0.805]\n",
      "[10.988 10.473  0.513  0.06   0.81 ]\n",
      "[4.422 3.895 0.526 0.06  0.809]\n",
      "[3.757 3.244 0.512 0.06  0.825]\n",
      "[3.395 2.872 0.521 0.06  0.826]\n",
      "[3.265 2.748 0.515 0.059 0.823]\n",
      "[3.176 2.659 0.516 0.06  0.822]\n",
      "[3.126 2.614 0.51  0.059 0.821]\n",
      "[3.078 2.566 0.511 0.059 0.826]\n",
      "[3.027 2.515 0.511 0.059 0.824]\n",
      "[2.997 2.501 0.495 0.059 0.828]\n",
      "[2.98  2.483 0.496 0.059 0.823]\n",
      "[2.957 2.478 0.477 0.06  0.831]\n",
      "[2.941 2.453 0.487 0.06  0.826]\n",
      "[2.921 2.439 0.481 0.059 0.828]\n",
      "[2.922 2.435 0.486 0.06  0.825]\n",
      "[2.893 2.417 0.475 0.059 0.832]\n",
      "[2.888 2.413 0.474 0.06  0.83 ]\n",
      "[2.885 2.394 0.49  0.06  0.823]\n",
      "[2.878 2.395 0.482 0.059 0.824]\n",
      "[2.878 2.389 0.488 0.059 0.824]\n",
      "[2.878 2.396 0.481 0.06  0.827]\n",
      "[2.878 2.381 0.496 0.06  0.814]\n",
      "[2.876 2.376 0.499 0.06  0.815]\n",
      "[2.865 2.364 0.5   0.06  0.814]\n",
      "[2.873 2.367 0.504 0.059 0.812]\n",
      "[2.855 2.355 0.499 0.06  0.814]\n",
      "[2.856 2.353 0.502 0.06  0.815]\n",
      "[2.858 2.352 0.504 0.059 0.813]\n",
      "[2.869 2.345 0.524 0.06  0.804]\n",
      "[2.873 2.34  0.533 0.06  0.805]\n",
      "[2.877 2.332 0.544 0.06  0.801]\n",
      "[2.875 2.334 0.54  0.059 0.8  ]\n",
      "[2.877 2.336 0.54  0.06  0.802]\n",
      "[2.876 2.325 0.55  0.06  0.801]\n",
      "[2.869 2.328 0.541 0.059 0.8  ]\n",
      "[2.865 2.311 0.553 0.059 0.799]\n",
      "Running sub 6, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.831 5.155 1.675 0.06  0.232]\n",
      "[13.097 11.77   1.326  0.06   0.462]\n",
      "[9.079 7.808 1.269 0.06  0.541]\n",
      "[11.065  9.816  1.248  0.059  0.554]\n",
      "[9.148 7.829 1.317 0.06  0.552]\n",
      "[8.826 7.526 1.298 0.06  0.561]\n",
      "[8.949 7.591 1.356 0.06  0.556]\n",
      "[9.138 7.755 1.382 0.06  0.552]\n",
      "[8.756 7.382 1.372 0.06  0.56 ]\n",
      "[9.042 7.668 1.373 0.06  0.557]\n",
      "[9.108 7.719 1.388 0.06  0.553]\n",
      "[9.138 7.713 1.423 0.06  0.546]\n",
      "[8.913 7.497 1.414 0.06  0.546]\n",
      "[9.025 7.554 1.469 0.06  0.539]\n",
      "[9.142 7.684 1.456 0.06  0.542]\n",
      "[5.024 3.836 1.186 0.06  0.563]\n",
      "[4.311 3.106 1.203 0.06  0.551]\n",
      "[4.052 2.867 1.184 0.06  0.555]\n",
      "[3.941 2.76  1.18  0.06  0.547]\n",
      "[3.876 2.691 1.183 0.06  0.548]\n",
      "[3.834 2.651 1.181 0.06  0.549]\n",
      "[3.794 2.621 1.172 0.06  0.56 ]\n",
      "[3.782 2.611 1.17  0.06  0.558]\n",
      "[3.746 2.584 1.16  0.06  0.566]\n",
      "[3.732 2.576 1.154 0.06  0.566]\n",
      "[3.734 2.563 1.169 0.06  0.568]\n",
      "[3.721 2.55  1.17  0.06  0.575]\n",
      "[3.733 2.555 1.176 0.06  0.575]\n",
      "[3.711 2.531 1.178 0.06  0.568]\n",
      "[3.713 2.526 1.186 0.06  0.563]\n",
      "[3.737 2.539 1.197 0.06  0.566]\n",
      "[3.708 2.505 1.201 0.06  0.57 ]\n",
      "[3.732 2.509 1.222 0.06  0.566]\n",
      "[3.728 2.502 1.225 0.06  0.564]\n",
      "[3.733 2.492 1.239 0.06  0.564]\n",
      "[3.732 2.491 1.239 0.06  0.567]\n",
      "[3.739 2.493 1.244 0.06  0.568]\n",
      "[3.723 2.476 1.245 0.06  0.566]\n",
      "[3.731 2.472 1.257 0.06  0.562]\n",
      "[3.732 2.466 1.265 0.06  0.561]\n",
      "[3.747 2.464 1.282 0.06  0.56 ]\n",
      "[3.77  2.453 1.316 0.06  0.559]\n",
      "[3.778 2.454 1.323 0.06  0.562]\n",
      "[3.778 2.451 1.325 0.06  0.562]\n",
      "[3.781 2.454 1.327 0.06  0.566]\n",
      "[3.802 2.442 1.359 0.06  0.563]\n",
      "[3.845 2.451 1.394 0.06  0.565]\n",
      "[3.838 2.451 1.385 0.06  0.557]\n",
      "[3.857 2.441 1.414 0.06  0.559]\n",
      "[3.895 2.444 1.45  0.06  0.563]\n"
     ]
    }
   ],
   "source": [
    "# Loop through training\n",
    "train_dict = {'sub_type':sub_type,'n_train':'fullgaussflat4', 'load':True, 'train_scale':5, 'epochs': 30, 'batch_size' : 128, 'sparsity':True,'dt':'manual','feat_type':'feat','noise':True, 'latent_dim':4,'mod':['lda'],'gens':50, 'mod_dt':'1012','train_grp':2}\n",
    "train_sess = session.Session(**train_dict)\n",
    "\n",
    "# loop through subjects\n",
    "for sub_i in range(2,np.max(params[:,0])+1):\n",
    "    for lat in range(5,6):#1,11):\n",
    "        train_sess.latent_dim = lat\n",
    "        train_out = train_sess.loop_cv(raw,params,sub=sub_i,mod=['ext_c','aligned'])\n",
    "        # for key,val in train_out.items():\n",
    "        #     exec(key + '=val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sub 1, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[5.893 4.266 1.626 0.059 0.347]\n",
      "[34.763 33.265  1.497  0.058  0.43 ]\n",
      "[20.604 19.236  1.367  0.057  0.491]\n",
      "[15.318 14.079  1.237  0.057  0.547]\n",
      "[13.256 12.105  1.15   0.057  0.576]\n",
      "[13.679 12.636  1.042  0.058  0.598]\n",
      "[11.903 10.872  1.029  0.058  0.614]\n",
      "[10.947  9.934  1.011  0.058  0.617]\n",
      "[10.734  9.795  0.938  0.058  0.651]\n",
      "[10.828  9.911  0.916  0.058  0.661]\n",
      "[11.065 10.129  0.935  0.058  0.653]\n",
      "[11.116 10.162  0.953  0.058  0.647]\n",
      "[11.52  10.547  0.972  0.059  0.642]\n",
      "[11.138 10.196  0.941  0.059  0.657]\n",
      "[11.357 10.448  0.908  0.058  0.673]\n",
      "[4.92  4.11  0.809 0.059 0.679]\n",
      "[4.047 3.232 0.814 0.059 0.696]\n",
      "[3.515 2.695 0.819 0.059 0.704]\n",
      "[3.303 2.443 0.86  0.059 0.69 ]\n",
      "[3.244 2.366 0.876 0.059 0.674]\n",
      "[3.075 2.223 0.851 0.058 0.695]\n",
      "[3.046 2.186 0.859 0.059 0.696]\n",
      "[3.022 2.15  0.87  0.058 0.689]\n",
      "[3.004 2.121 0.882 0.059 0.68 ]\n",
      "[2.994 2.088 0.904 0.059 0.675]\n",
      "[2.973 2.07  0.903 0.059 0.676]\n",
      "[2.969 2.053 0.915 0.059 0.672]\n",
      "[2.949 2.026 0.923 0.059 0.663]\n",
      "[2.937 2.02  0.916 0.059 0.664]\n",
      "[2.931 2.009 0.921 0.059 0.668]\n",
      "[2.9   2.005 0.894 0.059 0.675]\n",
      "[2.883 1.983 0.899 0.059 0.673]\n",
      "[2.878 1.984 0.893 0.059 0.678]\n",
      "[2.858 1.966 0.892 0.059 0.68 ]\n",
      "[2.843 1.97  0.872 0.059 0.687]\n",
      "[2.821 1.949 0.871 0.059 0.686]\n",
      "[2.838 1.953 0.884 0.059 0.682]\n",
      "[2.816 1.936 0.878 0.059 0.689]\n",
      "[2.816 1.937 0.879 0.059 0.687]\n",
      "[2.8   1.927 0.872 0.059 0.693]\n",
      "[2.811 1.93  0.879 0.059 0.688]\n",
      "[2.808 1.925 0.882 0.059 0.687]\n",
      "[2.794 1.91  0.882 0.059 0.69 ]\n",
      "[2.818 1.92  0.896 0.059 0.688]\n",
      "[2.814 1.909 0.903 0.059 0.68 ]\n",
      "[2.786 1.905 0.88  0.059 0.691]\n",
      "[2.809 1.903 0.906 0.059 0.682]\n",
      "[2.798 1.904 0.892 0.059 0.689]\n",
      "[2.776 1.898 0.876 0.059 0.693]\n",
      "[2.789 1.895 0.893 0.059 0.689]\n",
      "Running sub 2, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[7.729 5.903 1.825 0.059 0.257]\n",
      "[8.102 6.774 1.327 0.058 0.503]\n",
      "[8.968 7.823 1.143 0.056 0.589]\n",
      "[9.515 8.539 0.975 0.055 0.657]\n",
      "[10.355  9.439  0.914  0.056  0.682]\n",
      "[10.124  9.304  0.818  0.056  0.714]\n",
      "[10.158  9.359  0.797  0.057  0.721]\n",
      "[10.446  9.675  0.769  0.057  0.722]\n",
      "[10.589  9.821  0.766  0.057  0.723]\n",
      "[10.906 10.143  0.761  0.057  0.726]\n",
      "[11.014 10.265  0.747  0.057  0.737]\n",
      "[11.268 10.513  0.753  0.057  0.731]\n",
      "[11.428 10.679  0.747  0.057  0.737]\n",
      "[11.708 10.949  0.757  0.057  0.73 ]\n",
      "[11.923 11.165  0.756  0.058  0.732]\n",
      "[5.008 4.092 0.915 0.058 0.69 ]\n",
      "[3.592 2.67  0.921 0.057 0.712]\n",
      "[3.319 2.482 0.836 0.057 0.723]\n",
      "[3.146 2.305 0.839 0.057 0.726]\n",
      "[3.028 2.183 0.844 0.058 0.724]\n",
      "[2.979 2.143 0.835 0.057 0.72 ]\n",
      "[3.036 2.136 0.899 0.057 0.692]\n",
      "[2.907 2.103 0.803 0.057 0.728]\n",
      "[2.868 2.087 0.779 0.057 0.733]\n",
      "[2.852 2.075 0.777 0.058 0.732]\n",
      "[2.807 2.053 0.753 0.057 0.739]\n",
      "[2.792 2.046 0.745 0.057 0.743]\n",
      "[2.773 2.034 0.739 0.057 0.744]\n",
      "[2.742 2.006 0.735 0.057 0.745]\n",
      "[2.728 2.001 0.726 0.057 0.748]\n",
      "[2.727 1.993 0.733 0.057 0.741]\n",
      "[2.706 1.981 0.724 0.058 0.745]\n",
      "[2.683 1.971 0.711 0.058 0.753]\n",
      "[2.676 1.97  0.705 0.058 0.755]\n",
      "[2.65  1.957 0.693 0.057 0.757]\n",
      "[2.647 1.951 0.695 0.058 0.757]\n",
      "[2.638 1.951 0.686 0.058 0.76 ]\n",
      "[2.618 1.936 0.681 0.057 0.767]\n",
      "[2.614 1.93  0.683 0.057 0.763]\n",
      "[2.6   1.92  0.68  0.058 0.761]\n",
      "[2.6   1.919 0.68  0.058 0.765]\n",
      "[2.595 1.912 0.682 0.058 0.763]\n",
      "[2.593 1.913 0.679 0.057 0.765]\n",
      "[2.592 1.911 0.68  0.058 0.763]\n",
      "[2.585 1.909 0.675 0.058 0.767]\n",
      "[2.576 1.9   0.675 0.058 0.766]\n",
      "[2.582 1.901 0.68  0.058 0.758]\n",
      "[2.579 1.898 0.68  0.058 0.762]\n",
      "[2.578 1.897 0.681 0.058 0.762]\n",
      "[2.573 1.887 0.685 0.058 0.76 ]\n",
      "Running sub 3, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.69  4.978 1.711 0.059 0.314]\n",
      "[8.107 7.017 1.089 0.055 0.647]\n",
      "[8.787 7.878 0.907 0.05  0.714]\n",
      "[9.605 8.728 0.876 0.05  0.729]\n",
      "[10.579  9.702  0.875  0.048  0.741]\n",
      "[10.22   9.362  0.857  0.049  0.751]\n",
      "[11.368 10.476  0.89   0.049  0.747]\n",
      "[11.252 10.376  0.875  0.049  0.762]\n",
      "[12.121 11.256  0.863  0.049  0.766]\n",
      "[12.027 11.184  0.842  0.049  0.776]\n",
      "[12.728 11.868  0.858  0.05   0.774]\n",
      "[13.336 12.428  0.906  0.049  0.771]\n",
      "[13.703 12.758  0.943  0.05   0.773]\n",
      "[13.608 12.658  0.949  0.05   0.772]\n",
      "[14.621 13.62   0.999  0.049  0.763]\n",
      "[4.88  4.048 0.831 0.048 0.726]\n",
      "[3.835 3.087 0.746 0.048 0.778]\n",
      "[3.423 2.671 0.751 0.048 0.765]\n",
      "[3.048 2.259 0.787 0.048 0.751]\n",
      "[2.872 2.137 0.733 0.048 0.768]\n",
      "[2.759 2.008 0.749 0.048 0.755]\n",
      "[2.677 1.953 0.723 0.047 0.767]\n",
      "[2.633 1.923 0.708 0.048 0.775]\n",
      "[2.572 1.879 0.692 0.048 0.782]\n",
      "[2.541 1.851 0.688 0.047 0.787]\n",
      "[2.516 1.83  0.685 0.047 0.788]\n",
      "[2.494 1.812 0.68  0.048 0.787]\n",
      "[2.474 1.802 0.671 0.047 0.789]\n",
      "[2.467 1.787 0.679 0.047 0.785]\n",
      "[2.464 1.78  0.683 0.047 0.781]\n",
      "[2.445 1.764 0.68  0.047 0.782]\n",
      "[2.423 1.758 0.664 0.047 0.79 ]\n",
      "[2.429 1.755 0.673 0.047 0.79 ]\n",
      "[2.426 1.749 0.675 0.048 0.789]\n",
      "[2.413 1.736 0.676 0.048 0.785]\n",
      "[2.401 1.722 0.678 0.047 0.783]\n",
      "[2.404 1.715 0.688 0.047 0.78 ]\n",
      "[2.391 1.705 0.685 0.047 0.781]\n",
      "[2.4   1.711 0.688 0.047 0.779]\n",
      "[2.382 1.701 0.681 0.048 0.788]\n",
      "[2.408 1.704 0.703 0.048 0.78 ]\n",
      "[2.408 1.706 0.702 0.048 0.779]\n",
      "[2.405 1.695 0.709 0.048 0.774]\n",
      "[2.419 1.689 0.729 0.048 0.767]\n",
      "[2.399 1.686 0.712 0.048 0.77 ]\n",
      "[2.41  1.675 0.734 0.048 0.763]\n",
      "[2.42  1.67  0.749 0.048 0.758]\n",
      "[2.428 1.674 0.753 0.048 0.758]\n",
      "[2.428 1.667 0.76  0.048 0.759]\n",
      "[2.409 1.664 0.744 0.048 0.76 ]\n",
      "Running sub 4, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[22.342 20.845  1.496  0.059  0.439]\n",
      "[11.543 10.423  1.118  0.056  0.606]\n",
      "[11.552 10.477  1.073  0.05   0.621]\n",
      "[12.381 11.36   1.02   0.051  0.634]\n",
      "[11.837 10.873  0.963  0.049  0.656]\n",
      "[13.16  12.172  0.987  0.051  0.656]\n",
      "[11.943 10.996  0.946  0.051  0.68 ]\n",
      "[13.078 12.11   0.966  0.052  0.674]\n",
      "[12.768 11.811  0.955  0.051  0.687]\n",
      "[12.973 12.011  0.96   0.051  0.697]\n",
      "[12.964 11.981  0.982  0.051  0.689]\n",
      "[12.653 11.651  1.001  0.051  0.694]\n",
      "[13.294 12.265  1.027  0.052  0.692]\n",
      "[12.7   11.649  1.05   0.053  0.69 ]\n",
      "[13.305 12.237  1.066  0.052  0.685]\n",
      "[6.144 5.141 1.001 0.053 0.677]\n",
      "[4.696 3.745 0.95  0.052 0.695]\n",
      "[3.87  2.99  0.878 0.052 0.704]\n",
      "[3.392 2.52  0.87  0.052 0.704]\n",
      "[3.192 2.315 0.876 0.051 0.701]\n",
      "[3.064 2.178 0.885 0.052 0.698]\n",
      "[3.012 2.116 0.895 0.051 0.691]\n",
      "[2.967 2.066 0.899 0.052 0.691]\n",
      "[2.937 2.039 0.897 0.053 0.694]\n",
      "[2.92  2.016 0.902 0.052 0.698]\n",
      "[2.902 1.991 0.909 0.052 0.698]\n",
      "[2.902 1.983 0.917 0.053 0.693]\n",
      "[2.874 1.967 0.905 0.053 0.695]\n",
      "[2.884 1.96  0.923 0.053 0.693]\n",
      "[2.868 1.947 0.919 0.053 0.699]\n",
      "[2.87  1.948 0.922 0.053 0.703]\n",
      "[2.862 1.928 0.933 0.053 0.7  ]\n",
      "[2.874 1.932 0.94  0.053 0.702]\n",
      "[2.866 1.917 0.948 0.053 0.697]\n",
      "[2.872 1.914 0.957 0.053 0.696]\n",
      "[2.858 1.907 0.949 0.053 0.702]\n",
      "[2.862 1.899 0.961 0.053 0.698]\n",
      "[2.865 1.897 0.967 0.053 0.698]\n",
      "[2.861 1.89  0.97  0.053 0.696]\n",
      "[2.848 1.877 0.969 0.053 0.698]\n",
      "[2.861 1.878 0.982 0.053 0.693]\n",
      "[2.86  1.868 0.991 0.053 0.69 ]\n",
      "[2.863 1.873 0.989 0.053 0.697]\n",
      "[2.852 1.864 0.987 0.053 0.694]\n",
      "[2.863 1.853 1.009 0.053 0.691]\n",
      "[2.861 1.853 1.007 0.053 0.693]\n",
      "[2.871 1.848 1.021 0.053 0.696]\n",
      "[2.87  1.84  1.028 0.053 0.695]\n",
      "[2.884 1.844 1.039 0.053 0.693]\n",
      "[2.883 1.842 1.04  0.053 0.697]\n",
      "Running sub 5, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.322 4.567 1.754 0.059 0.267]\n",
      "[7.889 6.481 1.407 0.058 0.429]\n",
      "[7.742 6.664 1.077 0.056 0.596]\n",
      "[8.257 7.322 0.933 0.054 0.646]\n",
      "[9.09  8.219 0.87  0.054 0.66 ]\n",
      "[9.124 8.342 0.781 0.055 0.697]\n",
      "[9.725 8.979 0.745 0.055 0.714]\n",
      "[10.359  9.631  0.726  0.055  0.728]\n",
      "[10.65   9.927  0.721  0.055  0.733]\n",
      "[11.015 10.29   0.724  0.056  0.73 ]\n",
      "[11.295 10.585  0.708  0.056  0.738]\n",
      "[11.82  11.118  0.701  0.056  0.741]\n",
      "[11.651 10.968  0.681  0.055  0.747]\n",
      "[12.301 11.584  0.715  0.055  0.742]\n",
      "[12.505 11.799  0.705  0.056  0.744]\n",
      "[5.239 4.517 0.721 0.056 0.734]\n",
      "[3.924 3.169 0.754 0.057 0.736]\n",
      "[3.437 2.669 0.767 0.056 0.73 ]\n",
      "[3.206 2.434 0.771 0.056 0.727]\n",
      "[3.074 2.313 0.759 0.057 0.732]\n",
      "[2.981 2.22  0.76  0.056 0.732]\n",
      "[2.965 2.194 0.769 0.057 0.723]\n",
      "[2.926 2.164 0.761 0.056 0.726]\n",
      "[2.902 2.134 0.768 0.055 0.72 ]\n",
      "[2.868 2.111 0.757 0.055 0.727]\n",
      "[2.832 2.04  0.791 0.056 0.71 ]\n",
      "[2.796 2.058 0.736 0.055 0.726]\n",
      "[2.764 2.038 0.725 0.055 0.73 ]\n",
      "[2.753 2.034 0.718 0.055 0.732]\n",
      "[2.724 2.018 0.705 0.055 0.736]\n",
      "[2.706 2.005 0.699 0.055 0.738]\n",
      "[2.714 2.007 0.706 0.056 0.736]\n",
      "[2.68  1.984 0.695 0.055 0.741]\n",
      "[2.654 1.965 0.688 0.055 0.744]\n",
      "[2.671 1.962 0.708 0.055 0.74 ]\n",
      "[2.664 1.96  0.703 0.055 0.741]\n",
      "[2.659 1.952 0.706 0.055 0.743]\n",
      "[2.661 1.945 0.715 0.055 0.739]\n",
      "[2.661 1.938 0.722 0.055 0.738]\n",
      "[2.668 1.931 0.736 0.055 0.733]\n",
      "[2.637 1.923 0.713 0.055 0.741]\n",
      "[2.663 1.923 0.739 0.056 0.732]\n",
      "[2.648 1.912 0.735 0.056 0.737]\n",
      "[2.643 1.9   0.742 0.056 0.734]\n",
      "[2.626 1.912 0.713 0.056 0.747]\n",
      "[2.631 1.897 0.733 0.056 0.74 ]\n",
      "[2.604 1.891 0.712 0.055 0.745]\n",
      "[2.617 1.894 0.722 0.056 0.746]\n",
      "[2.62  1.884 0.735 0.055 0.742]\n",
      "[2.629 1.888 0.74  0.055 0.742]\n",
      "Running sub 6, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[16.914 15.32   1.592  0.059  0.344]\n",
      "[55.385 54.368  1.015  0.058  0.626]\n",
      "[161.681 160.927   0.753   0.056   0.737]\n",
      "[73.044 72.389  0.653  0.056  0.769]\n",
      "[98.88  98.247  0.631  0.056  0.773]\n",
      "[17.86  17.278  0.581  0.056  0.796]\n",
      "[13.456 12.804  0.65   0.057  0.778]\n",
      "[13.185 12.496  0.687  0.057  0.78 ]\n",
      "[11.787 11.105  0.681  0.057  0.774]\n",
      "[12.468 11.733  0.733  0.057  0.769]\n",
      "[12.495 11.774  0.719  0.056  0.777]\n",
      "[13.397 12.651  0.744  0.056  0.775]\n",
      "[13.412 12.671  0.74   0.057  0.782]\n",
      "[13.828 13.124  0.702  0.056  0.788]\n",
      "[13.864 13.173  0.689  0.056  0.789]\n",
      "[6.035 5.404 0.629 0.057 0.79 ]\n",
      "[4.134 3.567 0.565 0.057 0.813]\n",
      "[3.55  2.895 0.653 0.056 0.815]\n",
      "[3.114 2.525 0.587 0.056 0.827]\n",
      "[2.94  2.352 0.586 0.056 0.822]\n",
      "[2.815 2.229 0.585 0.056 0.82 ]\n",
      "[2.737 2.163 0.572 0.056 0.816]\n",
      "[2.706 2.129 0.575 0.056 0.814]\n",
      "[2.678 2.105 0.572 0.055 0.813]\n",
      "[2.651 2.074 0.575 0.055 0.809]\n",
      "[2.626 2.057 0.568 0.056 0.813]\n",
      "[2.61  2.043 0.565 0.056 0.816]\n",
      "[2.585 2.023 0.56  0.056 0.819]\n",
      "[2.577 2.015 0.56  0.056 0.817]\n",
      "[2.537 1.983 0.552 0.056 0.821]\n",
      "[2.536 1.979 0.555 0.056 0.822]\n",
      "[2.519 1.965 0.553 0.056 0.822]\n",
      "[2.516 1.959 0.556 0.056 0.823]\n",
      "[2.502 1.952 0.548 0.056 0.824]\n",
      "[2.49  1.944 0.545 0.056 0.825]\n",
      "[2.469 1.93  0.537 0.056 0.828]\n",
      "[2.472 1.925 0.545 0.056 0.827]\n",
      "[2.47  1.928 0.541 0.056 0.825]\n",
      "[2.469 1.919 0.549 0.057 0.823]\n",
      "[2.465 1.913 0.551 0.056 0.826]\n",
      "[2.451 1.906 0.543 0.056 0.823]\n",
      "[2.443 1.895 0.547 0.056 0.824]\n",
      "[2.441 1.893 0.547 0.056 0.823]\n",
      "[2.455 1.89  0.564 0.056 0.821]\n",
      "[2.453 1.888 0.563 0.056 0.823]\n",
      "[2.462 1.894 0.566 0.056 0.822]\n",
      "[2.443 1.878 0.563 0.056 0.823]\n",
      "[2.456 1.875 0.58  0.056 0.819]\n",
      "[2.456 1.871 0.584 0.056 0.819]\n",
      "[2.479 1.873 0.605 0.057 0.817]\n",
      "Running sub 7, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.577 5.189 1.386 0.059 0.467]\n",
      "[9.666 8.877 0.787 0.056 0.736]\n",
      "[9.896 9.303 0.591 0.051 0.795]\n",
      "[10.788 10.269  0.517  0.05   0.822]\n",
      "[12.027 11.592  0.434  0.051  0.851]\n",
      "[11.781 11.37   0.408  0.052  0.853]\n",
      "[12.594 12.191  0.401  0.052  0.857]\n",
      "[12.792 12.411  0.38   0.051  0.863]\n",
      "[12.311 11.943  0.366  0.052  0.865]\n",
      "[13.318 12.955  0.361  0.053  0.865]\n",
      "[13.041 12.696  0.342  0.053  0.873]\n",
      "[13.775 13.422  0.351  0.054  0.871]\n",
      "[14.101 13.752  0.347  0.052  0.869]\n",
      "[14.332 13.99   0.34   0.052  0.873]\n",
      "[15.128 14.788  0.338  0.052  0.877]\n",
      "[5.037 4.562 0.473 0.053 0.856]\n",
      "[3.658 3.244 0.412 0.052 0.888]\n",
      "[3.044 2.624 0.419 0.053 0.887]\n",
      "[2.71  2.274 0.434 0.052 0.89 ]\n",
      "[2.544 2.104 0.438 0.052 0.89 ]\n",
      "[2.424 1.956 0.466 0.052 0.88 ]\n",
      "[2.357 1.935 0.42  0.052 0.89 ]\n",
      "[2.305 1.899 0.404 0.053 0.894]\n",
      "[2.264 1.871 0.392 0.053 0.896]\n",
      "[2.228 1.842 0.384 0.053 0.894]\n",
      "[2.208 1.835 0.372 0.053 0.896]\n",
      "[2.182 1.813 0.367 0.053 0.896]\n",
      "[2.153 1.797 0.355 0.053 0.898]\n",
      "[2.144 1.791 0.351 0.053 0.896]\n",
      "[2.127 1.78  0.346 0.053 0.897]\n",
      "[2.106 1.758 0.347 0.052 0.896]\n",
      "[2.095 1.759 0.334 0.054 0.897]\n",
      "[2.08  1.749 0.33  0.052 0.897]\n",
      "[2.065 1.736 0.328 0.052 0.895]\n",
      "[2.05  1.728 0.321 0.053 0.897]\n",
      "[2.04  1.72  0.319 0.053 0.897]\n",
      "[2.034 1.716 0.317 0.053 0.897]\n",
      "[2.029 1.706 0.321 0.053 0.896]\n",
      "[2.013 1.69  0.322 0.052 0.895]\n",
      "[2.016 1.695 0.321 0.053 0.895]\n",
      "[2.018 1.69  0.327 0.053 0.892]\n",
      "[2.02  1.691 0.328 0.054 0.891]\n",
      "[2.006 1.681 0.324 0.053 0.893]\n",
      "[2.005 1.689 0.315 0.054 0.896]\n",
      "[2.002 1.678 0.323 0.054 0.893]\n",
      "[1.987 1.676 0.31  0.054 0.898]\n",
      "[1.984 1.674 0.309 0.053 0.897]\n",
      "[1.976 1.676 0.299 0.055 0.902]\n",
      "[1.973 1.679 0.293 0.054 0.904]\n",
      "[1.968 1.667 0.3   0.054 0.9  ]\n",
      "Running sub 8, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[8.513 6.696 1.816 0.059 0.238]\n",
      "[10.389  9.247  1.14   0.058  0.609]\n",
      "[9.839 8.771 1.066 0.057 0.61 ]\n",
      "[10.391  9.403  0.986  0.057  0.646]\n",
      "[10.941  9.973  0.966  0.056  0.657]\n",
      "[10.773  9.845  0.925  0.056  0.673]\n",
      "[10.418  9.538  0.877  0.057  0.692]\n",
      "[10.814  9.921  0.892  0.057  0.683]\n",
      "[10.857  9.963  0.892  0.058  0.688]\n",
      "[11.401 10.524  0.875  0.057  0.707]\n",
      "[11.553 10.672  0.878  0.057  0.712]\n",
      "[12.07  11.193  0.875  0.057  0.712]\n",
      "[12.325 11.451  0.872  0.058  0.713]\n",
      "[12.846 11.959  0.886  0.058  0.713]\n",
      "[13.033 12.163  0.868  0.058  0.72 ]\n",
      "[5.383 4.52  0.862 0.058 0.698]\n",
      "[4.252 3.395 0.856 0.058 0.709]\n",
      "[3.665 2.857 0.807 0.057 0.73 ]\n",
      "[3.425 2.605 0.818 0.058 0.72 ]\n",
      "[3.263 2.442 0.82  0.057 0.714]\n",
      "[3.196 2.376 0.818 0.057 0.712]\n",
      "[3.115 2.311 0.803 0.057 0.722]\n",
      "[3.079 2.27  0.808 0.057 0.719]\n",
      "[3.052 2.245 0.806 0.057 0.719]\n",
      "[3.028 2.223 0.804 0.057 0.719]\n",
      "[3.007 2.202 0.803 0.057 0.72 ]\n",
      "[2.997 2.193 0.803 0.058 0.717]\n",
      "[2.968 2.162 0.804 0.057 0.715]\n",
      "[2.964 2.149 0.814 0.057 0.712]\n",
      "[2.923 2.125 0.797 0.057 0.717]\n",
      "[2.922 2.117 0.803 0.057 0.715]\n",
      "[2.884 2.1   0.783 0.057 0.721]\n",
      "[2.873 2.093 0.779 0.057 0.722]\n",
      "[2.897 2.096 0.8   0.057 0.713]\n",
      "[2.885 2.085 0.799 0.057 0.718]\n",
      "[2.884 2.077 0.806 0.057 0.715]\n",
      "[2.9   2.079 0.819 0.057 0.713]\n",
      "[2.87  2.055 0.813 0.057 0.713]\n",
      "[2.851 2.057 0.792 0.057 0.721]\n",
      "[2.862 2.055 0.806 0.057 0.716]\n",
      "[2.849 2.043 0.805 0.057 0.716]\n",
      "[2.88  2.035 0.844 0.057 0.704]\n",
      "[2.854 2.031 0.822 0.057 0.709]\n",
      "[2.866 2.025 0.839 0.057 0.697]\n",
      "[2.866 2.022 0.843 0.057 0.699]\n",
      "[2.835 2.    0.834 0.057 0.705]\n",
      "[2.821 1.999 0.822 0.057 0.71 ]\n",
      "[2.852 2.002 0.848 0.057 0.705]\n",
      "[2.847 1.994 0.852 0.056 0.707]\n",
      "[2.818 1.991 0.827 0.057 0.708]\n",
      "Running sub 9, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.507 5.025 1.482 0.059 0.425]\n",
      "[8.569 7.66  0.907 0.051 0.665]\n",
      "[8.007 7.279 0.727 0.051 0.733]\n",
      "[8.559 7.938 0.62  0.048 0.777]\n",
      "[9.098 8.565 0.532 0.052 0.801]\n",
      "[9.553 9.065 0.487 0.051 0.818]\n",
      "[10.38   9.884  0.494  0.053  0.812]\n",
      "[10.381  9.904  0.476  0.053  0.817]\n",
      "[10.946 10.446  0.498  0.052  0.81 ]\n",
      "[11.283 10.771  0.511  0.054  0.804]\n",
      "[11.51  11.053  0.456  0.054  0.824]\n",
      "[12.094 11.604  0.489  0.053  0.815]\n",
      "[12.305 11.802  0.502  0.053  0.809]\n",
      "[12.065 11.553  0.51   0.055  0.809]\n",
      "[12.745 12.28   0.464  0.053  0.818]\n",
      "[4.576 4.015 0.559 0.054 0.818]\n",
      "[4.722 3.718 1.002 0.053 0.702]\n",
      "[3.83  3.188 0.641 0.053 0.783]\n",
      "[2.79  2.289 0.499 0.053 0.85 ]\n",
      "[2.632 2.123 0.507 0.053 0.84 ]\n",
      "[2.501 2.021 0.479 0.053 0.853]\n",
      "[2.414 1.956 0.457 0.053 0.861]\n",
      "[2.37  1.924 0.445 0.053 0.862]\n",
      "[2.34  1.894 0.445 0.054 0.858]\n",
      "[2.305 1.874 0.43  0.053 0.867]\n",
      "[2.276 1.85  0.425 0.053 0.867]\n",
      "[2.253 1.827 0.424 0.053 0.865]\n",
      "[2.237 1.814 0.421 0.053 0.863]\n",
      "[2.218 1.81  0.407 0.053 0.869]\n",
      "[2.203 1.795 0.408 0.053 0.869]\n",
      "[2.184 1.783 0.401 0.053 0.872]\n",
      "[2.169 1.768 0.399 0.053 0.872]\n",
      "[2.158 1.759 0.399 0.053 0.871]\n",
      "[2.162 1.746 0.414 0.054 0.866]\n",
      "[2.143 1.742 0.4   0.054 0.871]\n",
      "[2.132 1.729 0.402 0.053 0.872]\n",
      "[2.132 1.728 0.403 0.053 0.871]\n",
      "[2.131 1.715 0.415 0.053 0.86 ]\n",
      "[2.126 1.715 0.411 0.053 0.861]\n",
      "[2.114 1.717 0.395 0.054 0.869]\n",
      "[2.113 1.707 0.405 0.054 0.863]\n",
      "[2.105 1.703 0.4   0.053 0.866]\n",
      "[2.109 1.697 0.411 0.053 0.861]\n",
      "[2.109 1.702 0.406 0.054 0.857]\n",
      "[2.107 1.697 0.409 0.053 0.86 ]\n",
      "[2.099 1.69  0.408 0.053 0.861]\n",
      "[2.09  1.686 0.403 0.054 0.861]\n",
      "[2.109 1.691 0.417 0.053 0.855]\n",
      "[2.112 1.686 0.426 0.053 0.851]\n",
      "[2.099 1.688 0.411 0.054 0.857]\n",
      "Running sub 10, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[7.024 5.488 1.535 0.059 0.414]\n",
      "[10.411  9.565  0.844  0.057  0.693]\n",
      "[10.572  9.671  0.9    0.057  0.657]\n",
      "[10.768 10.033  0.733  0.057  0.73 ]\n",
      "[9.841 9.255 0.585 0.057 0.778]\n",
      "[10.679 10.154  0.524  0.058  0.8  ]\n",
      "[10.616 10.165  0.45   0.058  0.829]\n",
      "[11.185 10.745  0.438  0.058  0.83 ]\n",
      "[11.344 10.933  0.409  0.058  0.835]\n",
      "[11.68  11.308  0.371  0.058  0.856]\n",
      "[12.561 12.217  0.342  0.059  0.866]\n",
      "[12.943 12.604  0.337  0.059  0.872]\n",
      "[12.989 12.664  0.323  0.058  0.879]\n",
      "[14.398 14.071  0.325  0.058  0.882]\n",
      "[14.338 14.02   0.317  0.059  0.887]\n",
      "[5.129 4.657 0.47  0.059 0.848]\n",
      "[3.892 3.433 0.458 0.059 0.872]\n",
      "[3.365 2.926 0.437 0.059 0.879]\n",
      "[3.066 2.638 0.427 0.059 0.888]\n",
      "[2.913 2.487 0.424 0.058 0.884]\n",
      "[2.833 2.407 0.424 0.059 0.878]\n",
      "[2.758 2.334 0.423 0.059 0.879]\n",
      "[2.725 2.315 0.408 0.059 0.879]\n",
      "[2.69  2.29  0.398 0.059 0.88 ]\n",
      "[2.642 2.257 0.384 0.059 0.883]\n",
      "[2.595 2.212 0.381 0.059 0.888]\n",
      "[2.609 2.234 0.373 0.059 0.879]\n",
      "[2.567 2.204 0.362 0.059 0.88 ]\n",
      "[2.546 2.188 0.357 0.058 0.881]\n",
      "[2.519 2.166 0.352 0.059 0.886]\n",
      "[2.509 2.154 0.354 0.059 0.884]\n",
      "[2.482 2.139 0.342 0.059 0.886]\n",
      "[2.469 2.12  0.348 0.059 0.881]\n",
      "[2.443 2.105 0.337 0.059 0.888]\n",
      "[2.43  2.086 0.343 0.059 0.884]\n",
      "[2.443 2.091 0.351 0.059 0.883]\n",
      "[2.426 2.075 0.35  0.058 0.883]\n",
      "[2.405 2.069 0.335 0.059 0.887]\n",
      "[2.384 2.061 0.322 0.059 0.89 ]\n",
      "[2.374 2.052 0.321 0.059 0.89 ]\n",
      "[2.375 2.047 0.327 0.059 0.89 ]\n",
      "[2.351 2.039 0.312 0.059 0.897]\n",
      "[2.356 2.035 0.32  0.059 0.89 ]\n",
      "[2.367 2.038 0.328 0.059 0.888]\n",
      "[2.335 2.023 0.312 0.059 0.895]\n",
      "[2.339 2.031 0.307 0.059 0.892]\n",
      "[2.326 2.016 0.309 0.059 0.894]\n",
      "[2.323 2.013 0.31  0.059 0.892]\n",
      "[2.314 2.008 0.305 0.059 0.895]\n",
      "[2.326 2.016 0.308 0.059 0.894]\n",
      "Running sub 11, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.771 5.285 1.485 0.059 0.447]\n",
      "[9.408 8.506 0.9   0.056 0.666]\n",
      "[9.873 9.149 0.722 0.053 0.741]\n",
      "[10.327  9.656  0.669  0.054  0.762]\n",
      "[11.168 10.566  0.601  0.054  0.783]\n",
      "[12.464 11.902  0.561  0.055  0.795]\n",
      "[12.927 12.389  0.536  0.054  0.807]\n",
      "[13.39  12.884  0.504  0.055  0.813]\n",
      "[14.079 13.573  0.504  0.055  0.819]\n",
      "[14.238 13.765  0.471  0.054  0.829]\n",
      "[14.197 13.723  0.472  0.054  0.832]\n",
      "[14.429 13.962  0.465  0.054  0.835]\n",
      "[14.491 14.014  0.475  0.054  0.83 ]\n",
      "[14.581 14.092  0.488  0.054  0.826]\n",
      "[14.785 14.305  0.478  0.054  0.829]\n",
      "[5.446 4.839 0.605 0.055 0.796]\n",
      "[3.888 3.291 0.595 0.055 0.825]\n",
      "[3.306 2.732 0.572 0.053 0.841]\n",
      "[3.048 2.49  0.557 0.054 0.847]\n",
      "[2.889 2.32  0.567 0.054 0.845]\n",
      "[2.824 2.247 0.576 0.054 0.837]\n",
      "[2.754 2.18  0.572 0.054 0.836]\n",
      "[2.712 2.151 0.56  0.055 0.836]\n",
      "[2.669 2.114 0.553 0.054 0.835]\n",
      "[2.626 2.091 0.533 0.054 0.841]\n",
      "[2.605 2.078 0.526 0.054 0.841]\n",
      "[2.633 2.061 0.571 0.054 0.821]\n",
      "[2.548 2.031 0.515 0.054 0.845]\n",
      "[2.527 2.025 0.501 0.054 0.848]\n",
      "[2.516 2.012 0.503 0.054 0.845]\n",
      "[2.506 2.007 0.498 0.054 0.846]\n",
      "[2.482 1.993 0.489 0.054 0.85 ]\n",
      "[2.461 1.977 0.483 0.054 0.851]\n",
      "[2.455 1.975 0.479 0.054 0.847]\n",
      "[2.462 1.976 0.485 0.054 0.843]\n",
      "[2.451 1.964 0.486 0.054 0.842]\n",
      "[2.448 1.961 0.486 0.054 0.839]\n",
      "[2.433 1.943 0.489 0.054 0.84 ]\n",
      "[2.433 1.948 0.484 0.055 0.837]\n",
      "[2.431 1.937 0.494 0.054 0.833]\n",
      "[2.423 1.928 0.494 0.055 0.833]\n",
      "[2.414 1.924 0.489 0.055 0.835]\n",
      "[2.396 1.918 0.477 0.054 0.838]\n",
      "[2.389 1.907 0.481 0.055 0.838]\n",
      "[2.379 1.904 0.475 0.055 0.84 ]\n",
      "[2.374 1.9   0.473 0.054 0.841]\n",
      "[2.371 1.895 0.475 0.055 0.837]\n",
      "[2.354 1.887 0.467 0.054 0.84 ]\n",
      "[2.359 1.881 0.477 0.054 0.835]\n",
      "[2.364 1.879 0.484 0.055 0.832]\n",
      "Running sub 12, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[6.581 4.807 1.773 0.059 0.321]\n",
      "[8.055 6.686 1.368 0.058 0.475]\n",
      "[8.609 7.372 1.236 0.058 0.523]\n",
      "[9.656 8.502 1.153 0.058 0.531]\n",
      "[9.822 8.768 1.052 0.058 0.574]\n",
      "[10.488  9.469  1.017  0.058  0.6  ]\n",
      "[10.467  9.488  0.978  0.058  0.622]\n",
      "[10.674  9.69   0.982  0.058  0.632]\n",
      "[10.624  9.645  0.977  0.058  0.639]\n",
      "[11.037 10.058  0.977  0.058  0.644]\n",
      "[11.131 10.147  0.983  0.058  0.645]\n",
      "[11.559 10.574  0.983  0.059  0.644]\n",
      "[12.081 11.108  0.971  0.059  0.644]\n",
      "[12.233 11.271  0.96   0.058  0.653]\n",
      "[12.454 11.48   0.973  0.058  0.656]\n",
      "[4.841 3.951 0.888 0.059 0.65 ]\n",
      "[3.857 2.984 0.871 0.058 0.664]\n",
      "[3.382 2.524 0.856 0.059 0.661]\n",
      "[3.148 2.319 0.827 0.058 0.674]\n",
      "[3.025 2.192 0.831 0.058 0.677]\n",
      "[2.947 2.116 0.829 0.058 0.677]\n",
      "[2.901 2.07  0.83  0.058 0.677]\n",
      "[2.89  2.048 0.84  0.058 0.675]\n",
      "[2.85  2.017 0.832 0.058 0.672]\n",
      "[2.846 2.008 0.836 0.058 0.67 ]\n",
      "[2.817 1.985 0.83  0.059 0.668]\n",
      "[2.794 1.964 0.829 0.058 0.667]\n",
      "[2.779 1.955 0.822 0.058 0.67 ]\n",
      "[2.8   1.963 0.835 0.059 0.663]\n",
      "[2.766 1.931 0.834 0.058 0.669]\n",
      "[2.755 1.92  0.834 0.059 0.668]\n",
      "[2.741 1.907 0.833 0.058 0.67 ]\n",
      "[2.739 1.904 0.834 0.059 0.673]\n",
      "[2.73  1.894 0.835 0.058 0.671]\n",
      "[2.72  1.891 0.828 0.059 0.678]\n",
      "[2.705 1.889 0.815 0.058 0.681]\n",
      "[2.736 1.89  0.844 0.058 0.67 ]\n",
      "[2.733 1.885 0.846 0.059 0.668]\n",
      "[2.732 1.878 0.853 0.059 0.67 ]\n",
      "[2.721 1.868 0.851 0.059 0.671]\n",
      "[2.727 1.861 0.865 0.059 0.671]\n",
      "[2.721 1.856 0.864 0.059 0.673]\n",
      "[2.731 1.862 0.868 0.058 0.67 ]\n",
      "[2.716 1.862 0.853 0.059 0.679]\n",
      "[2.723 1.854 0.868 0.058 0.671]\n",
      "[2.728 1.862 0.864 0.058 0.672]\n",
      "[2.717 1.856 0.86  0.059 0.673]\n",
      "[2.693 1.848 0.844 0.058 0.677]\n",
      "[2.71  1.846 0.863 0.059 0.675]\n",
      "[2.713 1.838 0.874 0.059 0.669]\n",
      "Running sub 13, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[7.689 5.96  1.727 0.059 0.349]\n",
      "[13.22  11.968  1.25   0.057  0.538]\n",
      "[9.728 8.742 0.985 0.055 0.636]\n",
      "[10.617  9.733  0.882  0.054  0.667]\n",
      "[11.403 10.581  0.82   0.054  0.688]\n",
      "[12.977 12.195  0.78   0.054  0.707]\n",
      "[13.163 12.422  0.739  0.055  0.72 ]\n",
      "[14.005 13.281  0.722  0.054  0.733]\n",
      "[15.629 14.9    0.726  0.054  0.738]\n",
      "[15.041 14.35   0.689  0.055  0.763]\n",
      "[15.394 14.685  0.707  0.054  0.741]\n",
      "[16.179 15.479  0.697  0.053  0.749]\n",
      "[15.506 14.815  0.688  0.054  0.755]\n",
      "[15.819 15.143  0.675  0.053  0.758]\n",
      "[15.82  15.15   0.668  0.054  0.76 ]\n",
      "[4.971 4.165 0.804 0.054 0.72 ]\n",
      "[4.089 3.303 0.785 0.054 0.731]\n",
      "[3.503 2.724 0.778 0.054 0.737]\n",
      "[3.243 2.483 0.758 0.054 0.754]\n",
      "[3.148 2.278 0.869 0.054 0.7  ]\n",
      "[3.009 2.226 0.781 0.053 0.739]\n",
      "[2.952 2.192 0.758 0.054 0.749]\n",
      "[2.886 2.156 0.729 0.054 0.753]\n",
      "[2.813 2.122 0.69  0.052 0.77 ]\n",
      "[2.781 2.088 0.691 0.053 0.77 ]\n",
      "[2.766 2.081 0.684 0.054 0.771]\n",
      "[2.741 2.052 0.687 0.053 0.764]\n",
      "[2.725 2.042 0.681 0.054 0.768]\n",
      "[2.709 2.031 0.677 0.053 0.769]\n",
      "[2.673 2.001 0.671 0.054 0.771]\n",
      "[2.67  1.996 0.672 0.053 0.768]\n",
      "[2.652 1.984 0.667 0.053 0.766]\n",
      "[2.637 1.972 0.664 0.053 0.769]\n",
      "[2.617 1.962 0.653 0.054 0.77 ]\n",
      "[2.609 1.959 0.65  0.054 0.771]\n",
      "[2.601 1.939 0.66  0.054 0.77 ]\n",
      "[2.586 1.926 0.659 0.053 0.769]\n",
      "[2.583 1.924 0.658 0.053 0.769]\n",
      "[2.557 1.915 0.641 0.053 0.776]\n",
      "[2.566 1.906 0.659 0.053 0.766]\n",
      "[2.547 1.891 0.655 0.053 0.768]\n",
      "[2.558 1.885 0.672 0.053 0.765]\n",
      "[2.567 1.891 0.675 0.053 0.762]\n",
      "[2.557 1.875 0.681 0.053 0.759]\n",
      "[2.552 1.871 0.679 0.054 0.76 ]\n",
      "[2.562 1.874 0.687 0.053 0.756]\n",
      "[2.538 1.863 0.674 0.053 0.762]\n",
      "[2.554 1.859 0.694 0.054 0.754]\n",
      "[2.533 1.853 0.679 0.054 0.763]\n",
      "[2.535 1.852 0.682 0.054 0.76 ]\n",
      "Running sub 14, model 2, latent dim 5, cv 1\n",
      "loading data\n",
      "['loss', 'decoder_loss', 'clf_loss', 'decoder_accuracy', 'clf_accuracy']\n",
      "[7.65  6.262 1.386 0.059 0.515]\n",
      "[10.902 10.09   0.811  0.054  0.7  ]\n",
      "[9.522 8.983 0.537 0.051 0.807]\n",
      "[9.376 8.874 0.5   0.05  0.812]\n",
      "[10.835 10.344  0.489  0.051  0.821]\n",
      "[11.651 11.19   0.46   0.051  0.832]\n",
      "[12.14  11.675  0.462  0.051  0.832]\n",
      "[11.296 10.819  0.474  0.052  0.824]\n",
      "[12.008 11.539  0.468  0.052  0.828]\n",
      "[12.628 12.175  0.451  0.052  0.84 ]\n",
      "[12.385 11.974  0.409  0.052  0.852]\n",
      "[12.144 11.782  0.361  0.053  0.868]\n",
      "[12.675 12.306  0.366  0.052  0.866]\n",
      "[13.117 12.742  0.373  0.052  0.867]\n",
      "[12.857 12.481  0.374  0.051  0.866]\n",
      "[4.995 4.586 0.407 0.052 0.882]\n",
      "[3.889 3.432 0.456 0.053 0.866]\n",
      "[3.277 2.835 0.44  0.052 0.868]\n",
      "[2.93  2.495 0.433 0.052 0.873]\n",
      "[2.733 2.256 0.476 0.052 0.861]\n",
      "[2.612 2.144 0.466 0.052 0.861]\n",
      "[2.533 2.064 0.467 0.052 0.861]\n",
      "[2.457 2.005 0.45  0.052 0.864]\n",
      "[2.387 1.957 0.429 0.052 0.87 ]\n",
      "[2.35  1.923 0.426 0.052 0.868]\n",
      "[2.312 1.879 0.432 0.052 0.868]\n",
      "[2.281 1.861 0.419 0.053 0.87 ]\n",
      "[2.275 1.838 0.436 0.053 0.861]\n",
      "[2.23  1.82  0.409 0.052 0.87 ]\n",
      "[2.213 1.811 0.4   0.052 0.876]\n",
      "[2.188 1.785 0.402 0.053 0.873]\n",
      "[2.155 1.785 0.369 0.053 0.884]\n",
      "[2.116 1.747 0.368 0.053 0.883]\n",
      "[2.12  1.755 0.364 0.052 0.886]\n",
      "[2.109 1.738 0.37  0.053 0.884]\n",
      "[2.088 1.727 0.36  0.053 0.886]\n",
      "[2.063 1.719 0.343 0.053 0.89 ]\n",
      "[2.056 1.725 0.329 0.053 0.895]\n",
      "[2.052 1.712 0.339 0.053 0.889]\n",
      "[2.049 1.708 0.34  0.053 0.891]\n",
      "[2.051 1.694 0.356 0.053 0.887]\n",
      "[2.065 1.69  0.373 0.052 0.881]\n",
      "[2.075 1.692 0.382 0.052 0.872]\n",
      "[2.047 1.681 0.365 0.052 0.875]\n",
      "[2.049 1.68  0.368 0.052 0.876]\n",
      "[2.051 1.664 0.386 0.053 0.868]\n",
      "[2.048 1.671 0.376 0.052 0.872]\n",
      "[2.054 1.67  0.383 0.052 0.869]\n",
      "[2.058 1.66  0.398 0.052 0.867]\n",
      "[2.029 1.651 0.378 0.052 0.87 ]\n"
     ]
    }
   ],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)\n",
    "\n",
    "# Loop through training\n",
    "train_dict = {'sub_type':sub_type,'n_train':'fullgaussflat4', 'load':True, 'train_scale':5, 'epochs': 30, 'batch_size' : 128, 'sparsity':True,'dt':'manual','feat_type':'feat','noise':True, 'latent_dim':4,'mod':['lda'],'gens':50, 'mod_dt':'1012','train_grp':2}\n",
    "train_sess = session.Session(**train_dict)\n",
    "\n",
    "# loop through subjects\n",
    "for sub_i in range(1,np.max(params[:,0])+1):\n",
    "    for lat in range(5,6):#1,11):\n",
    "        train_sess.latent_dim = lat\n",
    "        train_out = train_sess.loop_cv(raw,params,sub=sub_i,mod=['ext_c','aligned'])\n",
    "        # for key,val in train_out.items():\n",
    "        #     exec(key + '=val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc, all_val = plot_utils.plot_latent_dim(params,train_sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/TR2_traindata_2.p\n",
      "loading data\n",
      "Loading training data: traindata_manual/TR2_traindata_2.p\n",
      "loading data\n",
      "Loading training data: traindata_manual/TR2_traindata_2.p\n",
      "loading data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-60cf68e8e04b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtest_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'part'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mntype\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtest_scale\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mred_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_latent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_scale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mred_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'=val'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\yteh\\Documents\\work\\git\\projects\\latent_rep\\python\\session.py\u001b[0m in \u001b[0;36mreduce_latent\u001b[1;34m(self, raw, params, sub, cv, test_scale)\u001b[0m\n\u001b[0;32m    984\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_svae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvae_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_vae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[0mx_test_sae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msae_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_dlsae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m             \u001b[0mx_test_cnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_vae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_vcnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvcnn_enc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test_vae\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1013\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m   1014\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[0;32m   1586\u001b[0m     \"\"\"\n\u001b[0;32m   1587\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1588\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1589\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1590\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3888\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   3889\u001b[0m     variant_tensor = gen_dataset_ops.map_dataset(\n\u001b[0;32m   3890\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m           optional_features=optional_features)\n\u001b[0;32m    233\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted_for_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mpermutation\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    310\u001b[0m       \u001b[1;31m# than reusing the same range Tensor. (presumably because of buffer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m       \u001b[1;31m# forwarding.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m       \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"batch\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_shuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mrange\u001b[1;34m(start, limit, delta, dtype, name)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"start\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1430\u001b[0m     \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"limit\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1431\u001b[1;33m     \u001b[0mdelta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"delta\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[1;31m# infer dtype if not explicitly provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 258\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    300\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[0;32m    301\u001b[0m              \"dtype\": dtype_value},\n\u001b[1;32m--> 302\u001b[1;33m       name=name).outputs[0]\n\u001b[0m\u001b[0;32m    303\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1594\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1595\u001b[0m   op_desc = c_api.TF_NewOperation(graph._c_graph, compat.as_str(node_def.op),\n\u001b[1;32m-> 1596\u001b[1;33m                                   compat.as_str(node_def.name))\n\u001b[0m\u001b[0;32m   1597\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# reduce dimensions of inputs\n",
    "sub = 2\n",
    "test_dict = {'sub_type':sub_type,'dt':'manual', 'mod_dt':'1012','sparsity':True, 'load':True, 'batch_size':128, 'latent_dim':5, 'epochs':30,'train_scale':5, 'n_train':'fullgaussflat4', 'n_test':'partgauss4','feat_type':'feat', 'noise':True,'train_grp':2}\n",
    "test_sess = session.Session(**test_dict)\n",
    "ntype = 'gauss'\n",
    "addon = True\n",
    "\n",
    "if not addon:\n",
    "    x_clean_lda = np.array([]).reshape(0,6)\n",
    "    x_clean_noise = np.array([]).reshape(0,6)\n",
    "    x_clean_sae = np.array([]).reshape(0,5)\n",
    "    x_clean_cnn = np.array([]).reshape(0,5)\n",
    "    x_clean_vcnn = np.array([]).reshape(0,5)\n",
    "    y_clean = np.array([]).reshape(0,1)\n",
    "\n",
    "    x_noisy_lda = np.array([]).reshape(0,6)\n",
    "    x_noisy_noise = np.array([]).reshape(0,6)\n",
    "    x_noisy_sae = np.array([]).reshape(0,5)\n",
    "    x_noisy_cnn = np.array([]).reshape(0,5)\n",
    "    x_noisy_vcnn = np.array([]).reshape(0,5)\n",
    "    y_noisy = np.array([]).reshape(0,1)\n",
    "\n",
    "if ntype == 'flat':\n",
    "    test_max = 2\n",
    "else:\n",
    "    test_max = 6\n",
    "\n",
    "for i in range(1,5):\n",
    "    test_sess.n_test = 'part' + ntype + str(i)\n",
    "    for test_scale in range(1,test_max):\n",
    "        red_out = test_sess.reduce_latent(raw, params, sub, cv=1,test_scale=1)\n",
    "        for key,val in red_out.items():\n",
    "            exec(key + '=val')\n",
    "        x_clean_lda = np.vstack([x_test_lda_red[:clean_size,:], x_clean_lda])\n",
    "        x_clean_noise = np.vstack([x_test_noise_red[:clean_size,:], x_clean_noise])\n",
    "        x_clean_sae = np.vstack([x_test_sae_red[:clean_size,:], x_clean_sae])\n",
    "        x_clean_cnn = np.vstack([x_test_cnn_red[:clean_size,:], x_clean_cnn])\n",
    "        x_clean_vcnn = np.vstack([x_test_vcnn_red[:clean_size,:], x_clean_vcnn])\n",
    "        y_clean = np.vstack([y_test[:clean_size,:], y_clean])\n",
    "\n",
    "        x_noisy_lda = np.vstack([x_test_lda_red[clean_size:,:], x_noisy_lda])\n",
    "        x_noisy_noise = np.vstack([x_test_noise_red[clean_size:,:], x_noisy_noise])\n",
    "        x_noisy_sae = np.vstack([x_test_sae_red[clean_size:,:], x_noisy_sae])\n",
    "        x_noisy_cnn = np.vstack([x_test_cnn_red[clean_size:,:], x_noisy_cnn])\n",
    "        x_noisy_vcnn = np.vstack([x_test_vcnn_red[clean_size:,:], x_noisy_vcnn])\n",
    "        y_noisy = np.vstack([y_test[clean_size:,:], y_noisy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reduced dimensions\n",
    "plot_utils.plot_latent_rep(x_clean_lda, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_noise, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_sae, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_cnn, y_clean)\n",
    "plot_utils.plot_latent_rep(x_clean_vcnn, y_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_latent_rep(x_noisy_lda, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_noise, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_sae, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_cnn, y_noisy)\n",
    "plot_utils.plot_latent_rep(x_noisy_vcnn, y_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all_lda = np.vstack([x_clean_lda,x_noisy_lda])\n",
    "x_all_noise = np.vstack([x_clean_noise,x_noisy_noise])\n",
    "x_all_sae = np.vstack([x_clean_sae,x_noisy_sae])\n",
    "x_all_cnn = np.vstack([x_clean_cnn,x_noisy_cnn])\n",
    "x_all_vcnn = np.vstack([x_clean_vcnn,x_noisy_vcnn])\n",
    "y_all = np.vstack([y_clean, y_noisy])\n",
    "plot_utils.plot_latent_rep(x_all_lda, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_noise, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_sae, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_cnn, y_all)\n",
    "plot_utils.plot_latent_rep(x_all_vcnn, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models_2_manual_1012/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR2_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR3_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR4_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR5_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR6_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR2_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR3_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR4_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR5_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR6_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR2_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR3_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR4_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR5_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n",
      "models_2_manual_1012/TR6_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_sparse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:47: RuntimeWarning: Mean of empty slice\n",
      "C:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\ipykernel_launcher.py:48: RuntimeWarning: Mean of empty slice\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy as cp\n",
    "test_dict = {'sub_type':sub_type,'dt':'manual', 'mod_dt':'1012','sparsity':True, 'load':True, 'batch_size':128, 'latent_dim':5, 'epochs':30,'train_scale':5, 'n_train':'fullgaussflat4', 'n_test':'partgauss4','feat_type':'feat', 'noise':True,'train_grp':2}\n",
    "test_sess = session.Session(**test_dict)\n",
    "\n",
    "load = False\n",
    "ntype = 'posgauss1'\n",
    "\n",
    "if ntype[:3] == 'pos':\n",
    "    i_start = 3\n",
    "    i_end = 6\n",
    "    acc_all = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "    acc_clean = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "    acc_noise = np.full([np.max(params[:,0]), 3, 4, 15],np.nan)\n",
    "elif ntype == 'flat':\n",
    "    i_start = 1\n",
    "    i_end = 5\n",
    "    acc_all = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "    acc_clean = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "    acc_noise = np.full([np.max(params[:,0]), 4, 1, 15],np.nan)\n",
    "else:\n",
    "    i_start = 1\n",
    "    i_end = 5\n",
    "    acc_all = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "    acc_clean = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "    acc_noise = np.full([np.max(params[:,0]), 4, 5, 15],np.nan)\n",
    "\n",
    "for i in range(i_start,i_end):\n",
    "    test_sess.n_test = 'part' + ntype + str(i)\n",
    "\n",
    "    if load:\n",
    "        for sub in range(1,np.max(params[:,0])):\n",
    "            foldername = test_sess.create_foldername()\n",
    "            filename = test_sess.create_filename(foldername,0,sub)#results=True)\n",
    "            print(filename)\n",
    "            if os.path.isfile(filename + '_' + test_sess.n_test + '_cvresults.p'):\n",
    "                with open(filename + '_' + test_sess.n_test + '_cvresults.p', 'rb') as f:\n",
    "                    temp_all, temp_noise, temp_clean = pickle.load(f)\n",
    "                acc_all[sub-1,i-i_start,:,:], acc_clean[sub-1,i-i_start,:,:], acc_noise[sub-1,i-i_start,:,:] = np.squeeze(temp_all),np.squeeze(temp_clean),np.squeeze(temp_noise)\n",
    "    else:\n",
    "        test_out = test_sess.loop_test(raw, params)\n",
    "        for key,val in test_out.items():\n",
    "            exec(key + '=val')\n",
    "\n",
    "ave_pos_noise= np.nanmean(acc_noise,axis=0)\n",
    "ave_pos_clean = np.nanmean(acc_clean,axis=0)\n",
    "ave_noise= np.nanmean(acc_noise,axis=2)\n",
    "ave_clean = np.nanmean(acc_clean,axis=2)\n",
    "ave_gauss_noise = np.nanmean(ave_noise,axis=0)\n",
    "ave_gauss_clean = np.nanmean(ave_clean,axis=0)\n",
    "\n",
    "if ntype == 'gauss':\n",
    "    ave_gauss= cp.deepcopy(ave_noise)\n",
    "elif ntype == '60hz':\n",
    "    ave_60hz= cp.deepcopy(ave_noise)\n",
    "else:\n",
    "    ave_flat= cp.deepcopy(ave_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_utils.plot_summary(ave_clean,ave_gauss,ave_60hz,ave_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. # noisy electrodes\n",
    "fig,ax = plt.subplots(1,3)\n",
    "c = ['k','r','m']\n",
    "c_i = 0\n",
    "for i in range(1,5):\n",
    "    ax[0].plot(100*ave_gauss_noise[:,i],'-o')\n",
    "for i in range(6,10):    \n",
    "    ax[1].plot(100*ave_gauss_noise[:,i],'-o')\n",
    "for i in [10,11,14]:\n",
    "    ax[2].plot(100*ave_gauss_noise[:,i],'-o',color=c[c_i])\n",
    "    c_i+=1    \n",
    "\n",
    "ax[0].set_ylabel('Accuracy (%)')\n",
    "fig.text(0.5, 0, 'Number of Noisy Electrodes', ha='center')\n",
    "ax[0].legend(['sae','cnn','vcnn','ecnn'])\n",
    "ax[1].legend(['sae-lda','cnn-lda','vcnn-lda','ecnn-lda'])\n",
    "ax[2].legend(['LDA','LDA-corrupt','LDA-ch'])\n",
    "ax[0].set_title('NN')\n",
    "ax[1].set_title('Aligned')\n",
    "ax[2].set_title('LDA')\n",
    "ax[1].set_yticks([])\n",
    "ax[2].set_yticks([])\n",
    "for i in range(0,3):\n",
    "    ax[i].set_ylim(0,100)\n",
    "    ax[i].set_xticks(range(0,4))\n",
    "    ax[i].set_xticklabels(['1','2','3','4'])\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Plot accuracy vs. # noisy electrodes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "x = np.arange(8)  # the label locations\n",
    "arr = [5,2,3,9,7,8,10,11]\n",
    "c = ['tab:blue','tab:orange','tab:green','tab:blue','tab:orange','tab:green','k','r']\n",
    "clean_all = ave_gauss_clean[0,arr]\n",
    "ax.bar(x,clean_all*100,color=c)\n",
    "ax.set_xticks(range(8))\n",
    "ax.set_xticklabels(['SAE','CNN','VCNN','SAE-LDA','CNN-LDA','VCNN-LDA','LDA','LDA-corrupt'])\n",
    "ax.set_ylabel('Accuracy (%)')\n",
    "ax.set_ylim([0,100])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. # noisy electrodes\n",
    "fig,ax = plt.subplots(3,3)\n",
    "for r in range(0,3):\n",
    "    for i in range(1,4):\n",
    "        ax[r,0].plot(ave_pos_noise[r,:,i],'-o')\n",
    "    for i in range(6,9):    \n",
    "        ax[r,1].plot(ave_pos_noise[r,:,i],'-o')\n",
    "        \n",
    "    for i in [10,11]:\n",
    "        ax[r,2].plot(ave_pos_noise[r,:,i],'-o')    \n",
    "    ax[r,1].set_yticks([])\n",
    "    ax[r,2].set_yticks([])\n",
    "    for i in range(0,3):\n",
    "        ax[r,i].set_ylim(0,1)\n",
    "        ax[r,i].set_xticks([])\n",
    "        ax[2,i].set_xticks(range(0,4))\n",
    "        ax[2,i].set_xticklabels(['1','2','3','4'])\n",
    "ax[1,0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0, 'Limb Position', ha='center')\n",
    "ax[0,0].legend(['sae','cnn','vcnn','ecnn'])\n",
    "ax[0,1].legend(['sae-lda','cnn-lda','vcnn-lda','ecnn-lda'])\n",
    "ax[0,2].legend(['LDA','LDA-corrupt'])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models_2_cv/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_cv_1_sparse_hist.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-14404b3f90b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfoldername\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msub_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfeat_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_dim_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_ep_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_bat_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mn_train\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scale\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_lr_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'_cv_'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_sparse'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_hist.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m             \u001b[0msvae_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msae_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn_hist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvcnn_hist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models_2_cv/TR1_feat_dim_5_ep_30_bat_128_fullgaussflat4_5_lr_10_cv_1_sparse_hist.p'"
     ]
    }
   ],
   "source": [
    "## Plot training metrics trajectories\n",
    "# initialize parameters\n",
    "train_grp = 2\n",
    "dt = 'cv'\n",
    "feat_type = 'feat'\n",
    "latent_dim = 5\n",
    "epochs = 30\n",
    "n_train = 'fullgaussflat4'\n",
    "train_scale = 5\n",
    "foldername = 'models' + '_' + str(train_grp) + '_' + dt\n",
    "batch_size = 128\n",
    "n_test = 0\n",
    "lr = 0.001\n",
    "\n",
    "# initialize loss and accuracy matrices\n",
    "loss = np.full([4,4,epochs],np.nan)\n",
    "val_loss = np.full([4,4,epochs],np.nan)\n",
    "acc = np.full([4,4,epochs],np.nan)\n",
    "val_acc = np.full([4,4,epochs],np.nan)\n",
    "\n",
    "# loop through subjects\n",
    "for sub in range(1,2):#6):\n",
    "    # loop through cross validations\n",
    "    for cv in range(1,5):\n",
    "        # load data\n",
    "        filename = foldername + '/' + sub_type + str(sub) + '_' + feat_type + '_dim_' + str(latent_dim) + '_ep_' + str(epochs) + '_bat_' + str(batch_size) + '_' + n_train + '_' + str(train_scale) + '_lr_' + str(int(lr*10000)) \n",
    "        filename += '_cv_'+ str(cv) + '_sparse'\n",
    "        with open(filename + '_hist.p', 'rb') as f:\n",
    "            svae_hist, sae_hist, cnn_hist, vcnn_hist = pickle.load(f)\n",
    "\n",
    "        svae_hist = np.transpose(svae_hist)\n",
    "\n",
    "        # compile losses and accuracies (uncomment back if all models have same # epochs)\n",
    "        loss[cv-1,:2,:] = np.array([svae_hist[2,:], sae_hist['loss']])#, cnn_hist['loss'], vcnn_hist['loss']])\n",
    "        val_loss[cv-1,:2,:] = np.array([svae_hist[9,:], sae_hist['val_loss']])#, cnn_hist['val_loss'], vcnn_hist['val_loss']])\n",
    "        acc[cv-1,:2,:] = np.array([svae_hist[5,:], sae_hist['accuracy']])#, cnn_hist['accuracy'], vcnn_hist['accuracy']])\n",
    "        val_acc[cv-1,:2,:] = np.array([svae_hist[-2,:], sae_hist['val_accuracy']])#, cnn_hist['val_accuracy'], vcnn_hist['val_accuracy']])\n",
    "\n",
    "        # 0-30 for 30 epochs\n",
    "        loss[cv-1,2:,0:30] = np.array([cnn_hist['loss'], vcnn_hist['loss']])\n",
    "        val_loss[cv-1,2:,0:30] = np.array([cnn_hist['val_loss'], vcnn_hist['val_loss']])\n",
    "        acc[cv-1,2:,0:30] = np.array([cnn_hist['accuracy'], vcnn_hist['clf_accuracy']])\n",
    "        val_acc[cv-1,2:,0:30] = np.array([cnn_hist['val_accuracy'], vcnn_hist['val_clf_accuracy']])\n",
    "\n",
    "    # load results\n",
    "    resultsfile = filename\n",
    "    # with open(resultsfile + '_results.p', 'rb') as f:\n",
    "    #     acc_all, acc_clean, acc_noise = pickle.load(f)\n",
    "\n",
    "    # average metrics over cvs\n",
    "    ave_loss = np.mean(loss,axis=0)\n",
    "    ave_val_loss = np.mean(val_loss,axis=0)\n",
    "    ave_acc = np.mean(acc,axis=0)\n",
    "    ave_val_acc = np.mean(val_acc,axis=0)\n",
    "\n",
    "    # plot metrics over epochs\n",
    "    plt.figure(sub)\n",
    "    for i in range(0,4):\n",
    "        ax = plt.subplot(2,2,i+1)\n",
    "        ax.plot(ave_loss[i])\n",
    "        ax.plot(ave_val_loss[i])\n",
    "        # ax.set_ylim(0,5)\n",
    "    plt.figure(sub+1)\n",
    "    for i in range(0,4):\n",
    "        ax2 = plt.subplot(2,2,i+1)\n",
    "        ax2.plot(ave_acc[i])\n",
    "        ax2.plot(ave_val_acc[i])\n",
    "        ax2.set_ylim(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot generated or reconstructed features\n",
    "col = ['k','b','r','g','c','y','m']\n",
    "fig = plt.figure()\n",
    "\n",
    "# number of channels\n",
    "ch_max = x_noise.shape[1]\n",
    "\n",
    "# number of classes\n",
    "cl_max = y_train.shape[1]\n",
    "\n",
    "# loop through channels\n",
    "for i in range(0,ch_max-1):\n",
    "    ax = plt.subplot(6,1,i+1)\n",
    "\n",
    "    # loop through classes\n",
    "    for cl in range(2,3):\n",
    "        # index inputs from current class\n",
    "        x_noise_cl = x_noise[y_train[:,cl]==1,i,:]\n",
    "        x_clean_cl = x_clean[y_train[:,cl]==1,i,:]\n",
    "        dec_ind = gen_clf == cl\n",
    "\n",
    "        ## plot all noisy features\n",
    "        # for x_all in range(0,x_noise_cl.shape[0]):\n",
    "        #     ax.plot(x_noise_cl[x_all,:,0],col[cl],linewidth=1)\n",
    "\n",
    "        ## plot all clean features\n",
    "        for x_all in range(0,x_clean_cl.shape[0]):\n",
    "            ax.plot(x_clean_cl[x_all,:,0],col[cl],linewidth=.5,linestyle='-')\n",
    "\n",
    "        ## plot all decoder output\n",
    "        max_gen = 100\n",
    "        ax.plot(np.transpose(dec_out[dec_ind,i,:,0][:max_gen,:]),col[cl+1],linewidth=.5,linestyle='--')\n",
    "        \n",
    "        ## plot mean of noisy features\n",
    "        # ax.plot(np.mean(x_cl[:x_cl.shape[0],:,0],axis=0),col[cl],linewidth=1)\n",
    "\n",
    "        ## plot mean of clean features\n",
    "        # ax.plot(np.mean(x_clean_cl[:x_clean_cl.shape[0],:,0],axis=0),col[cl],linewidth=1,linestyle=':')\n",
    "\n",
    "        ## plot mean of reconstructed output\n",
    "        # rec_cl = dec_out[y_train[:,cl]==1,i,:]\n",
    "        # ax.plot(np.mean(rec_cl[:,:,0],axis=0),col[cl],linewidth=1,linestyle='--')\n",
    "        \n",
    "        ## plot mean of decoder output\n",
    "        # ax.plot(np.mean(np.transpose(dec_out[dec_ind,i,:,0]),axis=1),col[cl+1],linewidth=.5,linestyle='--')\n",
    "\n",
    "    ax.set_ylim([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load noise results\n",
    "sub_all, sub_noise, sub_clean, ave_all, ave_noise, ave_clean = loop.ave_results(params, sub_type, train_grp=2, feat_type='feat',epochs=30,n_train='fullgaussflat4',train_scale=5,n_test='partgauss2', latent_dim=4,loop_i='noise', dt='cv')\n",
    "sub_all, sub_noise, sub_clean, flat_ave_all, flat_ave_noise, flat_ave_clean = loop.ave_results(params, sub_type, train_grp=2, feat_type='feat',epochs=30,n_train='fullgaussflat4',train_scale=5,n_test='partflat2', latent_dim=4,loop_i='noise',dt='cv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'flat_ave_noise' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f80215fd5ceb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_ave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_ave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mave_noise\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'-o'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flat_ave_noise' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1200 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy vs. noise\n",
    "fig,ax = plt.subplots(1,3)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'-o')\n",
    "for i in range(4,9):    \n",
    "    ax[1].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'-o')\n",
    "for i in range(9,14):    \n",
    "    ax[2].plot(np.hstack((flat_ave_noise[:,i],ave_noise[:,i])),'--o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0, 'Type of Noise', ha='center')\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])\n",
    "ax[1].legend(['svcnn-lda','sae-lda','cnn-lda','vcnn-lda','rec-lda'])\n",
    "ax[2].legend(['LDA','LDA-corrupt','QDA','QDA-corrupt','ch'])\n",
    "ax[1].set_yticks([])\n",
    "ax[2].set_yticks([])\n",
    "for i in range(0,3):\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_xticks(range(0,6))\n",
    "    ax[i].set_xticklabels(['Flat','1','2','3','4','5'])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot accuracy vs. noise\n",
    "fig,ax = plt.subplots(1,3)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'-o')\n",
    "for i in range(4,9):    \n",
    "    ax[1].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'-o')\n",
    "for i in range(9,14):    \n",
    "    ax[2].plot(np.hstack((flat_ave_clean[:,i],ave_clean[:,i])),'--o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0, 'Type of Noise', ha='center')\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])\n",
    "ax[1].legend(['svcnn-lda','sae-lda','cnn-lda','vcnn-lda','rec-lda'])\n",
    "ax[2].legend(['LDA','LDA-corrupt','QDA','QDA-corrupt','ch'])\n",
    "ax[1].set_yticks([])\n",
    "ax[2].set_yticks([])\n",
    "for i in range(0,3):\n",
    "    ax[i].set_ylim(0,1)\n",
    "    ax[i].set_xticks(range(0,6))\n",
    "    ax[i].set_xticklabels(['Flat','1','2','3','4','5'])\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 - clf_loss: 0.7969 - decoder_accuracy: 0.0039 - clf_accuracy: 0.6803 - val_loss: 1.0422 - val_decoder_loss: 0.0800 - val_clf_loss: 0.9572 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6190\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 2s 126us/sample - loss: 0.8524 - decoder_loss: 0.0848 - clf_loss: 0.7650 - decoder_accuracy: 0.0039 - clf_accuracy: 0.6941 - val_loss: 1.0159 - val_decoder_loss: 0.0789 - val_clf_loss: 0.9367 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6186\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 2s 115us/sample - loss: 0.8229 - decoder_loss: 0.0882 - clf_loss: 0.7323 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7081 - val_loss: 0.9926 - val_decoder_loss: 0.0857 - val_clf_loss: 0.9000 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6271\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.8014 - decoder_loss: 0.0913 - clf_loss: 0.7074 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7258 - val_loss: 1.0692 - val_decoder_loss: 0.0987 - val_clf_loss: 0.9689 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6473\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.7771 - decoder_loss: 0.0939 - clf_loss: 0.6808 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7356 - val_loss: 0.9612 - val_decoder_loss: 0.0850 - val_clf_loss: 0.8723 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6579\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.7592 - decoder_loss: 0.0960 - clf_loss: 0.6605 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7415 - val_loss: 1.0534 - val_decoder_loss: 0.0971 - val_clf_loss: 0.9582 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6359\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 2s 113us/sample - loss: 0.7400 - decoder_loss: 0.0978 - clf_loss: 0.6395 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7510 - val_loss: 0.9241 - val_decoder_loss: 0.0879 - val_clf_loss: 0.8316 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6870\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 2s 115us/sample - loss: 0.7153 - decoder_loss: 0.0994 - clf_loss: 0.6134 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7596 - val_loss: 0.9645 - val_decoder_loss: 0.0872 - val_clf_loss: 0.8736 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6748\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.7093 - decoder_loss: 0.1006 - clf_loss: 0.6063 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7679 - val_loss: 0.9345 - val_decoder_loss: 0.0939 - val_clf_loss: 0.8378 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6786\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 2s 114us/sample - loss: 0.6957 - decoder_loss: 0.1015 - clf_loss: 0.5913 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7700 - val_loss: 1.0018 - val_decoder_loss: 0.0965 - val_clf_loss: 0.9047 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6683\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 2s 119us/sample - loss: 0.6823 - decoder_loss: 0.1022 - clf_loss: 0.5776 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7784 - val_loss: 0.8892 - val_decoder_loss: 0.1014 - val_clf_loss: 0.7889 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7013\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 2s 123us/sample - loss: 0.6701 - decoder_loss: 0.1026 - clf_loss: 0.5647 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7856 - val_loss: 0.9618 - val_decoder_loss: 0.0983 - val_clf_loss: 0.8625 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6625\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 2s 127us/sample - loss: 0.6686 - decoder_loss: 0.1027 - clf_loss: 0.5633 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7844 - val_loss: 0.9086 - val_decoder_loss: 0.0949 - val_clf_loss: 0.8090 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6879\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 2s 121us/sample - loss: 0.6506 - decoder_loss: 0.1029 - clf_loss: 0.5451 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7887 - val_loss: 0.9867 - val_decoder_loss: 0.1028 - val_clf_loss: 0.8810 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6881\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.6377 - decoder_loss: 0.1028 - clf_loss: 0.5323 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7957 - val_loss: 0.9662 - val_decoder_loss: 0.0973 - val_clf_loss: 0.8656 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6989\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 2s 122us/sample - loss: 0.6410 - decoder_loss: 0.1027 - clf_loss: 0.5353 - decoder_accuracy: 0.0039 - clf_accuracy: 0.7954 - val_loss: 0.8722 - val_decoder_loss: 0.0925 - val_clf_loss: 0.7788 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7068\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 2s 121us/sample - loss: 0.6246 - decoder_loss: 0.1024 - clf_loss: 0.5198 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8031 - val_loss: 0.9064 - val_decoder_loss: 0.0979 - val_clf_loss: 0.8021 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7100\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 2s 106us/sample - loss: 0.6225 - decoder_loss: 0.1021 - clf_loss: 0.5176 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8027 - val_loss: 0.9816 - val_decoder_loss: 0.1066 - val_clf_loss: 0.8788 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6984\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 2s 107us/sample - loss: 0.6135 - decoder_loss: 0.1018 - clf_loss: 0.5088 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8022 - val_loss: 0.8649 - val_decoder_loss: 0.0989 - val_clf_loss: 0.7607 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7168\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 2s 109us/sample - loss: 0.6048 - decoder_loss: 0.1014 - clf_loss: 0.5007 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8070 - val_loss: 0.9282 - val_decoder_loss: 0.0952 - val_clf_loss: 0.8287 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.7043\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 2s 109us/sample - loss: 0.5918 - decoder_loss: 0.1010 - clf_loss: 0.4884 - decoder_accuracy: 0.0039 - clf_accuracy: 0.8131 - val_loss: 0.9987 - val_decoder_loss: 0.0995 - val_clf_loss: 0.8918 - val_decoder_accuracy: 0.0020 - val_clf_accuracy: 0.6973\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 90us/sample - loss: 2.0452 - accuracy: 0.1662 - val_loss: 1.9161 - val_accuracy: 0.2060\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 1.8632 - accuracy: 0.2239 - val_loss: 1.7475 - val_accuracy: 0.2765\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 1.6901 - accuracy: 0.3037 - val_loss: 1.5965 - val_accuracy: 0.3405\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.5500 - accuracy: 0.3784 - val_loss: 1.4882 - val_accuracy: 0.3752\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.4397 - accuracy: 0.4257 - val_loss: 1.4160 - val_accuracy: 0.4010\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.3500 - accuracy: 0.4588 - val_loss: 1.3419 - val_accuracy: 0.4441\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.2722 - accuracy: 0.4868 - val_loss: 1.3386 - val_accuracy: 0.4517\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.2249 - accuracy: 0.5035 - val_loss: 1.2498 - val_accuracy: 0.4789\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.1881 - accuracy: 0.5169 - val_loss: 1.3074 - val_accuracy: 0.4468\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.1456 - accuracy: 0.5278 - val_loss: 1.2494 - val_accuracy: 0.4940\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.1241 - accuracy: 0.5399 - val_loss: 1.2053 - val_accuracy: 0.4921\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 59us/sample - loss: 1.1041 - accuracy: 0.5448 - val_loss: 1.2224 - val_accuracy: 0.4925\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.0819 - accuracy: 0.5511 - val_loss: 1.1890 - val_accuracy: 0.5054\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0698 - accuracy: 0.5538 - val_loss: 1.1664 - val_accuracy: 0.5048\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0640 - accuracy: 0.5659 - val_loss: 1.2042 - val_accuracy: 0.4943\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0506 - accuracy: 0.5635 - val_loss: 1.2515 - val_accuracy: 0.4827\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 56us/sample - loss: 1.0426 - accuracy: 0.5713 - val_loss: 1.1926 - val_accuracy: 0.5060\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0357 - accuracy: 0.5733 - val_loss: 1.1857 - val_accuracy: 0.4978\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0275 - accuracy: 0.5798 - val_loss: 1.1746 - val_accuracy: 0.5159\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0249 - accuracy: 0.5763 - val_loss: 1.1662 - val_accuracy: 0.5121\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0182 - accuracy: 0.5802 - val_loss: 1.1726 - val_accuracy: 0.5071\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 1.0110 - accuracy: 0.5818 - val_loss: 1.2502 - val_accuracy: 0.5006\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 1.0005 - accuracy: 0.5875 - val_loss: 1.1660 - val_accuracy: 0.5143\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 55us/sample - loss: 0.9975 - accuracy: 0.5879 - val_loss: 1.1488 - val_accuracy: 0.5149\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9887 - accuracy: 0.5910 - val_loss: 1.1389 - val_accuracy: 0.5175\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 57us/sample - loss: 0.9891 - accuracy: 0.5895 - val_loss: 1.1643 - val_accuracy: 0.5098\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 61us/sample - loss: 0.9788 - accuracy: 0.5955 - val_loss: 1.1305 - val_accuracy: 0.5273\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 65us/sample - loss: 0.9813 - accuracy: 0.5941 - val_loss: 1.2040 - val_accuracy: 0.5097\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9716 - accuracy: 0.5968 - val_loss: 1.1121 - val_accuracy: 0.5254\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 54us/sample - loss: 0.9666 - accuracy: 0.5965 - val_loss: 1.1255 - val_accuracy: 0.5267\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 89us/sample - loss: 1.6775 - accuracy: 0.3281 - val_loss: 1.9610 - val_accuracy: 0.1538\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 52us/sample - loss: 1.3230 - accuracy: 0.4839 - val_loss: 1.3440 - val_accuracy: 0.4860\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 1.1476 - accuracy: 0.5527 - val_loss: 1.1524 - val_accuracy: 0.5581\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 1.0100 - accuracy: 0.6061 - val_loss: 1.0886 - val_accuracy: 0.5870\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.9111 - accuracy: 0.6429 - val_loss: 0.9760 - val_accuracy: 0.6143\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.8521 - accuracy: 0.6617 - val_loss: 0.8976 - val_accuracy: 0.6594\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.8019 - accuracy: 0.6763 - val_loss: 0.8646 - val_accuracy: 0.6659\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.7555 - accuracy: 0.6914 - val_loss: 0.8689 - val_accuracy: 0.6524\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.7245 - accuracy: 0.7047 - val_loss: 0.8760 - val_accuracy: 0.6614\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.7045 - accuracy: 0.7173 - val_loss: 0.8957 - val_accuracy: 0.6563\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6735 - accuracy: 0.7317 - val_loss: 0.8447 - val_accuracy: 0.6738\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6526 - accuracy: 0.7435 - val_loss: 0.9342 - val_accuracy: 0.6554\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.6371 - accuracy: 0.7534 - val_loss: 0.8494 - val_accuracy: 0.6838\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 51us/sample - loss: 0.6186 - accuracy: 0.7587 - val_loss: 0.8839 - val_accuracy: 0.6838\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 50us/sample - loss: 0.6085 - accuracy: 0.7623 - val_loss: 0.9472 - val_accuracy: 0.6600\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5867 - accuracy: 0.7740 - val_loss: 0.8320 - val_accuracy: 0.6911\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5762 - accuracy: 0.7780 - val_loss: 0.8080 - val_accuracy: 0.7032\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5667 - accuracy: 0.7794 - val_loss: 0.7982 - val_accuracy: 0.6952\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5558 - accuracy: 0.7869 - val_loss: 0.8498 - val_accuracy: 0.6967\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 49us/sample - loss: 0.5539 - accuracy: 0.7866 - val_loss: 0.7998 - val_accuracy: 0.7084\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5414 - accuracy: 0.7928 - val_loss: 0.7803 - val_accuracy: 0.7173\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5390 - accuracy: 0.7937 - val_loss: 0.7976 - val_accuracy: 0.7108\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5285 - accuracy: 0.7958 - val_loss: 0.7989 - val_accuracy: 0.7178\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.5201 - accuracy: 0.7971 - val_loss: 0.7965 - val_accuracy: 0.7110\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.5206 - accuracy: 0.7980 - val_loss: 0.7861 - val_accuracy: 0.7035\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.5158 - accuracy: 0.8025 - val_loss: 0.7738 - val_accuracy: 0.7076\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.4999 - accuracy: 0.8040 - val_loss: 0.7668 - val_accuracy: 0.7230\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 46us/sample - loss: 0.5034 - accuracy: 0.8053 - val_loss: 0.8087 - val_accuracy: 0.7011\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 48us/sample - loss: 0.4942 - accuracy: 0.8090 - val_loss: 0.9211 - val_accuracy: 0.6771\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 47us/sample - loss: 0.4908 - accuracy: 0.8116 - val_loss: 0.7583 - val_accuracy: 0.7241\n",
      "Train on 18900 samples, validate on 6300 samples\n",
      "Epoch 1/30\n",
      "18900/18900 [==============================] - 2s 120us/sample - loss: 0.5095 - accuracy: 0.2328 - val_loss: 0.4812 - val_accuracy: 0.2057\n",
      "Epoch 2/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.4240 - accuracy: 0.3496 - val_loss: 0.4153 - val_accuracy: 0.3584\n",
      "Epoch 3/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3843 - accuracy: 0.4206 - val_loss: 0.3910 - val_accuracy: 0.3819\n",
      "Epoch 4/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3542 - accuracy: 0.4760 - val_loss: 0.3744 - val_accuracy: 0.3992\n",
      "Epoch 5/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.3293 - accuracy: 0.5207 - val_loss: 0.3395 - val_accuracy: 0.4692\n",
      "Epoch 6/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.3120 - accuracy: 0.5492 - val_loss: 0.3215 - val_accuracy: 0.5054\n",
      "Epoch 7/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.3000 - accuracy: 0.5707 - val_loss: 0.3261 - val_accuracy: 0.5073\n",
      "Epoch 8/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2897 - accuracy: 0.5913 - val_loss: 0.3152 - val_accuracy: 0.5273\n",
      "Epoch 9/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2799 - accuracy: 0.6065 - val_loss: 0.3053 - val_accuracy: 0.5476\n",
      "Epoch 10/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2735 - accuracy: 0.6206 - val_loss: 0.3049 - val_accuracy: 0.5617\n",
      "Epoch 11/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2648 - accuracy: 0.6347 - val_loss: 0.3034 - val_accuracy: 0.5713\n",
      "Epoch 12/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2603 - accuracy: 0.6458 - val_loss: 0.2934 - val_accuracy: 0.5776\n",
      "Epoch 13/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2575 - accuracy: 0.6497 - val_loss: 0.2974 - val_accuracy: 0.5857\n",
      "Epoch 14/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2520 - accuracy: 0.6616 - val_loss: 0.2931 - val_accuracy: 0.5900\n",
      "Epoch 15/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2472 - accuracy: 0.6751 - val_loss: 0.3072 - val_accuracy: 0.5817\n",
      "Epoch 16/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2440 - accuracy: 0.6779 - val_loss: 0.2874 - val_accuracy: 0.6021\n",
      "Epoch 17/30\n",
      "18900/18900 [==============================] - 1s 65us/sample - loss: 0.2402 - accuracy: 0.6923 - val_loss: 0.2886 - val_accuracy: 0.6121\n",
      "Epoch 18/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2360 - accuracy: 0.6984 - val_loss: 0.2876 - val_accuracy: 0.6183\n",
      "Epoch 19/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2317 - accuracy: 0.7051 - val_loss: 0.2863 - val_accuracy: 0.6184\n",
      "Epoch 20/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2309 - accuracy: 0.7067 - val_loss: 0.2898 - val_accuracy: 0.6224\n",
      "Epoch 21/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2262 - accuracy: 0.7174 - val_loss: 0.2742 - val_accuracy: 0.6432\n",
      "Epoch 22/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2235 - accuracy: 0.7241 - val_loss: 0.2766 - val_accuracy: 0.6454\n",
      "Epoch 23/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2217 - accuracy: 0.7274 - val_loss: 0.2789 - val_accuracy: 0.6463\n",
      "Epoch 24/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2210 - accuracy: 0.7271 - val_loss: 0.2596 - val_accuracy: 0.6684\n",
      "Epoch 25/30\n",
      "18900/18900 [==============================] - 1s 67us/sample - loss: 0.2167 - accuracy: 0.7336 - val_loss: 0.2770 - val_accuracy: 0.6490\n",
      "Epoch 26/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2140 - accuracy: 0.7444 - val_loss: 0.2644 - val_accuracy: 0.6713\n",
      "Epoch 27/30\n",
      "18900/18900 [==============================] - 1s 66us/sample - loss: 0.2133 - accuracy: 0.7457 - val_loss: 0.2730 - val_accuracy: 0.6516\n",
      "Epoch 28/30\n",
      "18900/18900 [==============================] - 1s 68us/sample - loss: 0.2112 - accuracy: 0.7465 - val_loss: 0.2732 - val_accuracy: 0.6590\n",
      "Epoch 29/30\n",
      "18900/18900 [==============================] - 1s 72us/sample - loss: 0.2097 - accuracy: 0.7504 - val_loss: 0.2687 - val_accuracy: 0.6424\n",
      "Epoch 30/30\n",
      "18900/18900 [==============================] - 1s 72us/sample - loss: 0.2103 - accuracy: 0.7489 - val_loss: 0.2610 - val_accuracy: 0.6662\n"
     ]
    }
   ],
   "source": [
    "# Loop through noise\n",
    "acc_all, acc_noise, acc_clean, file_name = loop.loop_noise(raw, params, sub_type, load=False, n_train='fullgaussflat4', train_scale=5, n_test=0,epochs=30, batch_size = 128, sparsity=True,dt='cv',feat_type='feat',noise=True, latent_dim=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dimension Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through latent dimensions\n",
    "sub_all, sub_noise, sub_clean, file_name = loop.loop_alldim(raw, params, sub_type, load=False, n_train='gaussflat', train_scale=3, n_test='gauss', test_scale=1,epochs=30, sparsity=True, dt='0414')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load results from latent dimension loop, sparse vs. not sparse\n",
    "sub_all, sub_noise, sub_clean, sparse_all, sparse_noise, sparse_clean = loop.load_results(params, sub_type=sub_type,sparsity=True, dt='0414')\n",
    "sub_all, sub_noise, sub_clean, ave_all, ave_noise, ave_clean = loop.load_results(params, sub_type=sub_type,sparsity=False, dt='0414')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs. latent dimension\n",
    "fig,ax = plt.subplots(1,2)\n",
    "for i in range(0,4):\n",
    "    ax[0].plot(ave_noise[:,i],'-o')\n",
    "    ax[1].plot(sparse_noise[:,i],'-o')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "fig.text(0.5, 0.04, 'Latent Dimension', ha='center')\n",
    "ax[0].set_ylim(0.5,1)\n",
    "ax[1].set_ylim(0.5,1)\n",
    "ax[0].legend(['svcnn','sae','cnn','vcnn'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
