{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import os\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import process_data as prd\n",
    "set_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'TR'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/TR1_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "## Training \n",
    "sub = 1\n",
    "train_grp = 2\n",
    "n_train = 'fullallmix4'\n",
    "train_scale = 5\n",
    "cv_type = 'manual'\n",
    "scaler_load = False\n",
    "feat_type = 'feat'\n",
    "\n",
    "ind = (params[:,0] == sub) & (params[:,3] == train_grp)\n",
    "\n",
    "x_train, x_test, x_valid, p_train, p_test, p_valid = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,load=True,train_grp=train_grp)\n",
    "\n",
    "emg_scale = np.ones((np.size(x_train,1),1))\n",
    "for i in range(np.size(x_train,1)):\n",
    "    emg_scale[i] = 5/np.max(np.abs(x_train[:,i,:]))\n",
    "x_train = x_train*emg_scale\n",
    "x_valid = x_valid*emg_scale\n",
    "\n",
    "x_train_noise, x_train_clean, y_train_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "x_valid_noise, x_valid_clean, y_valid_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "\n",
    "x_train_noise[x_train_noise > 5] = 5\n",
    "x_train_noise[x_train_noise < -5] = -5\n",
    "x_train_clean[x_train_clean > 5] = 5\n",
    "x_train_clean[x_train_clean < -5] = -5\n",
    "\n",
    "x_valid_noise[x_valid_noise > 5] = 5\n",
    "x_valid_noise[x_valid_noise < -5] = -5\n",
    "x_valid_clean[x_valid_clean > 5] = 5\n",
    "x_valid_clean[x_valid_clean < -5] = -5\n",
    "\n",
    "# shuffle data to make even batches\n",
    "x_train_noise, x_train_clean, y_train_clean = shuffle(x_train_noise, x_train_clean, y_train_clean, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Extract features\n",
    "x_train_noise_cnn, scaler = prd.extract_scale(x_train_noise,scaler,scaler_load,ft=feat_type,emg_scale=emg_scale) \n",
    "x_train_clean_cnn, _ = prd.extract_scale(x_train_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_noise_cnn, _ = prd.extract_scale(x_valid_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_clean_cnn, _ = prd.extract_scale(x_valid_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "\n",
    "# reshape data for nonconvolutional network\n",
    "x_train_noise_mlp = x_train_noise_cnn.reshape(x_train_noise_cnn.shape[0],-1)\n",
    "x_valid_noise_mlp = x_valid_noise_cnn.reshape(x_valid_noise_cnn.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPenc(Model):\n",
    "  def __init__(self, latent_dim=4, name='enc'):\n",
    "    super(MLPenc, self).__init__(name=name)\n",
    "    self.dense1 = Dense(24, activation='relu')\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.dense2 = Dense(12, activation='relu')\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.dense3 = Dense(8, activation='relu')\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.latent = Dense(latent_dim, activity_regularizer=tf.keras.regularizers.l1(10e-5))\n",
    "    self.bn4 = BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.latent(x)\n",
    "    return self.bn4(x)\n",
    "\n",
    "class CLF(Model):\n",
    "  def __init__(self, n_class=7, name='clf'):\n",
    "    super(CLF, self).__init__(name=name)\n",
    "    self.dense1 = Dense(n_class, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.dense1(x)\n",
    "  \n",
    "class MLP(Model):\n",
    "  def __init__(self):\n",
    "    super(MLP, self).__init__()\n",
    "    self.enc = MLPenc()\n",
    "    self.clf = CLF()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    return self.clf(x)\n",
    "\n",
    "class CNNenc(Model):\n",
    "  def __init__(self, latent_dim=4, name='enc'):\n",
    "    super(CNNenc, self).__init__(name=name)\n",
    "    self.conv1 = Conv2D(32,(3,2), activation='relu', strides=1, padding=\"same\")\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.conv2 = Conv2D(32,3, activation='relu', strides=2, padding=\"same\")\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.flatten = Flatten()\n",
    "    self.dense1 = Dense(16, activation='relu')\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.latent = Dense(latent_dim, activity_regularizer=tf.keras.regularizers.l1(10e-5))\n",
    "    self.bn4 = BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.latent(x)\n",
    "    return self.bn4(x)\n",
    "  \n",
    "class CNN(Model):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    self.enc = CNNenc()\n",
    "    self.clf = CLF()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    return self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "cnn = CNN()\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y, mod='cnn'):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    if mod == 'cnn':\n",
    "      y_out = cnn(x)\n",
    "    elif mod == 'mlp':\n",
    "      y_out = mlp(x)\n",
    "    loss = loss_fn(y, y_out)\n",
    "  if mod == 'cnn':\n",
    "    gradients = tape.gradient(loss, cnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n",
    "  elif mod == 'mlp':\n",
    "    gradients = tape.gradient(loss, mlp.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y, mod='cnn'):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  if mod == 'cnn':\n",
    "    y_out = cnn(x)\n",
    "  elif mod == 'mlp':\n",
    "    y_out = mlp(x)\n",
    "  t_loss = loss_fn(y, y_out)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(y, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmlp_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_mlp, y_train_clean)).batch(128)\n",
    "testmlp_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_mlp, y_valid_clean)).batch(128)\n",
    "traincnn_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_cnn, y_train_clean)).batch(128)\n",
    "testcnn_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_cnn, y_valid_clean)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer mlp_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.9407752752304077, Accuracy: 17.341270446777344, Test Loss: 1.915333867073059, Test Accuracy: 23.265872955322266\n",
      "Epoch 2, Loss: 1.8639380931854248, Accuracy: 26.789682388305664, Test Loss: 1.8328561782836914, Test Accuracy: 31.0\n",
      "Epoch 3, Loss: 1.8124122619628906, Accuracy: 33.488094329833984, Test Loss: 1.7894322872161865, Test Accuracy: 36.94047546386719\n",
      "Epoch 4, Loss: 1.7797069549560547, Accuracy: 37.58333206176758, Test Loss: 1.7637282609939575, Test Accuracy: 39.234127044677734\n",
      "Epoch 5, Loss: 1.7565616369247437, Accuracy: 39.78174591064453, Test Loss: 1.7426151037216187, Test Accuracy: 41.599205017089844\n",
      "Epoch 6, Loss: 1.74114990234375, Accuracy: 41.36507797241211, Test Loss: 1.7306506633758545, Test Accuracy: 42.71825408935547\n",
      "Epoch 7, Loss: 1.7302790880203247, Accuracy: 42.47618865966797, Test Loss: 1.7213845252990723, Test Accuracy: 43.63492202758789\n",
      "Epoch 8, Loss: 1.721256971359253, Accuracy: 43.56745910644531, Test Loss: 1.7128328084945679, Test Accuracy: 44.7103157043457\n",
      "Epoch 9, Loss: 1.7125831842422485, Accuracy: 44.50396728515625, Test Loss: 1.7052241563796997, Test Accuracy: 45.42460632324219\n",
      "Epoch 10, Loss: 1.705614686012268, Accuracy: 45.18650817871094, Test Loss: 1.6986745595932007, Test Accuracy: 46.0\n",
      "Epoch 11, Loss: 1.7001430988311768, Accuracy: 45.591270446777344, Test Loss: 1.6935187578201294, Test Accuracy: 46.38888931274414\n",
      "Epoch 12, Loss: 1.6956706047058105, Accuracy: 46.257938385009766, Test Loss: 1.6901003122329712, Test Accuracy: 46.69047546386719\n",
      "Epoch 13, Loss: 1.691981315612793, Accuracy: 46.76984405517578, Test Loss: 1.684315800666809, Test Accuracy: 47.591270446777344\n",
      "Epoch 14, Loss: 1.6879535913467407, Accuracy: 47.07539749145508, Test Loss: 1.6811516284942627, Test Accuracy: 48.03174591064453\n",
      "Epoch 15, Loss: 1.6839120388031006, Accuracy: 47.69841003417969, Test Loss: 1.6780790090560913, Test Accuracy: 48.341270446777344\n",
      "Epoch 16, Loss: 1.6792349815368652, Accuracy: 48.11111068725586, Test Loss: 1.6756715774536133, Test Accuracy: 48.511905670166016\n",
      "Epoch 17, Loss: 1.675920844078064, Accuracy: 48.36904525756836, Test Loss: 1.6727874279022217, Test Accuracy: 48.74603271484375\n",
      "Epoch 18, Loss: 1.6725672483444214, Accuracy: 48.75396728515625, Test Loss: 1.670324444770813, Test Accuracy: 49.0\n",
      "Epoch 19, Loss: 1.6697032451629639, Accuracy: 49.13492202758789, Test Loss: 1.669053077697754, Test Accuracy: 48.984127044677734\n",
      "Epoch 20, Loss: 1.6676945686340332, Accuracy: 49.373016357421875, Test Loss: 1.6668875217437744, Test Accuracy: 49.26190185546875\n",
      "Epoch 21, Loss: 1.665562629699707, Accuracy: 49.619049072265625, Test Loss: 1.6649543046951294, Test Accuracy: 49.43650817871094\n",
      "Epoch 22, Loss: 1.6636806726455688, Accuracy: 49.80158996582031, Test Loss: 1.6634061336517334, Test Accuracy: 49.595237731933594\n",
      "Epoch 23, Loss: 1.662452220916748, Accuracy: 49.849205017089844, Test Loss: 1.6628164052963257, Test Accuracy: 49.66666793823242\n",
      "Epoch 24, Loss: 1.6609811782836914, Accuracy: 49.98015594482422, Test Loss: 1.6612005233764648, Test Accuracy: 49.76190185546875\n",
      "Epoch 25, Loss: 1.6598438024520874, Accuracy: 50.11111068725586, Test Loss: 1.660865068435669, Test Accuracy: 49.78174591064453\n",
      "Epoch 26, Loss: 1.658828616142273, Accuracy: 50.31745910644531, Test Loss: 1.6611428260803223, Test Accuracy: 49.8055534362793\n",
      "Epoch 27, Loss: 1.6577728986740112, Accuracy: 50.30158615112305, Test Loss: 1.6668603420257568, Test Accuracy: 49.21825408935547\n",
      "Epoch 28, Loss: 1.656785011291504, Accuracy: 50.53571319580078, Test Loss: 1.6735215187072754, Test Accuracy: 48.59920883178711\n",
      "Epoch 29, Loss: 1.6560205221176147, Accuracy: 50.57143020629883, Test Loss: 1.6589832305908203, Test Accuracy: 50.08333206176758\n",
      "Epoch 30, Loss: 1.6546251773834229, Accuracy: 50.68253707885742, Test Loss: 1.6568464040756226, Test Accuracy: 50.35317611694336\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for x, y in trainmlp_ds:\n",
    "    train_step(x, y, 'mlp')\n",
    "\n",
    "  for x_test, y_test in testmlp_ds:\n",
    "    test_step(x_test, y_test, 'mlp')\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_enc = mlp.get_layer(name='enc')\n",
    "mlp_weights = mlp_enc.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  test_loss.reset_states()\n",
    "  test_accuracy.reset_states()\n",
    "\n",
    "  for x, y in traincnn_ds:\n",
    "    train_step(x, y, 'cnn')\n",
    "\n",
    "  for x_test, y_test in testcnn_ds:\n",
    "    test_step(x_test, y_test, 'cnn')\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result()}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "    f'Test Loss: {test_loss.result()}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
