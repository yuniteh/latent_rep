{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import os\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization\n",
    "from tensorflow.keras import Model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import process_data as prd\n",
    "set_gpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'TR'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/TR1_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "## Training \n",
    "sub = 1\n",
    "train_grp = 2\n",
    "n_train = 'fullallmix4'\n",
    "train_scale = 5\n",
    "cv_type = 'manual'\n",
    "scaler_load = False\n",
    "feat_type = 'feat'\n",
    "\n",
    "ind = (params[:,0] == sub) & (params[:,3] == train_grp)\n",
    "\n",
    "x_train, x_test, x_valid, p_train, p_test, p_valid = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,load=True,train_grp=train_grp)\n",
    "\n",
    "emg_scale = np.ones((np.size(x_train,1),1))\n",
    "for i in range(np.size(x_train,1)):\n",
    "    emg_scale[i] = 5/np.max(np.abs(x_train[:,i,:]))\n",
    "x_train = x_train*emg_scale\n",
    "x_valid = x_valid*emg_scale\n",
    "\n",
    "x_train_noise, x_train_clean, y_train_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "x_valid_noise, x_valid_clean, y_valid_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "\n",
    "x_train_noise[x_train_noise > 5] = 5\n",
    "x_train_noise[x_train_noise < -5] = -5\n",
    "x_train_clean[x_train_clean > 5] = 5\n",
    "x_train_clean[x_train_clean < -5] = -5\n",
    "\n",
    "x_valid_noise[x_valid_noise > 5] = 5\n",
    "x_valid_noise[x_valid_noise < -5] = -5\n",
    "x_valid_clean[x_valid_clean > 5] = 5\n",
    "x_valid_clean[x_valid_clean < -5] = -5\n",
    "\n",
    "# shuffle data to make even batches\n",
    "x_train_noise, x_train_clean, y_train_clean = shuffle(x_train_noise, x_train_clean, y_train_clean, random_state = 0)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# Extract features\n",
    "x_train_noise_cnn, scaler = prd.extract_scale(x_train_noise,scaler,scaler_load,ft=feat_type,emg_scale=emg_scale) \n",
    "x_train_clean_cnn, _ = prd.extract_scale(x_train_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_noise_cnn, _ = prd.extract_scale(x_valid_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_clean_cnn, _ = prd.extract_scale(x_valid_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "\n",
    "# reshape data for nonconvolutional network\n",
    "x_train_noise_mlp = x_train_noise_cnn.reshape(x_train_noise_cnn.shape[0],-1)\n",
    "x_valid_noise_mlp = x_valid_noise_cnn.reshape(x_valid_noise_cnn.shape[0],-1)\n",
    "\n",
    "trainmlp_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_mlp, y_train_clean)).batch(128)\n",
    "testmlp_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_mlp, y_valid_clean)).batch(128)\n",
    "traincnn_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_cnn, y_train_clean)).batch(128)\n",
    "testcnn_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_cnn, y_valid_clean)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPenc(Model):\n",
    "  def __init__(self, latent_dim=4, name='enc'):\n",
    "    super(MLPenc, self).__init__(name=name)\n",
    "    self.dense1 = Dense(24, activation='relu')\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.dense2 = Dense(12, activation='relu')\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.dense3 = Dense(8, activation='relu')\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.latent = Dense(latent_dim, activity_regularizer=tf.keras.regularizers.l1(10e-5))\n",
    "    self.bn4 = BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.latent(x)\n",
    "    return self.bn4(x)\n",
    "\n",
    "class MLPenc_beta(Model):\n",
    "  def __init__(self, latent_dim=4, name='enc'):\n",
    "    super(MLPenc_beta, self).__init__(name=name)\n",
    "    self.dense1 = Dense(246, activation='relu')\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.dense2 = Dense(128, activation='relu')\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.dense3 = Dense(16, activation='relu')\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.latent = Dense(latent_dim, activity_regularizer=tf.keras.regularizers.l1(10e-5))\n",
    "    self.bn4 = BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.dense1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.dense2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.dense3(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.latent(x)\n",
    "    return self.bn4(x)\n",
    "\n",
    "class CLF(Model):\n",
    "  def __init__(self, n_class=7, name='clf'):\n",
    "    super(CLF, self).__init__(name=name)\n",
    "    self.dense1 = Dense(n_class, activation='softmax')\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.dense1(x)\n",
    "\n",
    "class CNNenc(Model):\n",
    "  def __init__(self, latent_dim=4, name='enc'):\n",
    "    super(CNNenc, self).__init__(name=name)\n",
    "    self.conv1 = Conv2D(32,(3,2), activation='relu', strides=1, padding=\"same\")\n",
    "    self.bn1 = BatchNormalization()\n",
    "    self.conv2 = Conv2D(32,3, activation='relu', strides=2, padding=\"same\")\n",
    "    self.bn2 = BatchNormalization()\n",
    "    self.flatten = Flatten()\n",
    "    self.dense1 = Dense(16, activation='relu')\n",
    "    self.bn3 = BatchNormalization()\n",
    "    self.latent = Dense(latent_dim, activity_regularizer=tf.keras.regularizers.l1(10e-5))\n",
    "    self.bn4 = BatchNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.conv2(x)\n",
    "    x = self.bn2(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.dense1(x)\n",
    "    x = self.bn3(x)\n",
    "    x = self.latent(x)\n",
    "    return self.bn4(x)\n",
    "\n",
    "class MLP(Model):\n",
    "  def __init__(self):\n",
    "    super(MLP, self).__init__()\n",
    "    self.enc = MLPenc()\n",
    "    self.clf = CLF()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    return self.clf(x)\n",
    "\n",
    "class MLPbeta(Model):\n",
    "  def __init__(self):\n",
    "    super(MLPbeta, self).__init__()\n",
    "    self.enc = MLPenc_beta()\n",
    "    self.clf = CLF()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    return self.clf(x)\n",
    "  \n",
    "class CNN(Model):\n",
    "  def __init__(self):\n",
    "    super(CNN, self).__init__()\n",
    "    self.enc = CNNenc()\n",
    "    self.clf = CLF()\n",
    "  \n",
    "  def call(self, x):\n",
    "    x = self.enc(x)\n",
    "    return self.clf(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "@tf.function\n",
    "def train_mlp(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_out = mlp(x)\n",
    "    loss = loss_fn(y, y_out)\n",
    "  gradients = tape.gradient(loss, mlp.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, mlp.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlp(x, y):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_out = mlp(x)\n",
    "  t_loss = loss_fn(y, y_out)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def train_mlpbeta(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_out = mlp_beta(x)\n",
    "    loss = loss_fn(y, y_out)\n",
    "  gradients = tape.gradient(loss, mlp_beta.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, mlp_beta.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlpbeta(x, y):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_out = mlp_beta(x)\n",
    "  t_loss = loss_fn(y, y_out)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def train_cnn(x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    y_out = cnn(x)\n",
    "    loss = loss_fn(y, y_out)\n",
    "  gradients = tape.gradient(loss, cnn.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_cnn(x, y):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  y_out = cnn(x)\n",
    "  t_loss = loss_fn(y, y_out)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(y, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer mlp_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.9241135120391846, Accuracy: 20.13492202758789, Test Loss: 1.865580439567566, Test Accuracy: 26.72222328186035\n",
      "Epoch 2, Loss: 1.8249220848083496, Accuracy: 32.36111068725586, Test Loss: 1.7958043813705444, Test Accuracy: 35.16270065307617\n",
      "Epoch 3, Loss: 1.784688115119934, Accuracy: 36.408729553222656, Test Loss: 1.7724262475967407, Test Accuracy: 37.31745910644531\n",
      "Epoch 4, Loss: 1.7680928707122803, Accuracy: 37.95634841918945, Test Loss: 1.759810447692871, Test Accuracy: 38.80555725097656\n",
      "Epoch 5, Loss: 1.7573832273483276, Accuracy: 39.595237731933594, Test Loss: 1.751391053199768, Test Accuracy: 39.841270446777344\n",
      "Epoch 6, Loss: 1.7502487897872925, Accuracy: 40.126983642578125, Test Loss: 1.7451599836349487, Test Accuracy: 40.33333206176758\n",
      "Epoch 7, Loss: 1.7448266744613647, Accuracy: 40.6944465637207, Test Loss: 1.7405287027359009, Test Accuracy: 40.76984405517578\n",
      "Epoch 8, Loss: 1.7403209209442139, Accuracy: 41.36111068725586, Test Loss: 1.7361092567443848, Test Accuracy: 41.77777862548828\n",
      "Epoch 9, Loss: 1.7358192205429077, Accuracy: 42.19841384887695, Test Loss: 1.731916904449463, Test Accuracy: 42.488094329833984\n",
      "Epoch 10, Loss: 1.7313861846923828, Accuracy: 42.92063522338867, Test Loss: 1.7317852973937988, Test Accuracy: 42.60714340209961\n",
      "Epoch 11, Loss: 1.726963758468628, Accuracy: 43.6944465637207, Test Loss: 1.7314887046813965, Test Accuracy: 42.64682388305664\n",
      "Epoch 12, Loss: 1.7221285104751587, Accuracy: 44.07539749145508, Test Loss: 1.7219932079315186, Test Accuracy: 43.73015594482422\n",
      "Epoch 13, Loss: 1.71710205078125, Accuracy: 44.511905670166016, Test Loss: 1.7189081907272339, Test Accuracy: 44.0079345703125\n",
      "Epoch 14, Loss: 1.7121531963348389, Accuracy: 44.896827697753906, Test Loss: 1.7106488943099976, Test Accuracy: 44.97222137451172\n",
      "Epoch 15, Loss: 1.708267092704773, Accuracy: 45.337303161621094, Test Loss: 1.7117983102798462, Test Accuracy: 44.599205017089844\n",
      "Epoch 16, Loss: 1.7047046422958374, Accuracy: 45.630950927734375, Test Loss: 1.7078826427459717, Test Accuracy: 44.91270065307617\n",
      "Epoch 17, Loss: 1.7023946046829224, Accuracy: 45.63888931274414, Test Loss: 1.7074331045150757, Test Accuracy: 45.07539749145508\n",
      "Epoch 18, Loss: 1.7003675699234009, Accuracy: 45.730159759521484, Test Loss: 1.7025901079177856, Test Accuracy: 45.511905670166016\n",
      "Epoch 19, Loss: 1.6982721090316772, Accuracy: 45.9603157043457, Test Loss: 1.700812816619873, Test Accuracy: 45.56349182128906\n",
      "Epoch 20, Loss: 1.6969971656799316, Accuracy: 46.10714340209961, Test Loss: 1.7003452777862549, Test Accuracy: 45.72222137451172\n",
      "Epoch 21, Loss: 1.695635437965393, Accuracy: 46.19841384887695, Test Loss: 1.6963189840316772, Test Accuracy: 46.154762268066406\n",
      "Epoch 22, Loss: 1.694043517112732, Accuracy: 46.349205017089844, Test Loss: 1.6955690383911133, Test Accuracy: 46.123016357421875\n",
      "Epoch 23, Loss: 1.6930034160614014, Accuracy: 46.51984405517578, Test Loss: 1.6969527006149292, Test Accuracy: 45.94444274902344\n",
      "Epoch 24, Loss: 1.6933488845825195, Accuracy: 46.353172302246094, Test Loss: 1.6922550201416016, Test Accuracy: 46.373016357421875\n",
      "Epoch 25, Loss: 1.6917368173599243, Accuracy: 46.53968048095703, Test Loss: 1.6906651258468628, Test Accuracy: 46.619049072265625\n",
      "Epoch 26, Loss: 1.6907715797424316, Accuracy: 46.67460250854492, Test Loss: 1.693610429763794, Test Accuracy: 46.130950927734375\n",
      "Epoch 27, Loss: 1.6904315948486328, Accuracy: 46.61111068725586, Test Loss: 1.6890652179718018, Test Accuracy: 46.76984405517578\n",
      "Epoch 28, Loss: 1.6892414093017578, Accuracy: 46.8055534362793, Test Loss: 1.6883529424667358, Test Accuracy: 46.900794982910156\n",
      "Epoch 29, Loss: 1.6885534524917603, Accuracy: 46.78174591064453, Test Loss: 1.6867824792861938, Test Accuracy: 46.92063522338867\n",
      "Epoch 30, Loss: 1.6870737075805664, Accuracy: 47.0079345703125, Test Loss: 1.6851418018341064, Test Accuracy: 47.1944465637207\n",
      "Epoch 31, Loss: 1.686785101890564, Accuracy: 47.03968048095703, Test Loss: 1.6847665309906006, Test Accuracy: 47.261905670166016\n",
      "Epoch 32, Loss: 1.6849945783615112, Accuracy: 47.18254089355469, Test Loss: 1.682280421257019, Test Accuracy: 47.519840240478516\n",
      "Epoch 33, Loss: 1.6844520568847656, Accuracy: 47.28571319580078, Test Loss: 1.6833568811416626, Test Accuracy: 47.2976188659668\n",
      "Epoch 34, Loss: 1.6831402778625488, Accuracy: 47.44841003417969, Test Loss: 1.680964469909668, Test Accuracy: 47.55555725097656\n",
      "Epoch 35, Loss: 1.6828128099441528, Accuracy: 47.45634841918945, Test Loss: 1.6810836791992188, Test Accuracy: 47.63888931274414\n",
      "Epoch 36, Loss: 1.6827703714370728, Accuracy: 47.44047546386719, Test Loss: 1.6807126998901367, Test Accuracy: 47.67857360839844\n",
      "Epoch 37, Loss: 1.6811693906784058, Accuracy: 47.5079345703125, Test Loss: 1.6785744428634644, Test Accuracy: 47.88492202758789\n",
      "Epoch 38, Loss: 1.6808249950408936, Accuracy: 47.67460250854492, Test Loss: 1.6784446239471436, Test Accuracy: 47.865081787109375\n",
      "Epoch 39, Loss: 1.6795443296432495, Accuracy: 47.96428680419922, Test Loss: 1.6778450012207031, Test Accuracy: 47.896827697753906\n",
      "Epoch 40, Loss: 1.6784416437149048, Accuracy: 47.916664123535156, Test Loss: 1.6775870323181152, Test Accuracy: 47.88095474243164\n",
      "Epoch 41, Loss: 1.6777069568634033, Accuracy: 48.011905670166016, Test Loss: 1.6780390739440918, Test Accuracy: 47.9603157043457\n",
      "Epoch 42, Loss: 1.6767752170562744, Accuracy: 48.17063522338867, Test Loss: 1.6766325235366821, Test Accuracy: 48.087303161621094\n",
      "Epoch 43, Loss: 1.6759251356124878, Accuracy: 48.28571319580078, Test Loss: 1.6751824617385864, Test Accuracy: 48.17856979370117\n",
      "Epoch 44, Loss: 1.6751347780227661, Accuracy: 48.33333206176758, Test Loss: 1.676954746246338, Test Accuracy: 48.18650817871094\n",
      "Epoch 45, Loss: 1.6760358810424805, Accuracy: 48.162696838378906, Test Loss: 1.6824443340301514, Test Accuracy: 47.53571319580078\n",
      "Epoch 46, Loss: 1.673801064491272, Accuracy: 48.44841384887695, Test Loss: 1.6727268695831299, Test Accuracy: 48.66666793823242\n",
      "Epoch 47, Loss: 1.6735286712646484, Accuracy: 48.47618865966797, Test Loss: 1.6716102361679077, Test Accuracy: 48.761905670166016\n",
      "Epoch 48, Loss: 1.6728525161743164, Accuracy: 48.64285659790039, Test Loss: 1.669338583946228, Test Accuracy: 48.83729934692383\n",
      "Epoch 49, Loss: 1.6709961891174316, Accuracy: 48.730159759521484, Test Loss: 1.6684542894363403, Test Accuracy: 49.03571319580078\n",
      "Epoch 50, Loss: 1.6691144704818726, Accuracy: 49.02777862548828, Test Loss: 1.667751669883728, Test Accuracy: 49.015872955322266\n",
      "WARNING:tensorflow:Layer ml_pbeta_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.8106144666671753, Accuracy: 34.123016357421875, Test Loss: 1.7599928379058838, Test Accuracy: 40.05158996582031\n",
      "Epoch 2, Loss: 1.7598282098770142, Accuracy: 39.69841384887695, Test Loss: 1.7457878589630127, Test Accuracy: 41.07539749145508\n",
      "Epoch 3, Loss: 1.7361443042755127, Accuracy: 41.71825408935547, Test Loss: 1.723809838294983, Test Accuracy: 42.96031951904297\n",
      "Epoch 4, Loss: 1.723128080368042, Accuracy: 42.9920654296875, Test Loss: 1.7214504480361938, Test Accuracy: 42.9920654296875\n",
      "Epoch 5, Loss: 1.701979160308838, Accuracy: 45.40079116821289, Test Loss: 1.696074366569519, Test Accuracy: 45.68650817871094\n",
      "Epoch 6, Loss: 1.6778863668441772, Accuracy: 48.011905670166016, Test Loss: 1.6667160987854004, Test Accuracy: 48.77777862548828\n",
      "Epoch 7, Loss: 1.6584093570709229, Accuracy: 50.325401306152344, Test Loss: 1.6460156440734863, Test Accuracy: 51.908729553222656\n",
      "Epoch 8, Loss: 1.6419837474822998, Accuracy: 52.20634841918945, Test Loss: 1.6237916946411133, Test Accuracy: 54.230159759521484\n",
      "Epoch 9, Loss: 1.6281609535217285, Accuracy: 53.41666793823242, Test Loss: 1.6169564723968506, Test Accuracy: 54.94841384887695\n",
      "Epoch 10, Loss: 1.6153290271759033, Accuracy: 54.833335876464844, Test Loss: 1.6024832725524902, Test Accuracy: 56.376983642578125\n",
      "Epoch 11, Loss: 1.598747968673706, Accuracy: 56.5396842956543, Test Loss: 1.5988396406173706, Test Accuracy: 56.623016357421875\n",
      "Epoch 12, Loss: 1.5867289304733276, Accuracy: 57.726192474365234, Test Loss: 1.5796927213668823, Test Accuracy: 58.52777862548828\n",
      "Epoch 13, Loss: 1.5743615627288818, Accuracy: 59.079368591308594, Test Loss: 1.5663419961929321, Test Accuracy: 59.93254089355469\n",
      "Epoch 14, Loss: 1.5678704977035522, Accuracy: 59.74603271484375, Test Loss: 1.5731830596923828, Test Accuracy: 59.126983642578125\n",
      "Epoch 15, Loss: 1.5626499652862549, Accuracy: 60.269840240478516, Test Loss: 1.556382417678833, Test Accuracy: 60.82142639160156\n",
      "Epoch 16, Loss: 1.5574537515640259, Accuracy: 60.6865119934082, Test Loss: 1.5768511295318604, Test Accuracy: 58.79762268066406\n",
      "Epoch 17, Loss: 1.5511894226074219, Accuracy: 61.32539367675781, Test Loss: 1.5505348443984985, Test Accuracy: 61.357139587402344\n",
      "Epoch 18, Loss: 1.5472139120101929, Accuracy: 61.67460250854492, Test Loss: 1.5443648099899292, Test Accuracy: 62.158729553222656\n",
      "Epoch 19, Loss: 1.5427619218826294, Accuracy: 62.174598693847656, Test Loss: 1.5449715852737427, Test Accuracy: 61.94444274902344\n",
      "Epoch 20, Loss: 1.5404871702194214, Accuracy: 62.30555725097656, Test Loss: 1.538265585899353, Test Accuracy: 62.56745910644531\n",
      "Epoch 21, Loss: 1.5351532697677612, Accuracy: 62.9523811340332, Test Loss: 1.5344092845916748, Test Accuracy: 63.0396842956543\n",
      "Epoch 22, Loss: 1.529921054840088, Accuracy: 63.380950927734375, Test Loss: 1.5251134634017944, Test Accuracy: 63.94841003417969\n",
      "Epoch 23, Loss: 1.5213654041290283, Accuracy: 64.40872955322266, Test Loss: 1.5193523168563843, Test Accuracy: 64.67857360839844\n",
      "Epoch 24, Loss: 1.5023659467697144, Accuracy: 66.21428680419922, Test Loss: 1.4952013492584229, Test Accuracy: 67.15079498291016\n",
      "Epoch 25, Loss: 1.4939393997192383, Accuracy: 67.09127044677734, Test Loss: 1.4871370792388916, Test Accuracy: 67.80952453613281\n",
      "Epoch 26, Loss: 1.488855242729187, Accuracy: 67.5714340209961, Test Loss: 1.4826045036315918, Test Accuracy: 68.28174591064453\n",
      "Epoch 27, Loss: 1.4852603673934937, Accuracy: 67.75396728515625, Test Loss: 1.4808969497680664, Test Accuracy: 68.36904907226562\n",
      "Epoch 28, Loss: 1.473926067352295, Accuracy: 69.0873031616211, Test Loss: 1.4654790163040161, Test Accuracy: 69.95237731933594\n",
      "Epoch 29, Loss: 1.460128903388977, Accuracy: 70.55158996582031, Test Loss: 1.4497368335723877, Test Accuracy: 71.63888549804688\n",
      "Epoch 30, Loss: 1.4554638862609863, Accuracy: 71.06745910644531, Test Loss: 1.4493101835250854, Test Accuracy: 71.70237731933594\n",
      "Epoch 31, Loss: 1.4517199993133545, Accuracy: 71.32539367675781, Test Loss: 1.4504668712615967, Test Accuracy: 71.5079345703125\n",
      "Epoch 32, Loss: 1.4501649141311646, Accuracy: 71.54762268066406, Test Loss: 1.4527907371520996, Test Accuracy: 71.23809814453125\n",
      "Epoch 33, Loss: 1.4464061260223389, Accuracy: 71.96825408935547, Test Loss: 1.4406459331512451, Test Accuracy: 72.38491821289062\n",
      "Epoch 34, Loss: 1.447932243347168, Accuracy: 71.77381134033203, Test Loss: 1.4471936225891113, Test Accuracy: 71.72618865966797\n",
      "Epoch 35, Loss: 1.4409655332565308, Accuracy: 72.4206314086914, Test Loss: 1.4675406217575073, Test Accuracy: 69.64286041259766\n",
      "Epoch 36, Loss: 1.4399372339248657, Accuracy: 72.61111450195312, Test Loss: 1.4390010833740234, Test Accuracy: 72.6190414428711\n",
      "Epoch 37, Loss: 1.436941385269165, Accuracy: 72.83333587646484, Test Loss: 1.4386824369430542, Test Accuracy: 72.6031723022461\n",
      "Epoch 38, Loss: 1.4372307062149048, Accuracy: 72.76983642578125, Test Loss: 1.4346259832382202, Test Accuracy: 73.07142639160156\n",
      "Epoch 39, Loss: 1.4346537590026855, Accuracy: 73.08333587646484, Test Loss: 1.4424530267715454, Test Accuracy: 72.19444274902344\n",
      "Epoch 40, Loss: 1.4336004257202148, Accuracy: 73.15079498291016, Test Loss: 1.430871844291687, Test Accuracy: 73.35713958740234\n",
      "Epoch 41, Loss: 1.4293369054794312, Accuracy: 73.53968048095703, Test Loss: 1.4219452142715454, Test Accuracy: 74.34127044677734\n",
      "Epoch 42, Loss: 1.4285765886306763, Accuracy: 73.59920501708984, Test Loss: 1.4339123964309692, Test Accuracy: 73.14285278320312\n",
      "Epoch 43, Loss: 1.4292441606521606, Accuracy: 73.57539367675781, Test Loss: 1.4232158660888672, Test Accuracy: 74.15872955322266\n",
      "Epoch 44, Loss: 1.4230879545211792, Accuracy: 74.10317993164062, Test Loss: 1.4215351343154907, Test Accuracy: 74.30952453613281\n",
      "Epoch 45, Loss: 1.422686219215393, Accuracy: 74.2738037109375, Test Loss: 1.4267299175262451, Test Accuracy: 73.76587677001953\n",
      "Epoch 46, Loss: 1.422646403312683, Accuracy: 74.18254089355469, Test Loss: 1.4178646802902222, Test Accuracy: 74.71428680419922\n",
      "Epoch 47, Loss: 1.4234716892242432, Accuracy: 74.18254089355469, Test Loss: 1.4234275817871094, Test Accuracy: 74.1388931274414\n",
      "Epoch 48, Loss: 1.4187324047088623, Accuracy: 74.5952377319336, Test Loss: 1.4143733978271484, Test Accuracy: 75.02777862548828\n",
      "Epoch 49, Loss: 1.4169118404388428, Accuracy: 74.79761505126953, Test Loss: 1.4188376665115356, Test Accuracy: 74.63491821289062\n",
      "Epoch 50, Loss: 1.4162571430206299, Accuracy: 74.8888931274414, Test Loss: 1.4322718381881714, Test Accuracy: 73.14285278320312\n",
      "WARNING:tensorflow:Layer cnn_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.791001319885254, Accuracy: 35.238094329833984, Test Loss: 1.696720838546753, Test Accuracy: 46.158729553222656\n",
      "Epoch 2, Loss: 1.6671373844146729, Accuracy: 49.507938385009766, Test Loss: 1.6241735219955444, Test Accuracy: 54.880950927734375\n",
      "Epoch 3, Loss: 1.5793088674545288, Accuracy: 59.00396728515625, Test Loss: 1.5541292428970337, Test Accuracy: 61.492061614990234\n",
      "Epoch 4, Loss: 1.5452460050582886, Accuracy: 62.277774810791016, Test Loss: 1.5358047485351562, Test Accuracy: 63.10317611694336\n",
      "Epoch 5, Loss: 1.5324008464813232, Accuracy: 63.476192474365234, Test Loss: 1.5200005769729614, Test Accuracy: 64.77777862548828\n",
      "Epoch 6, Loss: 1.5200088024139404, Accuracy: 64.57540130615234, Test Loss: 1.5096338987350464, Test Accuracy: 65.58332824707031\n",
      "Epoch 7, Loss: 1.5112199783325195, Accuracy: 65.48809051513672, Test Loss: 1.5074554681777954, Test Accuracy: 65.69444274902344\n",
      "Epoch 8, Loss: 1.5042001008987427, Accuracy: 66.2579345703125, Test Loss: 1.5009099245071411, Test Accuracy: 66.37698364257812\n",
      "Epoch 9, Loss: 1.4997652769088745, Accuracy: 66.6547622680664, Test Loss: 1.4980562925338745, Test Accuracy: 66.55555725097656\n",
      "Epoch 10, Loss: 1.4970791339874268, Accuracy: 66.88492584228516, Test Loss: 1.492707371711731, Test Accuracy: 67.31349182128906\n",
      "Epoch 11, Loss: 1.4941937923431396, Accuracy: 67.20238494873047, Test Loss: 1.4890379905700684, Test Accuracy: 67.64285278320312\n",
      "Epoch 12, Loss: 1.491256594657898, Accuracy: 67.43254089355469, Test Loss: 1.4891446828842163, Test Accuracy: 67.62698364257812\n",
      "Epoch 13, Loss: 1.489545226097107, Accuracy: 67.6111068725586, Test Loss: 1.4852015972137451, Test Accuracy: 67.96031951904297\n",
      "Epoch 14, Loss: 1.4853144884109497, Accuracy: 68.01984405517578, Test Loss: 1.4825572967529297, Test Accuracy: 68.234130859375\n",
      "Epoch 15, Loss: 1.4819436073303223, Accuracy: 68.44047546386719, Test Loss: 1.4824175834655762, Test Accuracy: 68.07539367675781\n",
      "Epoch 16, Loss: 1.4793002605438232, Accuracy: 68.67063903808594, Test Loss: 1.4811073541641235, Test Accuracy: 68.23809814453125\n",
      "Epoch 17, Loss: 1.4766721725463867, Accuracy: 68.81349182128906, Test Loss: 1.4778934717178345, Test Accuracy: 68.62301635742188\n",
      "Epoch 18, Loss: 1.473483681678772, Accuracy: 69.23412322998047, Test Loss: 1.4719656705856323, Test Accuracy: 69.25794219970703\n",
      "Epoch 19, Loss: 1.4709711074829102, Accuracy: 69.46428680419922, Test Loss: 1.4715301990509033, Test Accuracy: 69.3452377319336\n",
      "Epoch 20, Loss: 1.4686293601989746, Accuracy: 69.6547622680664, Test Loss: 1.4689712524414062, Test Accuracy: 69.57936096191406\n",
      "Epoch 21, Loss: 1.466956377029419, Accuracy: 69.9047622680664, Test Loss: 1.4686241149902344, Test Accuracy: 69.57936096191406\n",
      "Epoch 22, Loss: 1.464240312576294, Accuracy: 70.1031723022461, Test Loss: 1.4678183794021606, Test Accuracy: 69.69047546386719\n",
      "Epoch 23, Loss: 1.4633914232254028, Accuracy: 70.20635223388672, Test Loss: 1.4645572900772095, Test Accuracy: 69.8888931274414\n",
      "Epoch 24, Loss: 1.4608080387115479, Accuracy: 70.52777862548828, Test Loss: 1.4621543884277344, Test Accuracy: 70.19047546386719\n",
      "Epoch 25, Loss: 1.461108684539795, Accuracy: 70.41666412353516, Test Loss: 1.4625599384307861, Test Accuracy: 70.18254089355469\n",
      "Epoch 26, Loss: 1.4593178033828735, Accuracy: 70.58332824707031, Test Loss: 1.463617205619812, Test Accuracy: 70.08332824707031\n",
      "Epoch 27, Loss: 1.4582902193069458, Accuracy: 70.7579345703125, Test Loss: 1.4608761072158813, Test Accuracy: 70.31745910644531\n",
      "Epoch 28, Loss: 1.4569512605667114, Accuracy: 70.82142639160156, Test Loss: 1.460378885269165, Test Accuracy: 70.41666412353516\n",
      "Epoch 29, Loss: 1.4564675092697144, Accuracy: 70.85714721679688, Test Loss: 1.4592138528823853, Test Accuracy: 70.515869140625\n",
      "Epoch 30, Loss: 1.4547914266586304, Accuracy: 71.0793685913086, Test Loss: 1.4607616662979126, Test Accuracy: 70.3452377319336\n",
      "Epoch 31, Loss: 1.4524985551834106, Accuracy: 71.3809585571289, Test Loss: 1.4596587419509888, Test Accuracy: 70.54365539550781\n",
      "Epoch 32, Loss: 1.4528121948242188, Accuracy: 71.265869140625, Test Loss: 1.4590964317321777, Test Accuracy: 70.46031951904297\n",
      "Epoch 33, Loss: 1.452122449874878, Accuracy: 71.3452377319336, Test Loss: 1.4614468812942505, Test Accuracy: 70.21428680419922\n",
      "Epoch 34, Loss: 1.4505311250686646, Accuracy: 71.46031951904297, Test Loss: 1.4607915878295898, Test Accuracy: 70.33332824707031\n",
      "Epoch 35, Loss: 1.4504278898239136, Accuracy: 71.45635223388672, Test Loss: 1.4594743251800537, Test Accuracy: 70.46428680419922\n",
      "Epoch 36, Loss: 1.4497265815734863, Accuracy: 71.4920654296875, Test Loss: 1.4543615579605103, Test Accuracy: 70.98809814453125\n",
      "Epoch 37, Loss: 1.4477756023406982, Accuracy: 71.73809051513672, Test Loss: 1.456303358078003, Test Accuracy: 70.86111450195312\n",
      "Epoch 38, Loss: 1.4481180906295776, Accuracy: 71.67857360839844, Test Loss: 1.4556083679199219, Test Accuracy: 70.73412322998047\n",
      "Epoch 39, Loss: 1.449233055114746, Accuracy: 71.5793685913086, Test Loss: 1.4561346769332886, Test Accuracy: 70.81349182128906\n",
      "Epoch 40, Loss: 1.4461913108825684, Accuracy: 71.8293685913086, Test Loss: 1.4543380737304688, Test Accuracy: 70.9920654296875\n",
      "Epoch 41, Loss: 1.445286512374878, Accuracy: 72.03571319580078, Test Loss: 1.453332781791687, Test Accuracy: 71.1031723022461\n",
      "Epoch 42, Loss: 1.445489764213562, Accuracy: 71.8968276977539, Test Loss: 1.4550800323486328, Test Accuracy: 70.82142639160156\n",
      "Epoch 43, Loss: 1.4441030025482178, Accuracy: 72.05158233642578, Test Loss: 1.4524747133255005, Test Accuracy: 71.18254089355469\n",
      "Epoch 44, Loss: 1.4282811880111694, Accuracy: 73.5952377319336, Test Loss: 1.4124435186386108, Test Accuracy: 75.1626968383789\n",
      "Epoch 45, Loss: 1.4098107814788818, Accuracy: 75.50794219970703, Test Loss: 1.3973121643066406, Test Accuracy: 76.86111450195312\n",
      "Epoch 46, Loss: 1.3997589349746704, Accuracy: 76.48809814453125, Test Loss: 1.3983080387115479, Test Accuracy: 76.75396728515625\n",
      "Epoch 47, Loss: 1.3933584690093994, Accuracy: 77.29365539550781, Test Loss: 1.392958164215088, Test Accuracy: 77.29365539550781\n",
      "Epoch 48, Loss: 1.3895870447158813, Accuracy: 77.54762268066406, Test Loss: 1.3921153545379639, Test Accuracy: 77.3452377319336\n",
      "Epoch 49, Loss: 1.3858445882797241, Accuracy: 78.01190948486328, Test Loss: 1.3824772834777832, Test Accuracy: 78.38888549804688\n",
      "Epoch 50, Loss: 1.3822368383407593, Accuracy: 78.33333587646484, Test Loss: 1.3835490942001343, Test Accuracy: 78.16667175292969\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "for model in models:\n",
    "  # Train MLP\n",
    "  for epoch in range(EPOCHS):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    if 'mlp' in model:\n",
    "      for x, y in trainmlp_ds:\n",
    "        if model == 'mlp':\n",
    "          train_mlp(x, y)\n",
    "        elif model == 'mlpbeta':\n",
    "          train_mlpbeta(x,y)\n",
    "    else:\n",
    "      for x, y in traincnn_ds:\n",
    "        train_cnn(x,y)\n",
    "\n",
    "    if 'mlp' in model:\n",
    "      for x_test, y_test in testmlp_ds:\n",
    "        if model == 'mlp':\n",
    "          test_mlp(x_test, y_test)\n",
    "        elif model == 'mlpbeta':\n",
    "          test_mlpbeta(x_test, y_test)\n",
    "    else:\n",
    "      for x_test, y_test in testcnn_ds:\n",
    "        test_cnn(x_test, y_test)\n",
    "\n",
    "    print(\n",
    "      f'Epoch {epoch + 1}, '\n",
    "      f'Loss: {train_loss.result()}, '\n",
    "      f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "      f'Test Loss: {test_loss.result()}, '\n",
    "      f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
