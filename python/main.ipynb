{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import os\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "## Training \n",
    "sub = 2\n",
    "train_grp = 2\n",
    "n_train = 'fullallmix4'\n",
    "train_scale = 5\n",
    "cv_type = 'manual'\n",
    "scaler_load = False\n",
    "feat_type = 'mav'\n",
    "\n",
    "ind = (params[:,0] == sub) & (params[:,3] == train_grp)\n",
    "\n",
    "x_train, x_test, x_valid, p_train, p_test, p_valid = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,load=True,train_grp=train_grp)\n",
    "\n",
    "emg_scale = np.ones((np.size(x_train,1),1))\n",
    "for i in range(np.size(x_train,1)):\n",
    "    emg_scale[i] = 5/np.max(np.abs(x_train[:,i,:]))\n",
    "x_train = x_train*emg_scale\n",
    "x_valid = x_valid*emg_scale\n",
    "\n",
    "x_train_noise, x_train_clean, y_train_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "x_valid_noise, x_valid_clean, y_valid_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "\n",
    "# shuffle data to make even batches\n",
    "x_train_noise, x_train_clean, y_train_clean = shuffle(x_train_noise, x_train_clean, y_train_clean, random_state = 0)\n",
    "\n",
    "# Extract features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "x_train_noise_cnn, scaler = prd.extract_scale(x_train_noise,scaler,scaler_load,ft=feat_type,emg_scale=emg_scale) \n",
    "x_train_clean_cnn, _ = prd.extract_scale(x_train_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_noise_cnn, _ = prd.extract_scale(x_valid_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_clean_cnn, _ = prd.extract_scale(x_valid_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_train_noise_cnn = x_train_noise_cnn.astype('float32')\n",
    "x_valid_noise_cnn = x_valid_noise_cnn.astype('float32')\n",
    "\n",
    "# reshape data for nonconvolutional network\n",
    "x_train_noise_mlp = x_train_noise_cnn.reshape(x_train_noise_cnn.shape[0],-1)\n",
    "x_valid_noise_mlp = x_valid_noise_cnn.reshape(x_valid_noise_cnn.shape[0],-1)\n",
    "\n",
    "\n",
    "# create batches\n",
    "trainmlp_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_mlp, y_train_clean)).batch(128)\n",
    "testmlp_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_mlp, y_valid_clean)).batch(128)\n",
    "traincnn_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_cnn, y_train_clean)).batch(128)\n",
    "testcnn_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_cnn, y_valid_clean)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "## TRAIN TEST MLP\n",
    "@tf.function\n",
    "def train_mlp(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = mlp(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, mlp.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlp(x, y):\n",
    "    y_out = mlp(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)\n",
    "\n",
    "## TRAIN TEST MLP BETA\n",
    "@tf.function\n",
    "def train_mlpbeta(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = mlp_beta(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, mlp_beta.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp_beta.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlpbeta(x, y):\n",
    "    y_out = mlp_beta(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)\n",
    "\n",
    "## TRAIN TEST CNN\n",
    "@tf.function\n",
    "def train_cnn(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = cnn(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, cnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_cnn(x, y):\n",
    "    y_out = cnn(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp\n",
      "Epoch 1, Loss: 1.937026023864746, Accuracy: 20.428571701049805, Test Loss: 1.9195363521575928, Test Accuracy: 24.103174209594727\n",
      "Epoch 2, Loss: 1.8813036680221558, Accuracy: 29.0079345703125, Test Loss: 1.8396662473678589, Test Accuracy: 32.666664123535156\n",
      "Epoch 3, Loss: 1.8149486780166626, Accuracy: 35.42857360839844, Test Loss: 1.7933012247085571, Test Accuracy: 37.515872955322266\n",
      "Epoch 4, Loss: 1.7852373123168945, Accuracy: 37.5079345703125, Test Loss: 1.7749580144882202, Test Accuracy: 38.31349182128906\n",
      "Epoch 5, Loss: 1.7723084688186646, Accuracy: 38.55952453613281, Test Loss: 1.7653300762176514, Test Accuracy: 39.32936477661133\n",
      "Epoch 6, Loss: 1.7646064758300781, Accuracy: 39.408729553222656, Test Loss: 1.7590690851211548, Test Accuracy: 39.988094329833984\n",
      "Epoch 7, Loss: 1.7587984800338745, Accuracy: 40.19047546386719, Test Loss: 1.7538120746612549, Test Accuracy: 40.72222137451172\n",
      "Epoch 8, Loss: 1.7515652179718018, Accuracy: 41.18253707885742, Test Loss: 1.7453725337982178, Test Accuracy: 42.11111068725586\n",
      "Epoch 9, Loss: 1.7419967651367188, Accuracy: 42.591270446777344, Test Loss: 1.7367151975631714, Test Accuracy: 42.92063522338867\n",
      "Epoch 10, Loss: 1.7315617799758911, Accuracy: 43.654762268066406, Test Loss: 1.726209282875061, Test Accuracy: 44.23809814453125\n",
      "Epoch 11, Loss: 1.7211867570877075, Accuracy: 44.865081787109375, Test Loss: 1.7161115407943726, Test Accuracy: 45.13095474243164\n",
      "Epoch 12, Loss: 1.710321307182312, Accuracy: 45.9920654296875, Test Loss: 1.7037734985351562, Test Accuracy: 46.60714340209961\n",
      "Epoch 13, Loss: 1.7014399766921997, Accuracy: 46.845237731933594, Test Loss: 1.6963670253753662, Test Accuracy: 47.515872955322266\n",
      "Epoch 14, Loss: 1.6956017017364502, Accuracy: 47.373016357421875, Test Loss: 1.6915401220321655, Test Accuracy: 47.88492202758789\n",
      "Epoch 15, Loss: 1.6915706396102905, Accuracy: 47.742061614990234, Test Loss: 1.6879655122756958, Test Accuracy: 48.14285659790039\n",
      "Epoch 16, Loss: 1.6876115798950195, Accuracy: 47.984127044677734, Test Loss: 1.684297800064087, Test Accuracy: 48.22618865966797\n",
      "Epoch 17, Loss: 1.6831976175308228, Accuracy: 48.242061614990234, Test Loss: 1.680525302886963, Test Accuracy: 48.5396842956543\n",
      "Epoch 18, Loss: 1.6785675287246704, Accuracy: 48.68254089355469, Test Loss: 1.6767559051513672, Test Accuracy: 48.876983642578125\n",
      "Epoch 19, Loss: 1.6737951040267944, Accuracy: 49.11507797241211, Test Loss: 1.67452073097229, Test Accuracy: 49.0476188659668\n",
      "Epoch 20, Loss: 1.670373797416687, Accuracy: 49.39682388305664, Test Loss: 1.6742544174194336, Test Accuracy: 49.0396842956543\n",
      "Epoch 21, Loss: 1.6674097776412964, Accuracy: 49.757938385009766, Test Loss: 1.6691763401031494, Test Accuracy: 49.58729934692383\n",
      "Epoch 22, Loss: 1.6643595695495605, Accuracy: 50.07936477661133, Test Loss: 1.6664626598358154, Test Accuracy: 49.82142639160156\n",
      "Epoch 23, Loss: 1.6619324684143066, Accuracy: 50.43254089355469, Test Loss: 1.6652016639709473, Test Accuracy: 50.0476188659668\n",
      "Epoch 24, Loss: 1.6599730253219604, Accuracy: 50.57936477661133, Test Loss: 1.6624904870986938, Test Accuracy: 50.2896842956543\n",
      "Epoch 25, Loss: 1.658198356628418, Accuracy: 50.69047927856445, Test Loss: 1.6631298065185547, Test Accuracy: 50.09127426147461\n",
      "Epoch 26, Loss: 1.6573184728622437, Accuracy: 50.77381134033203, Test Loss: 1.656794786453247, Test Accuracy: 50.722225189208984\n",
      "Epoch 27, Loss: 1.6557451486587524, Accuracy: 50.94841003417969, Test Loss: 1.6607811450958252, Test Accuracy: 50.19444274902344\n",
      "Epoch 28, Loss: 1.6548876762390137, Accuracy: 50.904762268066406, Test Loss: 1.6569582223892212, Test Accuracy: 50.65873336791992\n",
      "Epoch 29, Loss: 1.6540557146072388, Accuracy: 50.96825408935547, Test Loss: 1.6536767482757568, Test Accuracy: 50.976192474365234\n",
      "Epoch 30, Loss: 1.6529303789138794, Accuracy: 51.12698745727539, Test Loss: 1.6536643505096436, Test Accuracy: 50.984127044677734\n",
      "Training mlpbeta\n",
      "Epoch 1, Loss: 1.8700534105300903, Accuracy: 26.547618865966797, Test Loss: 1.798456072807312, Test Accuracy: 36.28571319580078\n",
      "Epoch 2, Loss: 1.7771658897399902, Accuracy: 37.43254089355469, Test Loss: 1.7643622159957886, Test Accuracy: 38.2579345703125\n",
      "Epoch 3, Loss: 1.73750638961792, Accuracy: 41.2976188659668, Test Loss: 1.7174339294433594, Test Accuracy: 43.89285659790039\n",
      "Epoch 4, Loss: 1.7024543285369873, Accuracy: 45.61111068725586, Test Loss: 1.6959534883499146, Test Accuracy: 46.55158996582031\n",
      "Epoch 5, Loss: 1.6788408756256104, Accuracy: 48.91666793823242, Test Loss: 1.6751554012298584, Test Accuracy: 49.30952453613281\n",
      "Epoch 6, Loss: 1.6553901433944702, Accuracy: 51.27777862548828, Test Loss: 1.6480519771575928, Test Accuracy: 51.71031951904297\n",
      "Epoch 7, Loss: 1.6438041925430298, Accuracy: 51.94841003417969, Test Loss: 1.6334552764892578, Test Accuracy: 53.333335876464844\n",
      "Epoch 8, Loss: 1.635964035987854, Accuracy: 52.9682502746582, Test Loss: 1.6282545328140259, Test Accuracy: 53.66270065307617\n",
      "Epoch 9, Loss: 1.6303074359893799, Accuracy: 53.492061614990234, Test Loss: 1.6255782842636108, Test Accuracy: 53.865081787109375\n",
      "Epoch 10, Loss: 1.625595211982727, Accuracy: 53.94047546386719, Test Loss: 1.6247202157974243, Test Accuracy: 53.9603157043457\n",
      "Epoch 11, Loss: 1.6202902793884277, Accuracy: 54.37301254272461, Test Loss: 1.617204189300537, Test Accuracy: 54.68254089355469\n",
      "Epoch 12, Loss: 1.6152338981628418, Accuracy: 54.92856979370117, Test Loss: 1.6145811080932617, Test Accuracy: 54.81349182128906\n",
      "Epoch 13, Loss: 1.6120632886886597, Accuracy: 55.242061614990234, Test Loss: 1.6090937852859497, Test Accuracy: 55.35317611694336\n",
      "Epoch 14, Loss: 1.6097227334976196, Accuracy: 55.38492202758789, Test Loss: 1.6060523986816406, Test Accuracy: 55.876983642578125\n",
      "Epoch 15, Loss: 1.6055066585540771, Accuracy: 55.849205017089844, Test Loss: 1.6019256114959717, Test Accuracy: 56.19047927856445\n",
      "Epoch 16, Loss: 1.601652979850769, Accuracy: 56.35317611694336, Test Loss: 1.5979790687561035, Test Accuracy: 56.66666793823242\n",
      "Epoch 17, Loss: 1.6000481843948364, Accuracy: 56.32936477661133, Test Loss: 1.6053892374038696, Test Accuracy: 55.76984405517578\n",
      "Epoch 18, Loss: 1.596802830696106, Accuracy: 56.75, Test Loss: 1.6026637554168701, Test Accuracy: 56.015872955322266\n",
      "Epoch 19, Loss: 1.5945497751235962, Accuracy: 57.027774810791016, Test Loss: 1.6000900268554688, Test Accuracy: 56.380950927734375\n",
      "Epoch 20, Loss: 1.5922449827194214, Accuracy: 57.22222137451172, Test Loss: 1.5886034965515137, Test Accuracy: 57.5634880065918\n",
      "Epoch 21, Loss: 1.5900015830993652, Accuracy: 57.4523811340332, Test Loss: 1.5930358171463013, Test Accuracy: 57.08333206176758\n",
      "Epoch 22, Loss: 1.588555932044983, Accuracy: 57.734127044677734, Test Loss: 1.5985023975372314, Test Accuracy: 56.38492202758789\n",
      "Epoch 23, Loss: 1.587071180343628, Accuracy: 57.77381134033203, Test Loss: 1.6003749370574951, Test Accuracy: 56.36507797241211\n",
      "Epoch 24, Loss: 1.5865070819854736, Accuracy: 57.77381134033203, Test Loss: 1.584886074066162, Test Accuracy: 57.99603271484375\n",
      "Epoch 25, Loss: 1.5840624570846558, Accuracy: 57.976192474365234, Test Loss: 1.5892118215560913, Test Accuracy: 57.373016357421875\n",
      "Epoch 26, Loss: 1.5831588506698608, Accuracy: 58.17063522338867, Test Loss: 1.5880146026611328, Test Accuracy: 57.58333206176758\n",
      "Epoch 27, Loss: 1.5807151794433594, Accuracy: 58.33333206176758, Test Loss: 1.5868929624557495, Test Accuracy: 57.7023811340332\n",
      "Epoch 28, Loss: 1.579695224761963, Accuracy: 58.54762268066406, Test Loss: 1.5799528360366821, Test Accuracy: 58.42063522338867\n",
      "Epoch 29, Loss: 1.5791599750518799, Accuracy: 58.611114501953125, Test Loss: 1.5854772329330444, Test Accuracy: 57.78571319580078\n",
      "Epoch 30, Loss: 1.5798965692520142, Accuracy: 58.396827697753906, Test Loss: 1.5787217617034912, Test Accuracy: 58.33729934692383\n",
      "Training cnn\n",
      "Epoch 1, Loss: 1.8449385166168213, Accuracy: 30.218252182006836, Test Loss: 1.7248872518539429, Test Accuracy: 44.22222137451172\n",
      "Epoch 2, Loss: 1.6765788793563843, Accuracy: 49.25396728515625, Test Loss: 1.6330209970474243, Test Accuracy: 53.57142639160156\n",
      "Epoch 3, Loss: 1.608142375946045, Accuracy: 56.10317611694336, Test Loss: 1.5859026908874512, Test Accuracy: 58.22222137451172\n",
      "Epoch 4, Loss: 1.5734251737594604, Accuracy: 59.476192474365234, Test Loss: 1.5773833990097046, Test Accuracy: 58.626983642578125\n",
      "Epoch 5, Loss: 1.5604954957962036, Accuracy: 60.523807525634766, Test Loss: 1.5549620389938354, Test Accuracy: 61.115081787109375\n",
      "Epoch 6, Loss: 1.5469845533370972, Accuracy: 61.97618865966797, Test Loss: 1.5423790216445923, Test Accuracy: 62.400794982910156\n",
      "Epoch 7, Loss: 1.5389498472213745, Accuracy: 62.777774810791016, Test Loss: 1.5343451499938965, Test Accuracy: 63.32143020629883\n",
      "Epoch 8, Loss: 1.5316580533981323, Accuracy: 63.40873336791992, Test Loss: 1.529053807258606, Test Accuracy: 63.89682388305664\n",
      "Epoch 9, Loss: 1.5254383087158203, Accuracy: 64.04761505126953, Test Loss: 1.5201255083084106, Test Accuracy: 64.84127044677734\n",
      "Epoch 10, Loss: 1.5216948986053467, Accuracy: 64.5079345703125, Test Loss: 1.5165494680404663, Test Accuracy: 65.02381134033203\n",
      "Epoch 11, Loss: 1.5166678428649902, Accuracy: 64.91270446777344, Test Loss: 1.5112454891204834, Test Accuracy: 65.57142639160156\n",
      "Epoch 12, Loss: 1.5122603178024292, Accuracy: 65.40872955322266, Test Loss: 1.509411334991455, Test Accuracy: 65.67857360839844\n",
      "Epoch 13, Loss: 1.5095083713531494, Accuracy: 65.61111450195312, Test Loss: 1.5061039924621582, Test Accuracy: 65.96031951904297\n",
      "Epoch 14, Loss: 1.506710171699524, Accuracy: 65.91667175292969, Test Loss: 1.5042269229888916, Test Accuracy: 66.24603271484375\n",
      "Epoch 15, Loss: 1.5057885646820068, Accuracy: 65.98016357421875, Test Loss: 1.5005664825439453, Test Accuracy: 66.55555725097656\n",
      "Epoch 16, Loss: 1.5048022270202637, Accuracy: 66.08333587646484, Test Loss: 1.4982013702392578, Test Accuracy: 66.82539367675781\n",
      "Epoch 17, Loss: 1.5021312236785889, Accuracy: 66.2579345703125, Test Loss: 1.496567726135254, Test Accuracy: 66.9206314086914\n",
      "Epoch 18, Loss: 1.4996249675750732, Accuracy: 66.55952453613281, Test Loss: 1.4953804016113281, Test Accuracy: 66.97618865966797\n",
      "Epoch 19, Loss: 1.4971022605895996, Accuracy: 66.8293685913086, Test Loss: 1.4932929277420044, Test Accuracy: 67.22618865966797\n",
      "Epoch 20, Loss: 1.496117115020752, Accuracy: 66.9047622680664, Test Loss: 1.4932944774627686, Test Accuracy: 67.25\n",
      "Epoch 21, Loss: 1.4953407049179077, Accuracy: 67.01190185546875, Test Loss: 1.4920415878295898, Test Accuracy: 67.38888549804688\n",
      "Epoch 22, Loss: 1.4929323196411133, Accuracy: 67.28571319580078, Test Loss: 1.4914578199386597, Test Accuracy: 67.44047546386719\n",
      "Epoch 23, Loss: 1.4929255247116089, Accuracy: 67.22222137451172, Test Loss: 1.4922375679016113, Test Accuracy: 67.3293685913086\n",
      "Epoch 24, Loss: 1.4926632642745972, Accuracy: 67.30952453613281, Test Loss: 1.4906517267227173, Test Accuracy: 67.35317993164062\n",
      "Epoch 25, Loss: 1.4911447763442993, Accuracy: 67.35713958740234, Test Loss: 1.4894399642944336, Test Accuracy: 67.5952377319336\n",
      "Epoch 26, Loss: 1.4887990951538086, Accuracy: 67.67857360839844, Test Loss: 1.487191915512085, Test Accuracy: 67.78571319580078\n",
      "Epoch 27, Loss: 1.4885060787200928, Accuracy: 67.72618865966797, Test Loss: 1.4895268678665161, Test Accuracy: 67.53174591064453\n",
      "Epoch 28, Loss: 1.4868196249008179, Accuracy: 67.90872955322266, Test Loss: 1.4868171215057373, Test Accuracy: 67.8373031616211\n",
      "Epoch 29, Loss: 1.4881576299667358, Accuracy: 67.58333587646484, Test Loss: 1.493582844734192, Test Accuracy: 66.97222137451172\n",
      "Epoch 30, Loss: 1.4843356609344482, Accuracy: 68.01984405517578, Test Loss: 1.486638069152832, Test Accuracy: 67.77777862548828\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "for model in models:\n",
    "    print('Training ' + model)\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        # Train MLP\n",
    "        if model == 'mlp':\n",
    "            for x, y in trainmlp_ds:\n",
    "                train_mlp(x, y)\n",
    "            for x_test, y_test in testmlp_ds:\n",
    "                test_mlp(x_test, y_test)\n",
    "        # Train MLP Beta\n",
    "        elif model == 'mlpbeta':\n",
    "            for x, y in trainmlp_ds:\n",
    "                train_mlpbeta(x, y)\n",
    "            for x_test, y_test in testmlp_ds:\n",
    "                test_mlpbeta(x_test, y_test)\n",
    "        # Train CNN\n",
    "        elif model == 'cnn':\n",
    "            for x, y in traincnn_ds:\n",
    "                train_cnn(x,y)\n",
    "            for x_test, y_test in testcnn_ds:\n",
    "                test_cnn(x_test, y_test)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Loss: {test_loss.result()}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_enc = mlp.get_layer(name='enc')\n",
    "mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "cnn_enc = cnn.get_layer(name='enc')\n",
    "\n",
    "mlp_aligned = mlp_enc(x_train_noise_mlp).numpy()\n",
    "mlp_beta_aligned = mlpbeta_enc(x_train_noise_mlp).numpy()\n",
    "cnn_aligned = cnn_enc(x_train_noise_cnn).numpy()\n",
    "\n",
    "y_train_aligned = np.argmax(y_train_clean, axis=1)[...,np.newaxis]\n",
    "w_mlp, c_mlp,_, _, _ = train_lda(mlp_aligned,y_train_aligned)\n",
    "w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(mlp_beta_aligned,y_train_aligned)\n",
    "w_cnn, c_cnn,_, _, _ = train_lda(cnn_aligned,y_train_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_4.p\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer mlp_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Clean: MLP Accuracy: 81.48571014404297,MLP-LDA Accuracy: 79.37142857142857\n",
      "Noisy: MLP Accuracy: 57.48571014404297,MLP-LDA Accuracy: 53.88571428571428\n",
      "WARNING:tensorflow:Layer ml_pbeta_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Clean: MLP-Beta Accuracy: 84.14286041259766,MLP-Beta-LDA Accuracy: 80.02857142857142\n",
      "Noisy: MLP-Beta Accuracy: 63.25714111328125,MLP-Beta-LDA Accuracy: 63.6\n",
      "WARNING:tensorflow:Layer cnn_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Clean: CNN Accuracy: 82.74285888671875,CNN-LDA Accuracy: 86.45714285714286\n",
      "Noisy: CNN Accuracy: 66.4857177734375,CNN-LDA Accuracy: 64.54285714285714\n"
     ]
    }
   ],
   "source": [
    "test_grp = 4\n",
    "cv_type = 'manual'\n",
    "n_test = 'partposrealmixeven24'\n",
    "\n",
    "with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "    real_noise_temp, _ = pickle.load(f)\n",
    "\n",
    "_, x_test, _, _, p_test, _ = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,train_grp=test_grp)\n",
    "clean_size = int(np.size(x_test,axis=0))\n",
    "x_test = x_test*emg_scale\n",
    "\n",
    "x_test_noise, x_test_clean, y_test_clean = prd.add_noise(x_test, p_test, sub, n_test, 1, real_noise=real_noise_temp, emg_scale = emg_scale)\n",
    "x_test_cnn, _ = prd.extract_scale(x_test_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_test_cnn = x_test_cnn.astype('float32')\n",
    "x_test_mlp = x_test_cnn.reshape(x_test_cnn.shape[0],-1)\n",
    "\n",
    "mlp_test_aligned = mlp_enc(x_test_mlp).numpy()\n",
    "mlpbeta_test_aligned = mlpbeta_enc(x_test_mlp).numpy()\n",
    "cnn_test_aligned = cnn_enc(x_test_cnn).numpy()\n",
    "y_test_aligned = np.argmax(y_test_clean, axis=1)[...,np.newaxis]\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlp(x_test_mlp[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_mlp,c_mlp,mlp_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'MLP Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlp(x_test_mlp[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_mlp,c_mlp,mlp_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'MLP Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlpbeta(x_test_mlp[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'MLP-Beta Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-Beta-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlpbeta(x_test_mlp[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'MLP-Beta Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-Beta-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_cnn(x_test_cnn[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_cnn,c_cnn,cnn_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'CNN Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'CNN-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_cnn(x_test_cnn[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_cnn,c_cnn,cnn_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'CNN Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'CNN-LDA Accuracy: {acc * 100}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
