{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import os\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "## Training \n",
    "sub = 2\n",
    "train_grp = 2\n",
    "n_train = 'fullallmix4'\n",
    "train_scale = 5\n",
    "cv_type = 'manual'\n",
    "scaler_load = False\n",
    "feat_type = 'feat'\n",
    "\n",
    "ind = (params[:,0] == sub) & (params[:,3] == train_grp)\n",
    "\n",
    "x_train, x_test, x_valid, p_train, p_test, p_valid = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,load=True,train_grp=train_grp)\n",
    "\n",
    "emg_scale = np.ones((np.size(x_train,1),1))\n",
    "for i in range(np.size(x_train,1)):\n",
    "    emg_scale[i] = 5/np.max(np.abs(x_train[:,i,:]))\n",
    "x_train = x_train*emg_scale\n",
    "x_valid = x_valid*emg_scale\n",
    "\n",
    "x_train_noise, x_train_clean, y_train_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "x_valid_noise, x_valid_clean, y_valid_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "\n",
    "# shuffle data to make even batches\n",
    "x_train_noise, x_train_clean, y_train_clean = shuffle(x_train_noise, x_train_clean, y_train_clean, random_state = 0)\n",
    "\n",
    "# Extract features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "x_train_noise_cnn, scaler = prd.extract_scale(x_train_noise,scaler,scaler_load,ft=feat_type,emg_scale=emg_scale) \n",
    "x_train_clean_cnn, _ = prd.extract_scale(x_train_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_noise_cnn, _ = prd.extract_scale(x_valid_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_valid_clean_cnn, _ = prd.extract_scale(x_valid_clean,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "\n",
    "# reshape data for nonconvolutional network\n",
    "x_train_noise_mlp = x_train_noise_cnn.reshape(x_train_noise_cnn.shape[0],-1)\n",
    "x_valid_noise_mlp = x_valid_noise_cnn.reshape(x_valid_noise_cnn.shape[0],-1)\n",
    "\n",
    "# create batches\n",
    "trainmlp_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_mlp.astype('float32'), y_train_clean)).batch(128)\n",
    "testmlp_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_mlp.astype('float32'), y_valid_clean)).batch(128)\n",
    "traincnn_ds = tf.data.Dataset.from_tensor_slices((x_train_noise_cnn.astype('float32'), y_train_clean)).batch(128)\n",
    "testcnn_ds = tf.data.Dataset.from_tensor_slices((x_valid_noise_cnn.astype('float32'), y_valid_clean)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "## TRAIN TEST MLP\n",
    "@tf.function\n",
    "def train_mlp(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = mlp(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, mlp.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlp(x, y):\n",
    "    y_out = mlp(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)\n",
    "\n",
    "## TRAIN TEST MLP BETA\n",
    "@tf.function\n",
    "def train_mlpbeta(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = mlp_beta(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, mlp_beta.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, mlp_beta.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_mlpbeta(x, y):\n",
    "    y_out = mlp_beta(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)\n",
    "\n",
    "## TRAIN TEST CNN\n",
    "@tf.function\n",
    "def train_cnn(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_out = cnn(x)\n",
    "        loss = loss_fn(y, y_out)\n",
    "    gradients = tape.gradient(loss, cnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, cnn.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(y, y_out)\n",
    "\n",
    "@tf.function\n",
    "def test_cnn(x, y):\n",
    "    y_out = cnn(x)\n",
    "    t_loss = loss_fn(y, y_out)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(y, y_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp\n",
      "Epoch 1, Loss: 1.9352484941482544, Accuracy: 16.349206924438477, Test Loss: 1.8923747539520264, Test Accuracy: 23.293649673461914\n",
      "Epoch 2, Loss: 1.8609070777893066, Accuracy: 26.66269874572754, Test Loss: 1.8407220840454102, Test Accuracy: 31.071428298950195\n",
      "Epoch 3, Loss: 1.826223611831665, Accuracy: 31.992063522338867, Test Loss: 1.8161064386367798, Test Accuracy: 32.94047546386719\n",
      "Epoch 4, Loss: 1.803479552268982, Accuracy: 34.53571319580078, Test Loss: 1.785685420036316, Test Accuracy: 36.57936477661133\n",
      "Epoch 5, Loss: 1.7748278379440308, Accuracy: 37.869049072265625, Test Loss: 1.760231614112854, Test Accuracy: 39.68650817871094\n",
      "Epoch 6, Loss: 1.7518045902252197, Accuracy: 40.38888931274414, Test Loss: 1.7366530895233154, Test Accuracy: 42.337303161621094\n",
      "Epoch 7, Loss: 1.7298963069915771, Accuracy: 43.353172302246094, Test Loss: 1.7167818546295166, Test Accuracy: 45.10714340209961\n",
      "Epoch 8, Loss: 1.7090566158294678, Accuracy: 45.7103157043457, Test Loss: 1.6995830535888672, Test Accuracy: 47.44047546386719\n",
      "Epoch 9, Loss: 1.6935795545578003, Accuracy: 47.36111068725586, Test Loss: 1.6882175207138062, Test Accuracy: 48.583335876464844\n",
      "Epoch 10, Loss: 1.6828640699386597, Accuracy: 48.47222137451172, Test Loss: 1.68007230758667, Test Accuracy: 49.007938385009766\n",
      "Epoch 11, Loss: 1.672350287437439, Accuracy: 49.373016357421875, Test Loss: 1.6674002408981323, Test Accuracy: 50.099205017089844\n",
      "Epoch 12, Loss: 1.6636265516281128, Accuracy: 50.17460632324219, Test Loss: 1.6627237796783447, Test Accuracy: 50.48015594482422\n",
      "Epoch 13, Loss: 1.6586910486221313, Accuracy: 50.63492202758789, Test Loss: 1.6628589630126953, Test Accuracy: 50.26190185546875\n",
      "Epoch 14, Loss: 1.6519091129302979, Accuracy: 51.369049072265625, Test Loss: 1.6581981182098389, Test Accuracy: 50.81349182128906\n",
      "Epoch 15, Loss: 1.6459672451019287, Accuracy: 51.734127044677734, Test Loss: 1.645860195159912, Test Accuracy: 52.07142639160156\n",
      "Epoch 16, Loss: 1.642275094985962, Accuracy: 52.04365158081055, Test Loss: 1.6478904485702515, Test Accuracy: 51.507938385009766\n",
      "Epoch 17, Loss: 1.6378614902496338, Accuracy: 52.603172302246094, Test Loss: 1.6406279802322388, Test Accuracy: 52.33729934692383\n",
      "Epoch 18, Loss: 1.6329675912857056, Accuracy: 53.13888931274414, Test Loss: 1.633649468421936, Test Accuracy: 53.123016357421875\n",
      "Epoch 19, Loss: 1.6307109594345093, Accuracy: 53.26587677001953, Test Loss: 1.6285234689712524, Test Accuracy: 53.54762268066406\n",
      "Epoch 20, Loss: 1.6281120777130127, Accuracy: 53.591270446777344, Test Loss: 1.629366159439087, Test Accuracy: 53.45237731933594\n",
      "Epoch 21, Loss: 1.624955654144287, Accuracy: 53.70634841918945, Test Loss: 1.6168534755706787, Test Accuracy: 54.69841384887695\n",
      "Epoch 22, Loss: 1.6219439506530762, Accuracy: 53.96428680419922, Test Loss: 1.619052529335022, Test Accuracy: 54.32539749145508\n",
      "Epoch 23, Loss: 1.6179511547088623, Accuracy: 54.333335876464844, Test Loss: 1.617083191871643, Test Accuracy: 54.62301254272461\n",
      "Epoch 24, Loss: 1.6151392459869385, Accuracy: 54.71825408935547, Test Loss: 1.6138584613800049, Test Accuracy: 54.84920883178711\n",
      "Epoch 25, Loss: 1.6117151975631714, Accuracy: 55.10317611694336, Test Loss: 1.609836220741272, Test Accuracy: 55.222225189208984\n",
      "Epoch 26, Loss: 1.6068617105484009, Accuracy: 55.722225189208984, Test Loss: 1.6181050539016724, Test Accuracy: 54.39285659790039\n",
      "Epoch 27, Loss: 1.604182481765747, Accuracy: 55.7896842956543, Test Loss: 1.600549578666687, Test Accuracy: 56.34127426147461\n",
      "Epoch 28, Loss: 1.602367877960205, Accuracy: 56.11507797241211, Test Loss: 1.6054532527923584, Test Accuracy: 55.66666793823242\n",
      "Epoch 29, Loss: 1.5995515584945679, Accuracy: 56.26984405517578, Test Loss: 1.601332426071167, Test Accuracy: 56.13492202758789\n",
      "Epoch 30, Loss: 1.5990114212036133, Accuracy: 56.472225189208984, Test Loss: 1.601309895515442, Test Accuracy: 56.0476188659668\n",
      "Training mlpbeta\n",
      "Epoch 1, Loss: 1.8984400033950806, Accuracy: 22.992063522338867, Test Loss: 1.840247631072998, Test Accuracy: 29.7341251373291\n",
      "Epoch 2, Loss: 1.78904390335083, Accuracy: 36.777774810791016, Test Loss: 1.743406057357788, Test Accuracy: 43.03571319580078\n",
      "Epoch 3, Loss: 1.7043427228927612, Accuracy: 46.17856979370117, Test Loss: 1.666089653968811, Test Accuracy: 50.36507797241211\n",
      "Epoch 4, Loss: 1.6501396894454956, Accuracy: 51.599205017089844, Test Loss: 1.6267141103744507, Test Accuracy: 54.234130859375\n",
      "Epoch 5, Loss: 1.6149396896362305, Accuracy: 55.21428680419922, Test Loss: 1.596924901008606, Test Accuracy: 56.89682388305664\n",
      "Epoch 6, Loss: 1.588410496711731, Accuracy: 57.47222137451172, Test Loss: 1.578842282295227, Test Accuracy: 58.515869140625\n",
      "Epoch 7, Loss: 1.5728365182876587, Accuracy: 59.2579345703125, Test Loss: 1.5721782445907593, Test Accuracy: 59.17063522338867\n",
      "Epoch 8, Loss: 1.5619627237319946, Accuracy: 60.28174591064453, Test Loss: 1.5548148155212402, Test Accuracy: 60.857139587402344\n",
      "Epoch 9, Loss: 1.5536140203475952, Accuracy: 61.30952072143555, Test Loss: 1.542477011680603, Test Accuracy: 62.2579345703125\n",
      "Epoch 10, Loss: 1.542995810508728, Accuracy: 62.369049072265625, Test Loss: 1.5358431339263916, Test Accuracy: 62.94841003417969\n",
      "Epoch 11, Loss: 1.5309752225875854, Accuracy: 63.662696838378906, Test Loss: 1.518669605255127, Test Accuracy: 64.9206314086914\n",
      "Epoch 12, Loss: 1.5110421180725098, Accuracy: 65.53571319580078, Test Loss: 1.5020818710327148, Test Accuracy: 66.4206314086914\n",
      "Epoch 13, Loss: 1.5055357217788696, Accuracy: 65.90872955322266, Test Loss: 1.5021984577178955, Test Accuracy: 66.37301635742188\n",
      "Epoch 14, Loss: 1.4990745782852173, Accuracy: 66.64285278320312, Test Loss: 1.4953960180282593, Test Accuracy: 66.97618865966797\n",
      "Epoch 15, Loss: 1.491399884223938, Accuracy: 67.36508178710938, Test Loss: 1.494921326637268, Test Accuracy: 66.94047546386719\n",
      "Epoch 16, Loss: 1.4879834651947021, Accuracy: 67.67459869384766, Test Loss: 1.4954001903533936, Test Accuracy: 66.984130859375\n",
      "Epoch 17, Loss: 1.4822951555252075, Accuracy: 68.26984405517578, Test Loss: 1.4809750318527222, Test Accuracy: 68.51190185546875\n",
      "Epoch 18, Loss: 1.4775179624557495, Accuracy: 68.71031188964844, Test Loss: 1.4842007160186768, Test Accuracy: 68.11904907226562\n",
      "Epoch 19, Loss: 1.4767669439315796, Accuracy: 68.8531723022461, Test Loss: 1.474896788597107, Test Accuracy: 68.90079498291016\n",
      "Epoch 20, Loss: 1.4720779657363892, Accuracy: 69.24603271484375, Test Loss: 1.4663043022155762, Test Accuracy: 69.88095092773438\n",
      "Epoch 21, Loss: 1.4676421880722046, Accuracy: 69.7261962890625, Test Loss: 1.4699201583862305, Test Accuracy: 69.51190185546875\n",
      "Epoch 22, Loss: 1.4671574831008911, Accuracy: 69.72222137451172, Test Loss: 1.4625880718231201, Test Accuracy: 70.18650817871094\n",
      "Epoch 23, Loss: 1.4648959636688232, Accuracy: 70.015869140625, Test Loss: 1.4561913013458252, Test Accuracy: 70.9047622680664\n",
      "Epoch 24, Loss: 1.4598095417022705, Accuracy: 70.5, Test Loss: 1.4575778245925903, Test Accuracy: 70.69444274902344\n",
      "Epoch 25, Loss: 1.457698941230774, Accuracy: 70.7420654296875, Test Loss: 1.4544435739517212, Test Accuracy: 71.07540130615234\n",
      "Epoch 26, Loss: 1.4546165466308594, Accuracy: 71.0873031616211, Test Loss: 1.475737452507019, Test Accuracy: 68.73809814453125\n",
      "Epoch 27, Loss: 1.4512815475463867, Accuracy: 71.32539367675781, Test Loss: 1.4469646215438843, Test Accuracy: 71.72618865966797\n",
      "Epoch 28, Loss: 1.4487487077713013, Accuracy: 71.67857360839844, Test Loss: 1.4620920419692993, Test Accuracy: 70.23809814453125\n",
      "Epoch 29, Loss: 1.4471889734268188, Accuracy: 71.85713958740234, Test Loss: 1.4595756530761719, Test Accuracy: 70.40872955322266\n",
      "Epoch 30, Loss: 1.4449617862701416, Accuracy: 71.95237731933594, Test Loss: 1.475815773010254, Test Accuracy: 68.77777862548828\n",
      "Training cnn\n",
      "Epoch 1, Loss: 1.8308807611465454, Accuracy: 31.607141494750977, Test Loss: 1.7299811840057373, Test Accuracy: 42.908729553222656\n",
      "Epoch 2, Loss: 1.6869784593582153, Accuracy: 48.015872955322266, Test Loss: 1.658267617225647, Test Accuracy: 50.77381134033203\n",
      "Epoch 3, Loss: 1.6470104455947876, Accuracy: 51.82539749145508, Test Loss: 1.6362682580947876, Test Accuracy: 52.896827697753906\n",
      "Epoch 4, Loss: 1.6281496286392212, Accuracy: 53.658729553222656, Test Loss: 1.6220096349716187, Test Accuracy: 53.88095474243164\n",
      "Epoch 5, Loss: 1.6003003120422363, Accuracy: 56.42856979370117, Test Loss: 1.5805673599243164, Test Accuracy: 58.54365158081055\n",
      "Epoch 6, Loss: 1.5617550611495972, Accuracy: 60.658729553222656, Test Loss: 1.5504740476608276, Test Accuracy: 61.742061614990234\n",
      "Epoch 7, Loss: 1.5387552976608276, Accuracy: 62.73809814453125, Test Loss: 1.5284833908081055, Test Accuracy: 63.90873336791992\n",
      "Epoch 8, Loss: 1.519079566001892, Accuracy: 64.94841003417969, Test Loss: 1.5164576768875122, Test Accuracy: 65.03174591064453\n",
      "Epoch 9, Loss: 1.5058996677398682, Accuracy: 66.16667175292969, Test Loss: 1.5019001960754395, Test Accuracy: 66.42459869384766\n",
      "Epoch 10, Loss: 1.4969033002853394, Accuracy: 67.0079345703125, Test Loss: 1.4997743368148804, Test Accuracy: 66.59920501708984\n",
      "Epoch 11, Loss: 1.4891400337219238, Accuracy: 67.6468276977539, Test Loss: 1.4935204982757568, Test Accuracy: 67.1468276977539\n",
      "Epoch 12, Loss: 1.4816761016845703, Accuracy: 68.46825408935547, Test Loss: 1.4887871742248535, Test Accuracy: 67.57936096191406\n",
      "Epoch 13, Loss: 1.4754480123519897, Accuracy: 69.1785659790039, Test Loss: 1.480423092842102, Test Accuracy: 68.43254089355469\n",
      "Epoch 14, Loss: 1.472111701965332, Accuracy: 69.36904907226562, Test Loss: 1.4712293148040771, Test Accuracy: 69.43650817871094\n",
      "Epoch 15, Loss: 1.4674572944641113, Accuracy: 69.8373031616211, Test Loss: 1.4659916162490845, Test Accuracy: 69.96428680419922\n",
      "Epoch 16, Loss: 1.4620692729949951, Accuracy: 70.39682006835938, Test Loss: 1.4729869365692139, Test Accuracy: 69.14285278320312\n",
      "Epoch 17, Loss: 1.458878993988037, Accuracy: 70.73809814453125, Test Loss: 1.4703295230865479, Test Accuracy: 69.46428680419922\n",
      "Epoch 18, Loss: 1.4568724632263184, Accuracy: 71.0, Test Loss: 1.4598201513290405, Test Accuracy: 70.49603271484375\n",
      "Epoch 19, Loss: 1.4522446393966675, Accuracy: 71.35713958740234, Test Loss: 1.457047939300537, Test Accuracy: 70.8531723022461\n",
      "Epoch 20, Loss: 1.4504777193069458, Accuracy: 71.54762268066406, Test Loss: 1.4532493352890015, Test Accuracy: 71.18254089355469\n",
      "Epoch 21, Loss: 1.4481775760650635, Accuracy: 71.73412322998047, Test Loss: 1.4510802030563354, Test Accuracy: 71.44444274902344\n",
      "Epoch 22, Loss: 1.4458389282226562, Accuracy: 72.04364776611328, Test Loss: 1.4606431722640991, Test Accuracy: 70.32142639160156\n",
      "Epoch 23, Loss: 1.4433872699737549, Accuracy: 72.27777862548828, Test Loss: 1.457004189491272, Test Accuracy: 70.79761505126953\n",
      "Epoch 24, Loss: 1.4417228698730469, Accuracy: 72.484130859375, Test Loss: 1.454563856124878, Test Accuracy: 71.05555725097656\n",
      "Epoch 25, Loss: 1.4392038583755493, Accuracy: 72.65079498291016, Test Loss: 1.4410135746002197, Test Accuracy: 72.33333587646484\n",
      "Epoch 26, Loss: 1.4373748302459717, Accuracy: 72.89285278320312, Test Loss: 1.4409587383270264, Test Accuracy: 72.46031951904297\n",
      "Epoch 27, Loss: 1.4348129034042358, Accuracy: 73.11508178710938, Test Loss: 1.4468538761138916, Test Accuracy: 71.80158233642578\n",
      "Epoch 28, Loss: 1.435713291168213, Accuracy: 73.03174591064453, Test Loss: 1.4409191608428955, Test Accuracy: 72.3690414428711\n",
      "Epoch 29, Loss: 1.4324800968170166, Accuracy: 73.31349182128906, Test Loss: 1.4416182041168213, Test Accuracy: 72.3452377319336\n",
      "Epoch 30, Loss: 1.4322649240493774, Accuracy: 73.36508178710938, Test Loss: 1.4313710927963257, Test Accuracy: 73.42857360839844\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "for model in models:\n",
    "    print('Training ' + model)\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        # Train MLP\n",
    "        if model == 'mlp':\n",
    "            for x, y in trainmlp_ds:\n",
    "                train_mlp(x, y)\n",
    "            for x_test, y_test in testmlp_ds:\n",
    "                test_mlp(x_test, y_test)\n",
    "        # Train MLP Beta\n",
    "        elif model == 'mlpbeta':\n",
    "            for x, y in trainmlp_ds:\n",
    "                train_mlpbeta(x, y)\n",
    "            for x_test, y_test in testmlp_ds:\n",
    "                test_mlpbeta(x_test, y_test)\n",
    "        # Train CNN\n",
    "        elif model == 'cnn':\n",
    "            for x, y in traincnn_ds:\n",
    "                train_cnn(x,y)\n",
    "            for x_test, y_test in testcnn_ds:\n",
    "                test_cnn(x_test, y_test)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Loss: {test_loss.result()}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_enc = mlp.get_layer(name='enc')\n",
    "mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "cnn_enc = cnn.get_layer(name='enc')\n",
    "\n",
    "mlp_aligned = mlp_enc(x_train_noise_mlp)\n",
    "mlp_beta_aligned = mlpbeta_enc(x_train_noise_mlp)\n",
    "cnn_aligned = cnn_enc(x_train_noise_cnn)\n",
    "\n",
    "y_train_aligned = np.argmax(y_train_clean, axis=1)[...,np.newaxis]\n",
    "w_mlp, c_mlp,_, _, _ = train_lda(x_train_noise_mlp,y_train_aligned)\n",
    "w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(x_train_noise_mlpbeta,y_train_aligned)\n",
    "w_cnn, c_cnn,_, _, _ = train_lda(x_train_noise_cnn,y_train_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_4.p\n",
      "Test Accuracy: 73.05714416503906\n",
      "Test Accuracy: 67.17143249511719\n",
      "Test Accuracy: 81.17143249511719\n",
      "Test Accuracy: 74.14285278320312\n",
      "Test Accuracy: 91.05714416503906\n",
      "Test Accuracy: 82.25714874267578\n"
     ]
    }
   ],
   "source": [
    "test_grp = 4\n",
    "cv_type = 'manual'\n",
    "n_test = 'partposrealmixeven14'\n",
    "\n",
    "with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "    real_noise_temp, _ = pickle.load(f)\n",
    "\n",
    "_, x_test, _, _, p_test, _ = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,train_grp=test_grp)\n",
    "clean_size = int(np.size(x_test,axis=0))\n",
    "x_test = x_test*emg_scale\n",
    "\n",
    "x_test_noise, x_test_clean, y_test_clean = prd.add_noise(x_test, p_test, sub, n_test, 1, real_noise=real_noise_temp, emg_scale = emg_scale)\n",
    "x_test_cnn, _ = prd.extract_scale(x_test_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_test_mlp = x_test_cnn.reshape(x_test_cnn.shape[0],-1)\n",
    "\n",
    "mlp_test_aligned = mlp_enc(x_test_mlp)\n",
    "mlpbeta_test_aligned = mlpbeta_enc(x_test_mlp)\n",
    "cnn_test_aligned = cnn(x_test_cnn)\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlp(x_test_mlp[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_mlp,c_mlp,mlp_test_aligned[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'MLP Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlp(x_test_mlp[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_mlp,c_mlp,mlp_test_aligned[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'MLP Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlpbeta(x_test_mlp[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'MLP-Beta Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-Beta-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_mlpbeta(x_test_mlp[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'MLP-Beta Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'MLP-Beta-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_cnn(x_test_cnn[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "acc = eval_lda(w_cnn,c_cnn,cnn_test_aligned[:clean_size,...],y_test_clean[:clean_size,...])\n",
    "print(\n",
    "    f'Clean: '\n",
    "    f'CNN Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'CNN-LDA Accuracy: {acc * 100}'\n",
    ")\n",
    "\n",
    "test_accuracy.reset_states()\n",
    "test_cnn(x_test_cnn[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "acc = eval_lda(w_cnn,c_cnn,cnn_test_aligned[clean_size:,...],y_test_clean[clean_size:,...])\n",
    "print(\n",
    "    f'Noisy: '\n",
    "    f'CNN Accuracy: {test_accuracy.result() * 100},'\n",
    "    f'CNN-LDA Accuracy: {acc * 100}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
