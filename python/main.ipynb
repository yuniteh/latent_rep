{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import os\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN, eval_nn, train_mlp, test_mlp, train_mlpbeta, test_mlpbeta, train_cnn, test_cnn\n",
    "import session_new as sess\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,feat,feat_sq = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "d = {'train':'fullallmix4', 'train_grp':2, 'train_scale':5, 'cv_type':'manual','scaler_load':False,'feat_type':'feat'}\n",
    "sub = 2\n",
    "\n",
    "d_in = sess.Sess(**d)\n",
    "\n",
    "ind = (params[:,0] == sub) & (params[:,3] == d_in.train_grp)\n",
    "if np.sum(ind):\n",
    "    trainmlp, validmlp, traincnn, validcnn, y_train_clean, y_valid_clean, x_train_noise_mlp, x_train_noise_cnn, emg_scale, scaler = prd.prep_train_data(d_in,raw,params,sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training \n",
    "sub = 2\n",
    "train_grp = 2\n",
    "n_train = 'fullallmix4'\n",
    "train_scale = 5\n",
    "cv_type = 'manual'\n",
    "scaler_load = False\n",
    "feat_type = 'feat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_2.p\n"
     ]
    }
   ],
   "source": [
    "ind = (params[:,0] == sub) & (params[:,3] == train_grp)\n",
    "\n",
    "x_train, x_test, x_valid, p_train, p_test, p_valid = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,load=True,train_grp=train_grp)\n",
    "\n",
    "emg_scale = np.ones((np.size(x_train,1),1))\n",
    "for i in range(np.size(x_train,1)):\n",
    "    emg_scale[i] = 5/np.max(np.abs(x_train[:,i,:]))\n",
    "x_train = x_train*emg_scale\n",
    "x_valid = x_valid*emg_scale\n",
    "\n",
    "x_train_noise, x_train_clean, y_train_clean = prd.add_noise(x_train, p_train, sub, n_train, train_scale)\n",
    "x_valid_noise, x_valid_clean, y_valid_clean = prd.add_noise(x_valid, p_valid, sub, n_train, train_scale)\n",
    "\n",
    "# shuffle data to make even batches\n",
    "x_train_noise, x_train_clean, y_train_clean = shuffle(x_train_noise, x_train_clean, y_train_clean, random_state = 0)\n",
    "\n",
    "# Extract features\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "x_train_noise_cnn, scaler = prd.extract_scale(x_train_noise,scaler,scaler_load,ft=feat_type,emg_scale=emg_scale) \n",
    "x_valid_noise_cnn, _ = prd.extract_scale(x_valid_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_train_noise_cnn = x_train_noise_cnn.astype('float32')\n",
    "x_valid_noise_cnn = x_valid_noise_cnn.astype('float32')\n",
    "\n",
    "# reshape data for nonconvolutional network\n",
    "x_train_noise_mlp = x_train_noise_cnn.reshape(x_train_noise_cnn.shape[0],-1)\n",
    "x_valid_noise_mlp = x_valid_noise_cnn.reshape(x_valid_noise_cnn.shape[0],-1)\n",
    "\n",
    "# create batches\n",
    "trainmlp = tf.data.Dataset.from_tensor_slices((x_train_noise_mlp, y_train_clean)).batch(128)\n",
    "validmlp = tf.data.Dataset.from_tensor_slices((x_valid_noise_mlp, y_valid_clean)).batch(128)\n",
    "traincnn = tf.data.Dataset.from_tensor_slices((x_train_noise_cnn, y_train_clean)).batch(128)\n",
    "validcnn = tf.data.Dataset.from_tensor_slices((x_valid_noise_cnn, y_valid_clean)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mlp\n",
      "Epoch 1, Loss: 1.9409295320510864, Accuracy: 16.146825790405273, Test Loss: 1.920284390449524, Test Accuracy: 20.428571701049805\n",
      "Epoch 2, Loss: 1.8784139156341553, Accuracy: 26.821428298950195, Test Loss: 1.8290220499038696, Test Accuracy: 30.571428298950195\n",
      "Epoch 3, Loss: 1.8007692098617554, Accuracy: 35.36111068725586, Test Loss: 1.7818107604980469, Test Accuracy: 36.476192474365234\n",
      "Epoch 4, Loss: 1.7651963233947754, Accuracy: 39.603172302246094, Test Loss: 1.7600924968719482, Test Accuracy: 40.238094329833984\n",
      "Epoch 5, Loss: 1.7422057390213013, Accuracy: 42.15079116821289, Test Loss: 1.7425862550735474, Test Accuracy: 41.36507797241211\n",
      "Epoch 6, Loss: 1.7251882553100586, Accuracy: 43.873016357421875, Test Loss: 1.722000002861023, Test Accuracy: 45.28571319580078\n",
      "Epoch 7, Loss: 1.70674467086792, Accuracy: 46.32142639160156, Test Loss: 1.7046308517456055, Test Accuracy: 46.42856979370117\n",
      "Epoch 8, Loss: 1.6951076984405518, Accuracy: 47.404762268066406, Test Loss: 1.694179892539978, Test Accuracy: 47.82539749145508\n",
      "Epoch 9, Loss: 1.6874548196792603, Accuracy: 48.019840240478516, Test Loss: 1.6864700317382812, Test Accuracy: 48.44444274902344\n",
      "Epoch 10, Loss: 1.6819391250610352, Accuracy: 48.583335876464844, Test Loss: 1.6808037757873535, Test Accuracy: 48.904762268066406\n",
      "Training mlpbeta\n",
      "Epoch 1, Loss: 1.874179482460022, Accuracy: 25.920635223388672, Test Loss: 1.7965282201766968, Test Accuracy: 36.68254089355469\n",
      "Epoch 2, Loss: 1.770074486732483, Accuracy: 38.769840240478516, Test Loss: 1.7284737825393677, Test Accuracy: 45.25396728515625\n",
      "Epoch 3, Loss: 1.72963547706604, Accuracy: 43.44841384887695, Test Loss: 1.7268294095993042, Test Accuracy: 42.984127044677734\n",
      "Epoch 4, Loss: 1.7059749364852905, Accuracy: 45.54365158081055, Test Loss: 1.7189761400222778, Test Accuracy: 44.015872955322266\n",
      "Epoch 5, Loss: 1.6952474117279053, Accuracy: 46.69841384887695, Test Loss: 1.712701678276062, Test Accuracy: 44.619049072265625\n",
      "Epoch 6, Loss: 1.6899514198303223, Accuracy: 47.21825408935547, Test Loss: 1.7116862535476685, Test Accuracy: 44.53968048095703\n",
      "Epoch 7, Loss: 1.681726336479187, Accuracy: 47.78174591064453, Test Loss: 1.7061368227005005, Test Accuracy: 44.96825408935547\n",
      "Epoch 8, Loss: 1.6744364500045776, Accuracy: 48.72222137451172, Test Loss: 1.7034900188446045, Test Accuracy: 45.47618865966797\n",
      "Epoch 9, Loss: 1.6652119159698486, Accuracy: 49.66666793823242, Test Loss: 1.695552110671997, Test Accuracy: 46.507938385009766\n",
      "Epoch 10, Loss: 1.6547821760177612, Accuracy: 50.746028900146484, Test Loss: 1.6641185283660889, Test Accuracy: 49.349205017089844\n",
      "Training cnn\n",
      "Epoch 1, Loss: 1.8544358015060425, Accuracy: 29.261905670166016, Test Loss: 1.764367938041687, Test Accuracy: 38.42857360839844\n",
      "Epoch 2, Loss: 1.6725728511810303, Accuracy: 49.72222137451172, Test Loss: 1.7156504392623901, Test Accuracy: 43.619049072265625\n",
      "Epoch 3, Loss: 1.6180983781814575, Accuracy: 55.14285659790039, Test Loss: 1.667463779449463, Test Accuracy: 48.52381134033203\n",
      "Epoch 4, Loss: 1.5875571966171265, Accuracy: 58.04761505126953, Test Loss: 1.6384462118148804, Test Accuracy: 51.634918212890625\n",
      "Epoch 5, Loss: 1.5583831071853638, Accuracy: 61.003971099853516, Test Loss: 1.6023247241973877, Test Accuracy: 55.984127044677734\n",
      "Epoch 6, Loss: 1.5281041860580444, Accuracy: 63.996028900146484, Test Loss: 1.5741904973983765, Test Accuracy: 58.30158615112305\n",
      "Epoch 7, Loss: 1.5076754093170166, Accuracy: 65.98016357421875, Test Loss: 1.5459778308868408, Test Accuracy: 61.76190185546875\n",
      "Epoch 8, Loss: 1.4905205965042114, Accuracy: 67.87301635742188, Test Loss: 1.5381274223327637, Test Accuracy: 62.841270446777344\n",
      "Epoch 9, Loss: 1.4778412580490112, Accuracy: 69.0, Test Loss: 1.5260776281356812, Test Accuracy: 63.57143020629883\n",
      "Epoch 10, Loss: 1.4679569005966187, Accuracy: 69.96031951904297, Test Loss: 1.5099848508834839, Test Accuracy: 65.53968048095703\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "for model in models:\n",
    "    print('Training ' + model)\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        # Train MLP\n",
    "        if model == 'mlp':\n",
    "            for x, y in trainmlp:\n",
    "                train_mlp(x, y, mlp, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "            for x_test, y_test in validmlp:\n",
    "                test_mlp(x_test, y_test, mlp, loss_fn, test_loss, test_accuracy)\n",
    "        # Train MLP Beta\n",
    "        elif model == 'mlpbeta':\n",
    "            for x, y in trainmlp:\n",
    "                train_mlpbeta(x, y, mlp_beta, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "            for x_test, y_test in validmlp:\n",
    "                test_mlpbeta(x_test, y_test, mlp_beta, loss_fn, test_loss, test_accuracy)\n",
    "        # Train CNN\n",
    "        elif model == 'cnn':\n",
    "            for x, y in traincnn:\n",
    "                train_cnn(x, y, cnn, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "            for x_test, y_test in validcnn:\n",
    "                test_cnn(x_test, y_test, cnn, loss_fn, test_loss, test_accuracy)\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1}, '\n",
    "            f'Loss: {train_loss.result()}, '\n",
    "            f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "            f'Test Loss: {test_loss.result()}, '\n",
    "            f'Test Accuracy: {test_accuracy.result() * 100}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_enc = mlp.get_layer(name='enc')\n",
    "mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "cnn_enc = cnn.get_layer(name='enc')\n",
    "\n",
    "mlp_aligned = mlp_enc(x_train_noise_mlp).numpy()\n",
    "mlp_beta_aligned = mlpbeta_enc(x_train_noise_mlp).numpy()\n",
    "cnn_aligned = cnn_enc(x_train_noise_cnn).numpy()\n",
    "\n",
    "y_train_aligned = np.argmax(y_train_clean, axis=1)[...,np.newaxis]\n",
    "w_mlp, c_mlp,_, _, _ = train_lda(mlp_aligned,y_train_aligned)\n",
    "w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(mlp_beta_aligned,y_train_aligned)\n",
    "w_cnn, c_cnn,_, _, _ = train_lda(cnn_aligned,y_train_aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_manual/AB2_traindata_4.p\n",
      "MLP ---- Clean Accuracy: 74.23, Noisy Accuracy: 64.86, LDA Clean Accuracy: 69.86, LDA Noisy Accuracy: 65.14\n",
      "MLPB ---- Clean Accuracy: 76.57, Noisy Accuracy: 64.20, LDA Clean Accuracy: 77.17, LDA Noisy Accuracy: 60.20\n",
      "CNN ---- Clean Accuracy: 85.69, Noisy Accuracy: 74.31, LDA Clean Accuracy: 85.40, LDA Noisy Accuracy: 71.86\n"
     ]
    }
   ],
   "source": [
    "test_grp = 4\n",
    "cv_type = 'manual'\n",
    "n_test = 'partposrealmixeven24'\n",
    "\n",
    "with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "    real_noise_temp, _ = pickle.load(f)\n",
    "\n",
    "_, x_test, _, _, p_test, _ = prd.train_data_split(raw,params,sub,sub_type,dt=cv_type,train_grp=test_grp)\n",
    "clean_size = int(np.size(x_test,axis=0))\n",
    "x_test = x_test*emg_scale\n",
    "\n",
    "x_test_noise, x_test_clean, y_test_clean = prd.add_noise(x_test, p_test, sub, n_test, 1, real_noise=real_noise_temp, emg_scale = emg_scale)\n",
    "x_test_cnn, _ = prd.extract_scale(x_test_noise,scaler,ft=feat_type,emg_scale=emg_scale)\n",
    "x_test_cnn = x_test_cnn.astype('float32')\n",
    "x_test_mlp = x_test_cnn.reshape(x_test_cnn.shape[0],-1)\n",
    "\n",
    "mlp_test_aligned = mlp_enc(x_test_mlp).numpy()\n",
    "mlpbeta_test_aligned = mlpbeta_enc(x_test_mlp).numpy()\n",
    "cnn_test_aligned = cnn_enc(x_test_cnn).numpy()\n",
    "y_test_aligned = np.argmax(y_test_clean, axis=1)[...,np.newaxis]\n",
    "\n",
    "clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test_clean,mlp,clean_size)\n",
    "clean_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "noisy_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'MLP ---- '\n",
    "    f'Clean Accuracy: {clean_acc * 100:.2f}, '\n",
    "    f'Noisy Accuracy: {noisy_acc * 100:.2f}, '\n",
    "    f'LDA Clean Accuracy: {clean_lda * 100:.2f}, '\n",
    "    f'LDA Noisy Accuracy: {noisy_lda * 100:.2f}'\n",
    ")\n",
    "\n",
    "clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test_clean,mlp_beta,clean_size)\n",
    "clean_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "noisy_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'MLPB ---- '\n",
    "    f'Clean Accuracy: {clean_acc * 100:.2f}, '\n",
    "    f'Noisy Accuracy: {noisy_acc * 100:.2f}, '\n",
    "    f'LDA Clean Accuracy: {clean_lda * 100:.2f}, '\n",
    "    f'LDA Noisy Accuracy: {noisy_lda * 100:.2f}'\n",
    ")\n",
    "\n",
    "clean_acc, noisy_acc = eval_nn(x_test_cnn, y_test_clean,cnn,clean_size)\n",
    "clean_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "noisy_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "print(\n",
    "    f'CNN ---- '\n",
    "    f'Clean Accuracy: {clean_acc * 100:.2f}, '\n",
    "    f'Noisy Accuracy: {noisy_acc * 100:.2f}, '\n",
    "    f'LDA Clean Accuracy: {clean_lda * 100:.2f}, '\n",
    "    f'LDA Noisy Accuracy: {noisy_lda * 100:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP ---- Clean Accuracy: 74.86, Noisy Accuracy: 64.49, LDA Clean Accuracy: 49.63, LDA Noisy Accuracy: 50.80\n",
    "MLPB ---- Clean Accuracy: 76.83, Noisy Accuracy: 65.97, LDA Clean Accuracy: 80.29, LDA Noisy Accuracy: 67.94\n",
    "CNN ---- Clean Accuracy: 85.37, Noisy Accuracy: 72.77, LDA Clean Accuracy: 82.91, LDA Noisy Accuracy: 68.17\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
