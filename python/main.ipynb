{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN, eval_nn, train_mlp, test_mlp, train_mlpbeta, test_mlpbeta, train_cnn, test_cnn\n",
    "import session_new as session\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,_,_ = pickle.load(f)\n",
    "\n",
    "n_subs = np.max(params[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "mlp(x_train_mlp)\n",
    "mlp_beta(x_train_mlp)\n",
    "cnn(x_train_cnn)\n",
    "\n",
    "mlp.save_weights('mlp_init_2.h5')\n",
    "mlp_beta.save_weights('mlpbeta_init_2.h5')\n",
    "cnn.save_weights('cnn_init_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_all/AB3_traindata_2.p\n",
      "Training mlp\n",
      "WARNING:tensorflow:Layer mlp is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.77, Accuracy: 32.27, \n",
      "Training mlpbeta\n",
      "WARNING:tensorflow:Layer ml_pbeta is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.33, Accuracy: 54.41, \n",
      "Training cnn\n",
      "WARNING:tensorflow:Layer cnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-12cc9305b67a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cnn'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraincnn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                         \u001b[0mtrain_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                     \u001b[1;31m# for x_test, y_test in validcnn:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                     \u001b[1;31m#     test_cnn(x_test, y_test, cnn, loss_fn, test_loss, test_accuracy)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tf-2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_sess = {'sub_type':sub_type,'train':'fullallmix4', 'train_grp':2, 'train_scale':5, 'cv_type':'all','scaler_load':False,'feat_type':'feat','epochs':30}\n",
    "test_sess = {'test_grp':4, 'test':'partposrealmixeven24'}\n",
    "sess = session.Sess(**train_sess)\n",
    "sess.update(**test_sess)\n",
    "n_subs = 4#np.max(params[:,0])\n",
    "\n",
    "for sub in range(3,n_subs+1):\n",
    "    sess.sub = sub\n",
    "    # tf.keras.backend.clear_session()\n",
    "\n",
    "    if sub > 3:\n",
    "        mlp.load_weights('mlp_init.h5')\n",
    "        mlp_beta.load_weights('mlpbeta_init.h5')\n",
    "        cnn.load_weights('cnn_init.h5')\n",
    "    else:\n",
    "    # Train NNs\n",
    "        mlp = MLP()\n",
    "        mlp_beta = MLPbeta()\n",
    "        cnn = CNN()\n",
    "\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "        test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    # mlp.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # mlp_beta.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    # cnn.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "    train_ind = (params[:,0] == sess.sub) & (params[:,3] == sess.train_grp)\n",
    "    if np.sum(train_ind):\n",
    "        trainmlp, validmlp, traincnn, validcnn, y_train, y_valid, x_train_mlp, x_train_cnn, x_train_lda, y_train_lda, x_train_aug, y_train_aug = prd.prep_train_data(sess,raw,params)\n",
    "        with open('noisedata_all_emgscalelim/AB' + str(sub) + '_grp_2_fullallmix4_5.p','rb') as f:\n",
    "            scaler, x_train_cnn, _, x_valid_cnn, _, y_train, y_valid, x_train_lda, y_train_lda, x_train_aug, y_train_aug = pickle.load(f)\n",
    "        \n",
    "        x_train_cnn = x_train_cnn.astype('float32')\n",
    "        x_train_mlp = x_train_cnn.reshape(x_train_cnn.shape[0],-1)\n",
    "        \n",
    "        trainmlp = tf.data.Dataset.from_tensor_slices((x_train_mlp, y_train)).shuffle(x_train_mlp.shape[0],reshuffle_each_iteration=True).batch(128)\n",
    "        traincnn = tf.data.Dataset.from_tensor_slices((x_train_cnn, y_train)).shuffle(x_train_cnn.shape[0],reshuffle_each_iteration=True).batch(128)\n",
    "        \n",
    "        sess.scaler = scaler\n",
    "\n",
    "        models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "        # mlp.fit(x_train_mlp,y_train,epochs=30,batch_size=128,verbose=0)\n",
    "        # mlp_beta.fit(x_train_mlp,y_train,epochs=30,batch_size=128,verbose=0)\n",
    "        # cnn.fit(x_train_cnn,y_train,epochs=30,batch_size=128,verbose=0)\n",
    "\n",
    "        for model in models:\n",
    "            print('Training ' + model)\n",
    "            for epoch in range(sess.epochs):\n",
    "                # Reset the metrics at the start of the next epoch\n",
    "                train_loss.reset_states()\n",
    "                train_accuracy.reset_states()\n",
    "                test_loss.reset_states()\n",
    "                test_accuracy.reset_states()\n",
    "\n",
    "                # Train MLP\n",
    "                if model == 'mlp':\n",
    "                    for x, y in trainmlp:\n",
    "                        train_mlp(x, y, mlp, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    # for x_test, y_test in validmlp:\n",
    "                    #     test_mlp(x_test, y_test, mlp, loss_fn, test_loss, test_accuracy)\n",
    "                # Train MLP Beta\n",
    "                elif model == 'mlpbeta':\n",
    "                    for x, y in trainmlp:\n",
    "                        train_mlpbeta(x, y, mlp_beta, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    # for x_test, y_test in validmlp:\n",
    "                    #     test_mlpbeta(x_test, y_test, mlp_beta, loss_fn, test_loss, test_accuracy)\n",
    "                # Train CNN\n",
    "                elif model == 'cnn':\n",
    "                    for x, y in traincnn:\n",
    "                        train_cnn(x, y, cnn, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    # for x_test, y_test in validcnn:\n",
    "                    #     test_cnn(x_test, y_test, cnn, loss_fn, test_loss, test_accuracy)\n",
    "\n",
    "                if epoch == 0 or epoch == sess.epochs-1:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}, '\n",
    "                        f'Loss: {train_loss.result():.2f}, '\n",
    "                        f'Accuracy: {train_accuracy.result() * 100:.2f}, '\n",
    "                        # f'Test Loss: {test_loss.result():.2f}, '\n",
    "                        # f'Test Accuracy: {test_accuracy.result() * 100:.2f}'\n",
    "                    )\n",
    "        \n",
    "        # Train aligned\n",
    "        y_train_aligned = np.argmax(y_train, axis=1)[...,np.newaxis]\n",
    "\n",
    "        mlp_enc = mlp.get_layer(name='enc')\n",
    "        w_mlp, c_mlp,_, _, _ = train_lda(mlp_enc(x_train_mlp).numpy(),y_train_aligned)\n",
    "\n",
    "        mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "        w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(mlpbeta_enc(x_train_mlp).numpy(),y_train_aligned)\n",
    "        \n",
    "        cnn_enc = cnn.get_layer(name='enc')\n",
    "        temp = cnn_enc(x_train_cnn[:x_train_cnn.shape[0]//2,...]).numpy()\n",
    "        temp2 = np.vstack((temp,cnn_enc(x_train_cnn[x_train_cnn.shape[0]//2:,...]).numpy()))\n",
    "        w_cnn, c_cnn,_, _, _ = train_lda(temp2,y_train_aligned)\n",
    "\n",
    "        # Train LDA\n",
    "        w,c, _, _, _ = train_lda(x_train_lda,y_train_lda)\n",
    "        w_aug,c_aug, _, _, _ = train_lda(x_train_aug,y_train_aug)\n",
    "\n",
    "        # Load test data\n",
    "        with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "            real_noise_temp, _ = pickle.load(f)\n",
    "\n",
    "        test_ind = (params[:,0] == sess.sub) & (params[:,3] == sess.test_grp)\n",
    "\n",
    "        if np.sum(test_ind):\n",
    "            x_test_cnn, x_test_mlp, x_test_lda, y_test, clean_size = prd.prep_test_data(sess, raw, params, real_noise_temp)\n",
    "            with open('testdata_all_emgscalelim_noisescalelim/AB' + str(sub) + '_grp_2_partposrealmixeven14_1.p','rb') as f:\n",
    "                x_test_cnn, _, x_test_lda, y_test = pickle.load(f) \n",
    "            x_temp = np.transpose(x_test_lda.reshape((x_test_lda.shape[0],4,-1)),(0,2,1))[...,np.newaxis]\n",
    "            x_test_cnn = scaler.transform(x_temp.reshape(x_temp.shape[0]*x_temp.shape[1],-1)).reshape(x_temp.shape)\n",
    "            x_test_cnn = x_test_cnn.astype('float32')\n",
    "\n",
    "            # Reshape for nonconvolutional SAE\n",
    "            x_test_mlp = x_test_cnn.reshape(x_test_cnn.shape[0],-1)\n",
    "            # Test\n",
    "            mlp_test_aligned = mlp_enc(x_test_mlp).numpy()\n",
    "            mlpbeta_test_aligned = mlpbeta_enc(x_test_mlp).numpy()\n",
    "            cnn_test_aligned = cnn_enc(x_test_cnn).numpy()\n",
    "            y_test_aligned = np.argmax(y_test, axis=1)[...,np.newaxis]\n",
    "\n",
    "            clean_lda = eval_lda(w, c, x_test_lda[:clean_size,...], y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w, c, x_test_lda[clean_size:,...], y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'LDA ---- '\n",
    "                f'Clean: {clean_lda * 100:.2f}, '\n",
    "                f'Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_lda = eval_lda(w_aug, c_aug, x_test_lda[:clean_size,...], y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_aug, c_aug, x_test_lda[clean_size:,...], y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'AUG ---- '\n",
    "                f'Clean: {clean_lda * 100:.2f}, '\n",
    "                f'Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test,mlp,clean_size)\n",
    "            clean_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'MLP ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test,mlp_beta,clean_size)\n",
    "            clean_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'MLPB ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_cnn, y_test,cnn,clean_size)\n",
    "            clean_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'CNN ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "        else:\n",
    "            print('no testing data')\n",
    "    else:\n",
    "        print('no training data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading training data: traindata_all/AB3_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.89, Accuracy: 27.14, \n",
    "Epoch 30, Loss: 0.60, Accuracy: 79.52, \n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.07, Accuracy: 62.94, \n",
    "Epoch 30, Loss: 0.21, Accuracy: 92.56, \n",
    "Training cnn\n",
    "Epoch 1, Loss: 0.91, Accuracy: 71.01, \n",
    "Epoch 30, Loss: 0.17, Accuracy: 93.55, \n",
    "Loading training data: traindata_all/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.14, Noisy: 48.57\n",
    "AUG ---- Clean: 58.46, Noisy: 47.11\n",
    "MLP ---- Clean: 74.49, Noisy: 64.69, LDA Clean: 75.03, LDA Noisy: 64.31\n",
    "MLPB ---- Clean: 81.80, Noisy: 76.69, LDA Clean: 80.74, LDA Noisy: 78.06\n",
    "CNN ---- Clean: 74.09, Noisy: 70.54, LDA Clean: 74.00, LDA Noisy: 70.91\n",
    "Loading training data: traindata_all/AB4_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.78, Accuracy: 28.23, \n",
    "Epoch 30, Loss: 0.76, Accuracy: 70.97, \n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.37, Accuracy: 50.24, \n",
    "Epoch 30, Loss: 0.36, Accuracy: 86.45, \n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.41, Accuracy: 44.92, \n",
    "Epoch 30, Loss: 0.32, Accuracy: 88.22, \n",
    "Loading training data: traindata_all/AB4_traindata_4.p\n",
    "LDA ---- Clean: 86.49, Noisy: 52.31\n",
    "AUG ---- Clean: 50.91, Noisy: 44.46\n",
    "MLP ---- Clean: 81.17, Noisy: 74.23, LDA Clean: 84.43, LDA Noisy: 75.57\n",
    "MLPB ---- Clean: 81.63, Noisy: 76.11, LDA Clean: 83.11, LDA Noisy: 78.14\n",
    "CNN ---- Clean: 82.69, Noisy: 75.34, LDA Clean: 84.51, LDA Noisy: 77.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape shuffle\n",
    "Loading training data: traindata_all/AB2_traindata_2.p\n",
    "Loading training data: traindata_all/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.23, Noisy: 39.63\n",
    "AUG ---- Clean: 56.11, Noisy: 45.74\n",
    "MLP ---- Clean: 80.66, Noisy: 73.31, LDA Clean: 80.66, LDA Noisy: 72.86\n",
    "MLPB ---- Clean: 91.17, Noisy: 78.11, LDA Clean: 90.57, LDA Noisy: 78.23\n",
    "CNN ---- Clean: 91.11, Noisy: 77.63, LDA Clean: 93.89, LDA Noisy: 80.77\n",
    "Loading training data: traindata_all/AB3_traindata_2.p\n",
    "Loading training data: traindata_all/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.14, Noisy: 49.54\n",
    "AUG ---- Clean: 57.69, Noisy: 45.71\n",
    "MLP ---- Clean: 64.17, Noisy: 58.89, LDA Clean: 69.77, LDA Noisy: 61.26\n",
    "MLPB ---- Clean: 76.51, Noisy: 73.29, LDA Clean: 76.57, LDA Noisy: 72.91\n",
    "CNN ---- Clean: 66.86, Noisy: 63.11, LDA Clean: 71.66, LDA Noisy: 67.29\n",
    "Loading training data: traindata_all/AB4_traindata_2.p\n",
    "Loading training data: traindata_all/AB4_traindata_4.p\n",
    "LDA ---- Clean: 86.49, Noisy: 52.06\n",
    "AUG ---- Clean: 51.26, Noisy: 45.43\n",
    "MLP ---- Clean: 75.37, Noisy: 65.43, LDA Clean: 73.60, LDA Noisy: 64.09\n",
    "MLPB ---- Clean: 85.80, Noisy: 78.26, LDA Clean: 83.66, LDA Noisy: 77.89\n",
    "CNN ---- Clean: 77.94, Noisy: 73.23, LDA Clean: 83.57, LDA Noisy: 75.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape no shuffle\n",
    "Loading training data: traindata_all/AB2_traindata_2.p\n",
    "Loading training data: traindata_all/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.23, Noisy: 39.66\n",
    "AUG ---- Clean: 56.91, Noisy: 46.77\n",
    "MLP ---- Clean: 80.77, Noisy: 69.49, LDA Clean: 78.09, LDA Noisy: 67.66\n",
    "MLPB ---- Clean: 89.34, Noisy: 74.29, LDA Clean: 88.66, LDA Noisy: 74.71\n",
    "CNN ---- Clean: 88.34, Noisy: 77.51, LDA Clean: 91.03, LDA Noisy: 79.26\n",
    "Loading training data: traindata_all/AB3_traindata_2.p\n",
    "Loading training data: traindata_all/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.14, Noisy: 49.29\n",
    "AUG ---- Clean: 58.09, Noisy: 46.06\n",
    "MLP ---- Clean: 66.46, Noisy: 60.09, LDA Clean: 67.71, LDA Noisy: 61.11\n",
    "MLPB ---- Clean: 74.23, Noisy: 73.63, LDA Clean: 79.57, LDA Noisy: 75.03\n",
    "CNN ---- Clean: 65.20, Noisy: 62.77, LDA Clean: 67.71, LDA Noisy: 65.40\n",
    "Loading training data: traindata_all/AB4_traindata_2.p\n",
    "Loading training data: traindata_all/AB4_traindata_4.p\n",
    "LDA ---- Clean: 86.49, Noisy: 53.06\n",
    "AUG ---- Clean: 50.54, Noisy: 45.23\n",
    "MLP ---- Clean: 61.31, Noisy: 57.29, LDA Clean: 61.71, LDA Noisy: 57.91\n",
    "MLPB ---- Clean: 83.89, Noisy: 77.69, LDA Clean: 84.91, LDA Noisy: 78.60\n",
    "CNN ---- Clean: 84.49, Noisy: 79.00, LDA Clean: 86.03, LDA Noisy: 78.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model fit shuffle\n",
    "Loading training data: traindata_all/AB2_traindata_2.p\n",
    "Loading training data: traindata_all/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.23, Noisy: 40.06\n",
    "AUG ---- Clean: 56.00, Noisy: 45.74\n",
    "MLP ---- Clean: 79.26, Noisy: 70.54, LDA Clean: 79.54, LDA Noisy: 70.37\n",
    "MLPB ---- Clean: 90.31, Noisy: 78.31, LDA Clean: 90.31, LDA Noisy: 79.89\n",
    "CNN ---- Clean: 89.94, Noisy: 75.54, LDA Clean: 89.14, LDA Noisy: 75.63\n",
    "Loading training data: traindata_all/AB3_traindata_2.p\n",
    "Loading training data: traindata_all/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.14, Noisy: 49.69\n",
    "AUG ---- Clean: 57.46, Noisy: 46.23\n",
    "MLP ---- Clean: 77.43, Noisy: 65.29, LDA Clean: 77.43, LDA Noisy: 70.29\n",
    "MLPB ---- Clean: 83.83, Noisy: 78.54, LDA Clean: 81.23, LDA Noisy: 77.60\n",
    "CNN ---- Clean: 75.00, Noisy: 69.09, LDA Clean: 74.00, LDA Noisy: 69.26\n",
    "Loading training data: traindata_all/AB4_traindata_2.p\n",
    "Loading training data: traindata_all/AB4_traindata_4.p\n",
    "LDA ---- Clean: 86.49, Noisy: 52.60\n",
    "AUG ---- Clean: 50.91, Noisy: 44.31\n",
    "MLP ---- Clean: 82.00, Noisy: 72.91, LDA Clean: 80.69, LDA Noisy: 72.29\n",
    "MLPB ---- Clean: 80.80, Noisy: 75.74, LDA Clean: 81.86, LDA Noisy: 77.63\n",
    "CNN ---- Clean: 85.86, Noisy: 79.29, LDA Clean: 84.94, LDA Noisy: 79.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model fit no shuffle\n",
    "Loading training data: traindata_all/AB2_traindata_2.p\n",
    "Loading training data: traindata_all/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.23, Noisy: 40.11\n",
    "AUG ---- Clean: 56.57, Noisy: 45.71\n",
    "MLP ---- Clean: 77.11, Noisy: 68.03, LDA Clean: 78.86, LDA Noisy: 69.00\n",
    "MLPB ---- Clean: 86.09, Noisy: 75.20, LDA Clean: 90.03, LDA Noisy: 77.83\n",
    "CNN ---- Clean: 88.20, Noisy: 76.83, LDA Clean: 89.46, LDA Noisy: 77.54\n",
    "Loading training data: traindata_all/AB3_traindata_2.p\n",
    "Loading training data: traindata_all/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.14, Noisy: 49.66\n",
    "AUG ---- Clean: 57.54, Noisy: 46.34\n",
    "MLP ---- Clean: 72.97, Noisy: 66.71, LDA Clean: 79.11, LDA Noisy: 71.26\n",
    "MLPB ---- Clean: 73.03, Noisy: 70.89, LDA Clean: 76.94, LDA Noisy: 73.74\n",
    "CNN ---- Clean: 73.83, Noisy: 70.54, LDA Clean: 73.83, LDA Noisy: 68.31\n",
    "Loading training data: traindata_all/AB4_traindata_2.p\n",
    "Loading training data: traindata_all/AB4_traindata_4.p\n",
    "LDA ---- Clean: 86.49, Noisy: 52.06\n",
    "AUG ---- Clean: 51.06, Noisy: 44.74\n",
    "MLP ---- Clean: 75.97, Noisy: 66.54, LDA Clean: 76.23, LDA Noisy: 65.86\n",
    "MLPB ---- Clean: 77.60, Noisy: 72.11, LDA Clean: 81.29, LDA Noisy: 75.17\n",
    "CNN ---- Clean: 86.46, Noisy: 78.54, LDA Clean: 85.00, LDA Noisy: 78.03"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
