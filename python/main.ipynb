{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN, eval_nn, train_mlp, test_mlp, train_mlpbeta, test_mlpbeta, train_cnn, test_cnn\n",
    "import session_new as session\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_type = 'AB'\n",
    "with open('train_data_raw_'  + sub_type + '.p', 'rb') as f:\n",
    "    raw, params,_,_ = pickle.load(f)\n",
    "\n",
    "n_subs = np.max(params[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp_beta = MLPbeta()\n",
    "cnn = CNN()\n",
    "\n",
    "mlp(x_train_mlp)\n",
    "mlp_beta(x_train_mlp)\n",
    "cnn(x_train_cnn)\n",
    "\n",
    "mlp.save_weights('mlp_init_2.h5')\n",
    "mlp_beta.save_weights('mlpbeta_init_2.h5')\n",
    "cnn.save_weights('cnn_init_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data: traindata_all/AB2_traindata_2.p\n",
      "Training mlp\n",
      "WARNING:tensorflow:Layer mlp is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.94, Accuracy: 18.12, Test Loss: 1.90, Test Accuracy: 23.00\n",
      "Epoch 30, Loss: 1.61, Accuracy: 55.68, Test Loss: 1.59, Test Accuracy: 56.81\n",
      "Training mlpbeta\n",
      "WARNING:tensorflow:Layer ml_pbeta is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.86, Accuracy: 26.97, Test Loss: 1.84, Test Accuracy: 29.40\n",
      "Epoch 30, Loss: 1.49, Accuracy: 67.26, Test Loss: 1.49, Test Accuracy: 67.52\n",
      "Training cnn\n",
      "WARNING:tensorflow:Layer cnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 1, Loss: 1.78, Accuracy: 36.97, Test Loss: 1.62, Test Accuracy: 56.68\n",
      "Epoch 30, Loss: 1.34, Accuracy: 82.93, Test Loss: 1.37, Test Accuracy: 79.00\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer enc is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Loading training data: traindata_all/AB2_traindata_4.p\n",
      "LDA ---- Clean: 90.23, Noisy: 58.54\n",
      "AUG ---- Clean: 56.60, Noisy: 53.51\n",
      "MLP ---- Clean: 74.29, Noisy: 70.20, LDA Clean: 75.11, LDA Noisy: 70.06\n",
      "MLPB ---- Clean: 88.03, Noisy: 81.26, LDA Clean: 83.31, LDA Noisy: 78.37\n",
      "CNN ---- Clean: 93.40, Noisy: 86.26, LDA Clean: 88.74, LDA Noisy: 82.71\n",
      "Loading training data: traindata_all/AB3_traindata_2.p\n",
      "Training mlp\n",
      "Epoch 1, Loss: 1.91, Accuracy: 22.37, Test Loss: 1.86, Test Accuracy: 30.17\n",
      "Epoch 30, Loss: 1.52, Accuracy: 64.91, Test Loss: 1.54, Test Accuracy: 62.71\n",
      "Training mlpbeta\n",
      "Epoch 1, Loss: 1.80, Accuracy: 36.35, Test Loss: 1.72, Test Accuracy: 47.54\n",
      "Epoch 30, Loss: 1.34, Accuracy: 82.51, Test Loss: 1.36, Test Accuracy: 80.11\n",
      "Training cnn\n",
      "Epoch 1, Loss: 1.86, Accuracy: 30.53, Test Loss: 1.82, Test Accuracy: 34.79\n",
      "Epoch 30, Loss: 1.51, Accuracy: 65.91, Test Loss: 1.51, Test Accuracy: 65.73\n",
      "Loading training data: traindata_all/AB3_traindata_4.p\n",
      "LDA ---- Clean: 76.14, Noisy: 60.57\n",
      "AUG ---- Clean: 57.60, Noisy: 51.94\n",
      "MLP ---- Clean: 61.40, Noisy: 58.94, LDA Clean: 61.71, LDA Noisy: 57.74\n",
      "MLPB ---- Clean: 75.94, Noisy: 75.40, LDA Clean: 74.86, LDA Noisy: 75.20\n",
      "CNN ---- Clean: 59.86, Noisy: 60.63, LDA Clean: 74.11, LDA Noisy: 69.66\n",
      "Loading training data: traindata_all/AB4_traindata_2.p\n",
      "Training mlp\n",
      "Epoch 1, Loss: 1.95, Accuracy: 14.27, Test Loss: 1.94, Test Accuracy: 14.38\n",
      "Epoch 30, Loss: 1.66, Accuracy: 49.80, Test Loss: 1.66, Test Accuracy: 49.60\n",
      "Training mlpbeta\n",
      "Epoch 1, Loss: 1.90, Accuracy: 23.43, Test Loss: 1.83, Test Accuracy: 32.32\n",
      "Epoch 30, Loss: 1.55, Accuracy: 61.68, Test Loss: 1.55, Test Accuracy: 60.79\n",
      "Training cnn\n",
      "Epoch 1, Loss: 1.88, Accuracy: 24.53, Test Loss: 1.80, Test Accuracy: 34.56\n",
      "Epoch 30, Loss: 1.36, Accuracy: 80.90, Test Loss: 1.40, Test Accuracy: 76.11\n",
      "Loading training data: traindata_all/AB4_traindata_4.p\n",
      "LDA ---- Clean: 86.49, Noisy: 65.66\n",
      "AUG ---- Clean: 51.40, Noisy: 48.03\n",
      "MLP ---- Clean: 65.14, Noisy: 59.40, LDA Clean: 61.51, LDA Noisy: 56.06\n",
      "MLPB ---- Clean: 81.89, Noisy: 78.14, LDA Clean: 79.06, LDA Noisy: 73.94\n",
      "CNN ---- Clean: 87.49, Noisy: 83.14, LDA Clean: 81.94, LDA Noisy: 78.80\n"
     ]
    }
   ],
   "source": [
    "train_sess = {'sub_type':sub_type,'train':'fullallmix4', 'train_grp':2, 'train_scale':5, 'cv_type':'all','scaler_load':False,'feat_type':'feat','epochs':30}\n",
    "test_sess = {'test_grp':4, 'test':'partposrealmixeven24'}\n",
    "sess = session.Sess(**train_sess)\n",
    "sess.update(**test_sess)\n",
    "n_subs = 4#np.max(params[:,0])\n",
    "\n",
    "for sub in range(2,n_subs+1):\n",
    "    sess.sub = sub\n",
    "    # tf.keras.backend.clear_session()\n",
    "\n",
    "    if sub > 2:\n",
    "        mlp.load_weights('mlp_init.h5')\n",
    "        mlp_beta.load_weights('mlpbeta_init.h5')\n",
    "        cnn.load_weights('cnn_init.h5')\n",
    "    else:\n",
    "        # Train NNs\n",
    "        mlp = MLP()\n",
    "        mlp_beta = MLPbeta()\n",
    "        cnn = CNN()\n",
    "\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "        test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    train_ind = (params[:,0] == sess.sub) & (params[:,3] == sess.train_grp)\n",
    "    if np.sum(train_ind):\n",
    "        trainmlp, validmlp, traincnn, validcnn, y_train, y_valid, x_train_mlp, x_train_cnn, x_train_lda, y_train_lda, x_train_aug, y_train_aug = prd.prep_train_data(sess,raw,params)\n",
    "        with open('noisedata_all_emgscalelim/AB' + str(sub) + '_grp_2_fullallmix4_5.p','rb') as f:\n",
    "            scaler, x_train_cnn, _, x_valid_cnn, _, y_train, y_valid, x_train_lda, y_train_lda, x_train_aug, y_train_aug = pickle.load(f)\n",
    "        \n",
    "        x_train_mlp = x_train_cnn.reshape(x_train_cnn.shape[0],-1)\n",
    "        x_valid_mlp = x_valid_cnn.reshape(x_valid_cnn.shape[0],-1)\n",
    "        \n",
    "        trainmlp = tf.data.Dataset.from_tensor_slices((x_train_mlp, y_train)).batch(128)\n",
    "        validmlp = tf.data.Dataset.from_tensor_slices((x_valid_mlp, y_valid)).batch(128)\n",
    "        traincnn = tf.data.Dataset.from_tensor_slices((x_train_cnn, y_train)).batch(128)\n",
    "        validcnn = tf.data.Dataset.from_tensor_slices((x_valid_cnn, y_valid)).batch(128)\n",
    "        \n",
    "        sess.scaler = scaler\n",
    "\n",
    "        models = ['mlp','mlpbeta','cnn']\n",
    "\n",
    "        for model in models:\n",
    "            print('Training ' + model)\n",
    "            for epoch in range(sess.epochs):\n",
    "                # Reset the metrics at the start of the next epoch\n",
    "                train_loss.reset_states()\n",
    "                train_accuracy.reset_states()\n",
    "                test_loss.reset_states()\n",
    "                test_accuracy.reset_states()\n",
    "\n",
    "                # Train MLP\n",
    "                if model == 'mlp':\n",
    "                    for x, y in trainmlp:\n",
    "                        train_mlp(x, y, mlp, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    for x_test, y_test in validmlp:\n",
    "                        test_mlp(x_test, y_test, mlp, loss_fn, test_loss, test_accuracy)\n",
    "                # Train MLP Beta\n",
    "                elif model == 'mlpbeta':\n",
    "                    for x, y in trainmlp:\n",
    "                        train_mlpbeta(x, y, mlp_beta, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    for x_test, y_test in validmlp:\n",
    "                        test_mlpbeta(x_test, y_test, mlp_beta, loss_fn, test_loss, test_accuracy)\n",
    "                # Train CNN\n",
    "                elif model == 'cnn':\n",
    "                    for x, y in traincnn:\n",
    "                        train_cnn(x, y, cnn, loss_fn, optimizer, train_loss, train_accuracy)\n",
    "                    for x_test, y_test in validcnn:\n",
    "                        test_cnn(x_test, y_test, cnn, loss_fn, test_loss, test_accuracy)\n",
    "\n",
    "                if epoch == 0 or epoch == sess.epochs-1:\n",
    "                    print(\n",
    "                        f'Epoch {epoch + 1}, '\n",
    "                        f'Loss: {train_loss.result():.2f}, '\n",
    "                        f'Accuracy: {train_accuracy.result() * 100:.2f}, '\n",
    "                        f'Test Loss: {test_loss.result():.2f}, '\n",
    "                        f'Test Accuracy: {test_accuracy.result() * 100:.2f}'\n",
    "                    )\n",
    "        \n",
    "        # Train aligned\n",
    "        y_train_aligned = np.argmax(y_train, axis=1)[...,np.newaxis]\n",
    "\n",
    "        mlp_enc = mlp.get_layer(name='enc')\n",
    "        w_mlp, c_mlp,_, _, _ = train_lda(mlp_enc(x_train_mlp).numpy(),y_train_aligned)\n",
    "\n",
    "        mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "        w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(mlpbeta_enc(x_train_mlp).numpy(),y_train_aligned)\n",
    "        \n",
    "        cnn_enc = cnn.get_layer(name='enc')\n",
    "        temp = cnn_enc(x_train_cnn[:x_train_cnn.shape[0]//2,...]).numpy()\n",
    "        temp2 = np.vstack((temp,cnn_enc(x_train_cnn[x_train_cnn.shape[0]//2:,...]).numpy()))\n",
    "        w_cnn, c_cnn,_, _, _ = train_lda(temp2,y_train_aligned)\n",
    "\n",
    "        # Train LDA\n",
    "        w,c, _, _, _ = train_lda(x_train_lda,y_train_lda)\n",
    "        w_aug,c_aug, _, _, _ = train_lda(x_train_aug,y_train_aug)\n",
    "\n",
    "        # Load test data\n",
    "        with open('real_noise/all_real_noise.p', 'rb') as f:\n",
    "            real_noise_temp, _ = pickle.load(f)\n",
    "\n",
    "        test_ind = (params[:,0] == sess.sub) & (params[:,3] == sess.test_grp)\n",
    "\n",
    "        if np.sum(test_ind):\n",
    "            x_test_cnn, x_test_mlp, x_test_lda, y_test, clean_size = prd.prep_test_data(sess, raw, params, real_noise_temp)\n",
    "            with open('testdata_all_emgscalelim_noisescalelim/AB' + str(sub) + '_grp_2_partposrealmixeven14_1.p','rb') as f:\n",
    "                x_test_cnn, _, x_test_lda, y_test = pickle.load(f) \n",
    "            x_temp = np.transpose(x_test_lda.reshape((x_test_lda.shape[0],4,-1)),(0,2,1))[...,np.newaxis]\n",
    "            x_test_cnn = scaler.transform(x_temp.reshape(x_temp.shape[0]*x_temp.shape[1],-1)).reshape(x_temp.shape)\n",
    "\n",
    "            # Reshape for nonconvolutional SAE\n",
    "            x_test_mlp = x_test_cnn.reshape(x_test_cnn.shape[0],-1)\n",
    "            # Test\n",
    "            mlp_test_aligned = mlp_enc(x_test_mlp).numpy()\n",
    "            mlpbeta_test_aligned = mlpbeta_enc(x_test_mlp).numpy()\n",
    "            cnn_test_aligned = cnn_enc(x_test_cnn).numpy()\n",
    "            y_test_aligned = np.argmax(y_test, axis=1)[...,np.newaxis]\n",
    "\n",
    "            clean_lda = eval_lda(w, c, x_test_lda[:clean_size,...], y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w, c, x_test_lda[clean_size:,...], y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'LDA ---- '\n",
    "                f'Clean: {clean_lda * 100:.2f}, '\n",
    "                f'Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_lda = eval_lda(w_aug, c_aug, x_test_lda[:clean_size,...], y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_aug, c_aug, x_test_lda[clean_size:,...], y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'AUG ---- '\n",
    "                f'Clean: {clean_lda * 100:.2f}, '\n",
    "                f'Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test,mlp,clean_size)\n",
    "            clean_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_mlp,c_mlp,mlp_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'MLP ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_mlp, y_test,mlp_beta,clean_size)\n",
    "            clean_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_mlpbeta,c_mlpbeta,mlpbeta_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'MLPB ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "\n",
    "            clean_acc, noisy_acc = eval_nn(x_test_cnn, y_test,cnn,clean_size)\n",
    "            clean_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[:clean_size,...],y_test_aligned[:clean_size,...])\n",
    "            noisy_lda = eval_lda(w_cnn,c_cnn,cnn_test_aligned[clean_size:,...],y_test_aligned[clean_size:,...])\n",
    "            print(\n",
    "                f'CNN ---- '\n",
    "                f'Clean: {clean_acc * 100:.2f}, '\n",
    "                f'Noisy: {noisy_acc * 100:.2f}, '\n",
    "                f'LDA Clean: {clean_lda * 100:.2f}, '\n",
    "                f'LDA Noisy: {noisy_lda * 100:.2f}'\n",
    "            )\n",
    "        else:\n",
    "            print('no testing data')\n",
    "    else:\n",
    "        print('no training data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading training data: traindata_all/TR1_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.89, Accuracy: 20.88, Test Loss: 1.83, Test Accuracy: 30.49\n",
    "Epoch 30, Loss: 1.57, Accuracy: 59.02, Test Loss: 1.56, Test Accuracy: 60.24\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.79, Accuracy: 35.47, Test Loss: 1.72, Test Accuracy: 41.67\n",
    "Epoch 30, Loss: 1.46, Accuracy: 70.65, Test Loss: 1.44, Test Accuracy: 72.68\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.74, Accuracy: 41.01, Test Loss: 1.60, Test Accuracy: 57.08\n",
    "Epoch 30, Loss: 1.41, Accuracy: 75.42, Test Loss: 1.43, Test Accuracy: 73.17\n",
    "Loading training data: traindata_all/TR1_traindata_4.p\n",
    "LDA ---- Clean: 63.46, Noisy: 24.32\n",
    "AUG ---- Clean: 43.89, Noisy: 40.93\n",
    "MLP ---- Clean: 49.25, Noisy: 42.11, LDA Clean: 55.54, LDA Noisy: 44.25\n",
    "MLPB ---- Clean: 66.71, Noisy: 54.79, LDA Clean: 61.93, LDA Noisy: 50.71\n",
    "CNN ---- Clean: 62.29, Noisy: 51.64, LDA Clean: 63.25, LDA Noisy: 51.64\n",
    "Loading training data: traindata_all/TR2_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 16.15, Test Loss: 1.94, Test Accuracy: 17.52\n",
    "Epoch 30, Loss: 1.62, Accuracy: 54.85, Test Loss: 1.57, Test Accuracy: 58.52\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.82, Accuracy: 32.19, Test Loss: 1.71, Test Accuracy: 43.29\n",
    "Epoch 30, Loss: 1.49, Accuracy: 67.88, Test Loss: 1.46, Test Accuracy: 70.54\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.83, Accuracy: 33.09, Test Loss: 1.69, Test Accuracy: 49.10\n",
    "Epoch 30, Loss: 1.52, Accuracy: 64.43, Test Loss: 1.51, Test Accuracy: 65.65\n",
    "Loading training data: traindata_all/TR2_traindata_4.p\n",
    "LDA ---- Clean: 84.75, Noisy: 46.36\n",
    "AUG ---- Clean: 63.57, Noisy: 50.82\n",
    "MLP ---- Clean: 64.75, Noisy: 58.39, LDA Clean: 57.61, LDA Noisy: 54.61\n",
    "MLPB ---- Clean: 66.00, Noisy: 61.18, LDA Clean: 77.39, LDA Noisy: 68.86\n",
    "CNN ---- Clean: 65.79, Noisy: 62.96, LDA Clean: 70.86, LDA Noisy: 64.46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loading training data: traindata_manual/AB1_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.92, Accuracy: 20.06, Test Loss: 1.86, Test Accuracy: 26.00\n",
    "Epoch 30, Loss: 1.64, Accuracy: 52.17, Test Loss: 1.79, Test Accuracy: 36.86\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.76, Accuracy: 40.00, Test Loss: 1.73, Test Accuracy: 45.56\n",
    "Epoch 30, Loss: 1.51, Accuracy: 65.74, Test Loss: 1.57, Test Accuracy: 59.14\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.77, Accuracy: 38.78, Test Loss: 1.73, Test Accuracy: 42.92\n",
    "Epoch 30, Loss: 1.31, Accuracy: 86.21, Test Loss: 1.43, Test Accuracy: 72.92\n",
    "no testing data\n",
    "Loading training data: traindata_manual/AB2_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.94, Accuracy: 15.98, Test Loss: 1.94, Test Accuracy: 15.81\n",
    "Epoch 30, Loss: 1.66, Accuracy: 50.40, Test Loss: 1.68, Test Accuracy: 46.89\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.90, Accuracy: 22.63, Test Loss: 1.86, Test Accuracy: 27.02\n",
    "Epoch 30, Loss: 1.42, Accuracy: 74.50, Test Loss: 1.47, Test Accuracy: 68.56\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.90, Accuracy: 23.94, Test Loss: 1.84, Test Accuracy: 31.03\n",
    "Epoch 30, Loss: 1.40, Accuracy: 76.73, Test Loss: 1.48, Test Accuracy: 68.33\n",
    "Loading training data: traindata_manual/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.49, Noisy: 41.23\n",
    "AUG ---- Clean: 57.14, Noisy: 46.51\n",
    "MLP ---- Clean: 59.66, Noisy: 52.14, LDA Clean: 51.51, LDA Noisy: 41.83\n",
    "MLPB ---- Clean: 86.97, Noisy: 73.43, LDA Clean: 82.20, LDA Noisy: 68.66\n",
    "CNN ---- Clean: 84.74, Noisy: 74.00, LDA Clean: 85.66, LDA Noisy: 74.17\n",
    "Loading training data: traindata_manual/AB3_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 15.29, Test Loss: 1.94, Test Accuracy: 14.56\n",
    "Epoch 30, Loss: 1.60, Accuracy: 56.27, Test Loss: 1.61, Test Accuracy: 54.48\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.87, Accuracy: 29.90, Test Loss: 1.81, Test Accuracy: 39.08\n",
    "Epoch 30, Loss: 1.41, Accuracy: 75.97, Test Loss: 1.50, Test Accuracy: 66.37\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.98, Test Loss: 1.94, Test Accuracy: 21.48\n",
    "Epoch 30, Loss: 1.50, Accuracy: 66.58, Test Loss: 1.59, Test Accuracy: 57.27\n",
    "Loading training data: traindata_manual/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.29, Noisy: 50.60\n",
    "AUG ---- Clean: 58.46, Noisy: 47.40\n",
    "MLP ---- Clean: 60.00, Noisy: 58.46, LDA Clean: 58.31, LDA Noisy: 51.09\n",
    "MLPB ---- Clean: 71.74, Noisy: 72.43, LDA Clean: 59.51, LDA Noisy: 59.46\n",
    "CNN ---- Clean: 56.77, Noisy: 54.34, LDA Clean: 65.97, LDA Noisy: 58.80\n",
    "Loading training data: traindata_manual/AB4_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 14.43, Test Loss: 1.95, Test Accuracy: 15.02\n",
    "Epoch 30, Loss: 1.62, Accuracy: 54.47, Test Loss: 1.63, Test Accuracy: 52.00\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.91, Accuracy: 23.23, Test Loss: 1.85, Test Accuracy: 30.92\n",
    "Epoch 30, Loss: 1.50, Accuracy: 66.51, Test Loss: 1.54, Test Accuracy: 61.73\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 17.33, Test Loss: 1.91, Test Accuracy: 22.13\n",
    "Epoch 30, Loss: 1.35, Accuracy: 81.49, Test Loss: 1.48, Test Accuracy: 68.05\n",
    "Loading training data: traindata_manual/AB4_traindata_4.p\n",
    "LDA ---- Clean: 85.94, Noisy: 52.77\n",
    "AUG ---- Clean: 53.20, Noisy: 46.23\n",
    "MLP ---- Clean: 74.80, Noisy: 59.26, LDA Clean: 64.34, LDA Noisy: 50.89\n",
    "MLPB ---- Clean: 73.54, Noisy: 69.54, LDA Clean: 78.69, LDA Noisy: 71.63\n",
    "CNN ---- Clean: 82.23, Noisy: 77.91, LDA Clean: 79.97, LDA Noisy: 73.86\n",
    "Loading training data: traindata_manual/AB5_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 14.54, Test Loss: 1.95, Test Accuracy: 13.56\n",
    "Epoch 30, Loss: 1.63, Accuracy: 53.09, Test Loss: 1.64, Test Accuracy: 51.56\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 17.38, Test Loss: 1.94, Test Accuracy: 15.84\n",
    "Epoch 30, Loss: 1.48, Accuracy: 68.13, Test Loss: 1.51, Test Accuracy: 65.24\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.80, Test Loss: 1.94, Test Accuracy: 17.41\n",
    "Epoch 30, Loss: 1.49, Accuracy: 67.65, Test Loss: 1.51, Test Accuracy: 64.67\n",
    "Loading training data: traindata_manual/AB5_traindata_4.p\n",
    "LDA ---- Clean: 91.09, Noisy: 59.49\n",
    "AUG ---- Clean: 70.37, Noisy: 47.54\n",
    "MLP ---- Clean: 77.54, Noisy: 63.63, LDA Clean: 69.51, LDA Noisy: 58.54\n",
    "MLPB ---- Clean: 79.14, Noisy: 73.23, LDA Clean: 79.26, LDA Noisy: 73.26\n",
    "CNN ---- Clean: 78.34, Noisy: 72.40, LDA Clean: 75.11, LDA Noisy: 67.00\n",
    "Loading training data: traindata_manual/AB1_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.92, Accuracy: 20.06, Test Loss: 1.86, Test Accuracy: 26.00\n",
    "Epoch 30, Loss: 1.64, Accuracy: 52.17, Test Loss: 1.79, Test Accuracy: 36.86\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.76, Accuracy: 40.00, Test Loss: 1.73, Test Accuracy: 45.56\n",
    "Epoch 30, Loss: 1.51, Accuracy: 65.74, Test Loss: 1.57, Test Accuracy: 59.14\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.77, Accuracy: 38.78, Test Loss: 1.73, Test Accuracy: 42.92\n",
    "Epoch 30, Loss: 1.31, Accuracy: 86.21, Test Loss: 1.43, Test Accuracy: 72.92\n",
    "no testing data\n",
    "Loading training data: traindata_manual/AB2_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.94, Accuracy: 15.98, Test Loss: 1.94, Test Accuracy: 15.81\n",
    "Epoch 30, Loss: 1.66, Accuracy: 50.40, Test Loss: 1.68, Test Accuracy: 46.89\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.90, Accuracy: 22.63, Test Loss: 1.86, Test Accuracy: 27.02\n",
    "Epoch 30, Loss: 1.42, Accuracy: 74.50, Test Loss: 1.47, Test Accuracy: 68.56\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.90, Accuracy: 23.94, Test Loss: 1.84, Test Accuracy: 31.03\n",
    "Epoch 30, Loss: 1.40, Accuracy: 76.73, Test Loss: 1.48, Test Accuracy: 68.33\n",
    "Loading training data: traindata_manual/AB2_traindata_4.p\n",
    "LDA ---- Clean: 90.49, Noisy: 41.23\n",
    "AUG ---- Clean: 57.14, Noisy: 46.51\n",
    "MLP ---- Clean: 59.66, Noisy: 52.14, LDA Clean: 51.51, LDA Noisy: 41.83\n",
    "MLPB ---- Clean: 86.97, Noisy: 73.43, LDA Clean: 82.20, LDA Noisy: 68.66\n",
    "CNN ---- Clean: 84.74, Noisy: 74.00, LDA Clean: 85.66, LDA Noisy: 74.17\n",
    "Loading training data: traindata_manual/AB3_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 15.29, Test Loss: 1.94, Test Accuracy: 14.56\n",
    "Epoch 30, Loss: 1.60, Accuracy: 56.27, Test Loss: 1.61, Test Accuracy: 54.48\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.87, Accuracy: 29.90, Test Loss: 1.81, Test Accuracy: 39.08\n",
    "Epoch 30, Loss: 1.41, Accuracy: 75.97, Test Loss: 1.50, Test Accuracy: 66.37\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.98, Test Loss: 1.94, Test Accuracy: 21.48\n",
    "Epoch 30, Loss: 1.50, Accuracy: 66.58, Test Loss: 1.59, Test Accuracy: 57.27\n",
    "Loading training data: traindata_manual/AB3_traindata_4.p\n",
    "LDA ---- Clean: 76.29, Noisy: 50.60\n",
    "AUG ---- Clean: 58.46, Noisy: 47.40\n",
    "MLP ---- Clean: 60.00, Noisy: 58.46, LDA Clean: 58.31, LDA Noisy: 51.09\n",
    "MLPB ---- Clean: 71.74, Noisy: 72.43, LDA Clean: 59.51, LDA Noisy: 59.46\n",
    "CNN ---- Clean: 56.77, Noisy: 54.34, LDA Clean: 65.97, LDA Noisy: 58.80\n",
    "Loading training data: traindata_manual/AB4_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 14.43, Test Loss: 1.95, Test Accuracy: 15.02\n",
    "Epoch 30, Loss: 1.62, Accuracy: 54.47, Test Loss: 1.63, Test Accuracy: 52.00\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.91, Accuracy: 23.23, Test Loss: 1.85, Test Accuracy: 30.92\n",
    "Epoch 30, Loss: 1.50, Accuracy: 66.51, Test Loss: 1.54, Test Accuracy: 61.73\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 17.33, Test Loss: 1.91, Test Accuracy: 22.13\n",
    "Epoch 30, Loss: 1.35, Accuracy: 81.49, Test Loss: 1.48, Test Accuracy: 68.05\n",
    "Loading training data: traindata_manual/AB4_traindata_4.p\n",
    "LDA ---- Clean: 85.94, Noisy: 52.77\n",
    "AUG ---- Clean: 53.20, Noisy: 46.23\n",
    "MLP ---- Clean: 74.80, Noisy: 59.26, LDA Clean: 64.34, LDA Noisy: 50.89\n",
    "MLPB ---- Clean: 73.54, Noisy: 69.54, LDA Clean: 78.69, LDA Noisy: 71.63\n",
    "CNN ---- Clean: 82.23, Noisy: 77.91, LDA Clean: 79.97, LDA Noisy: 73.86\n",
    "Loading training data: traindata_manual/AB5_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 14.54, Test Loss: 1.95, Test Accuracy: 13.56\n",
    "Epoch 30, Loss: 1.63, Accuracy: 53.09, Test Loss: 1.64, Test Accuracy: 51.56\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 17.38, Test Loss: 1.94, Test Accuracy: 15.84\n",
    "Epoch 30, Loss: 1.48, Accuracy: 68.13, Test Loss: 1.51, Test Accuracy: 65.24\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.80, Test Loss: 1.94, Test Accuracy: 17.41\n",
    "Epoch 30, Loss: 1.49, Accuracy: 67.65, Test Loss: 1.51, Test Accuracy: 64.67\n",
    "Loading training data: traindata_manual/AB5_traindata_4.p\n",
    "LDA ---- Clean: 91.09, Noisy: 59.49\n",
    "AUG ---- Clean: 70.37, Noisy: 47.54\n",
    "MLP ---- Clean: 77.54, Noisy: 63.63, LDA Clean: 69.51, LDA Noisy: 58.54\n",
    "MLPB ---- Clean: 79.14, Noisy: 73.23, LDA Clean: 79.26, LDA Noisy: 73.26\n",
    "CNN ---- Clean: 78.34, Noisy: 72.40, LDA Clean: 75.11, LDA Noisy: 67.00\n",
    "Loading training data: traindata_manual/AB10_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 15.08, Test Loss: 1.95, Test Accuracy: 15.48\n",
    "Epoch 30, Loss: 1.62, Accuracy: 53.96, Test Loss: 1.59, Test Accuracy: 56.59\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 19.71, Test Loss: 1.91, Test Accuracy: 27.05\n",
    "Epoch 30, Loss: 1.40, Accuracy: 76.52, Test Loss: 1.38, Test Accuracy: 78.08\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 21.37, Test Loss: 1.94, Test Accuracy: 25.35\n",
    "Epoch 30, Loss: 1.34, Accuracy: 83.16, Test Loss: 1.33, Test Accuracy: 83.27\n",
    "Loading training data: traindata_manual/AB10_traindata_4.p\n",
    "LDA ---- Clean: 84.00, Noisy: 42.89\n",
    "AUG ---- Clean: 61.57, Noisy: 53.54\n",
    "MLP ---- Clean: 66.63, Noisy: 54.43, LDA Clean: 68.29, LDA Noisy: 58.91\n",
    "MLPB ---- Clean: 84.51, Noisy: 75.46, LDA Clean: 83.11, LDA Noisy: 75.49\n",
    "CNN ---- Clean: 82.29, Noisy: 75.00, LDA Clean: 80.94, LDA Noisy: 72.37\n",
    "Loading training data: traindata_manual/AB11_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 14.35, Test Loss: 1.95, Test Accuracy: 15.24\n",
    "Epoch 30, Loss: 1.60, Accuracy: 56.78, Test Loss: 1.57, Test Accuracy: 58.92\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.25, Test Loss: 1.91, Test Accuracy: 20.37\n",
    "Epoch 30, Loss: 1.37, Accuracy: 80.21, Test Loss: 1.40, Test Accuracy: 76.35\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 20.92, Test Loss: 1.94, Test Accuracy: 23.05\n",
    "Epoch 30, Loss: 1.51, Accuracy: 65.06, Test Loss: 1.53, Test Accuracy: 63.02\n",
    "Loading training data: traindata_manual/AB11_traindata_4.p\n",
    "LDA ---- Clean: 85.69, Noisy: 52.49\n",
    "AUG ---- Clean: 65.34, Noisy: 50.74\n",
    "MLP ---- Clean: 72.20, Noisy: 64.11, LDA Clean: 59.74, LDA Noisy: 53.94\n",
    "MLPB ---- Clean: 82.03, Noisy: 75.29, LDA Clean: 82.29, LDA Noisy: 74.29\n",
    "CNN ---- Clean: 61.37, Noisy: 57.09, LDA Clean: 71.63, LDA Noisy: 64.26\n",
    "Loading training data: traindata_manual/AB12_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 15.10, Test Loss: 1.95, Test Accuracy: 16.98\n",
    "Epoch 30, Loss: 1.69, Accuracy: 47.33, Test Loss: 1.71, Test Accuracy: 44.11\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 18.50, Test Loss: 1.94, Test Accuracy: 15.71\n",
    "Epoch 30, Loss: 1.51, Accuracy: 65.19, Test Loss: 1.60, Test Accuracy: 56.11\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 19.15, Test Loss: 1.94, Test Accuracy: 22.89\n",
    "Epoch 30, Loss: 1.55, Accuracy: 61.46, Test Loss: 1.66, Test Accuracy: 49.03\n",
    "Loading training data: traindata_manual/AB12_traindata_4.p\n",
    "LDA ---- Clean: 54.09, Noisy: 38.23\n",
    "AUG ---- Clean: 44.00, Noisy: 39.71\n",
    "MLP ---- Clean: 42.03, Noisy: 40.66, LDA Clean: 47.77, LDA Noisy: 43.97\n",
    "MLPB ---- Clean: 53.63, Noisy: 50.71, LDA Clean: 50.00, LDA Noisy: 48.49\n",
    "CNN ---- Clean: 59.43, Noisy: 55.40, LDA Clean: 66.77, LDA Noisy: 58.46\n",
    "Loading training data: traindata_manual/AB13_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 16.41, Test Loss: 1.95, Test Accuracy: 16.95\n",
    "Epoch 30, Loss: 1.70, Accuracy: 45.85, Test Loss: 1.73, Test Accuracy: 43.14\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.94, Accuracy: 16.49, Test Loss: 1.93, Test Accuracy: 19.30\n",
    "Epoch 30, Loss: 1.47, Accuracy: 69.38, Test Loss: 1.50, Test Accuracy: 66.06\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.95, Accuracy: 18.84, Test Loss: 1.94, Test Accuracy: 24.41\n",
    "Epoch 30, Loss: 1.43, Accuracy: 73.38, Test Loss: 1.50, Test Accuracy: 66.51\n",
    "Loading training data: traindata_manual/AB13_traindata_4.p\n",
    "LDA ---- Clean: 81.51, Noisy: 42.29\n",
    "AUG ---- Clean: 41.57, Noisy: 48.40\n",
    "MLP ---- Clean: 51.49, Noisy: 47.43, LDA Clean: 47.34, LDA Noisy: 45.86\n",
    "MLPB ---- Clean: 86.57, Noisy: 77.46, LDA Clean: 87.94, LDA Noisy: 76.43\n",
    "CNN ---- Clean: 67.91, Noisy: 65.91, LDA Clean: 74.46, LDA Noisy: 71.20\n",
    "Loading training data: traindata_manual/AB14_traindata_2.p\n",
    "Training mlp\n",
    "Epoch 1, Loss: 1.95, Accuracy: 15.85, Test Loss: 1.95, Test Accuracy: 15.35\n",
    "Epoch 30, Loss: 1.54, Accuracy: 62.94, Test Loss: 1.57, Test Accuracy: 59.11\n",
    "Training mlpbeta\n",
    "Epoch 1, Loss: 1.89, Accuracy: 24.88, Test Loss: 1.84, Test Accuracy: 29.06\n",
    "Epoch 30, Loss: 1.46, Accuracy: 70.60, Test Loss: 1.45, Test Accuracy: 71.25\n",
    "Training cnn\n",
    "Epoch 1, Loss: 1.94, Accuracy: 24.26, Test Loss: 1.94, Test Accuracy: 28.78\n",
    "Epoch 30, Loss: 1.37, Accuracy: 79.69, Test Loss: 1.41, Test Accuracy: 75.11\n",
    "Loading training data: traindata_manual/AB14_traindata_4.p\n",
    "LDA ---- Clean: 85.49, Noisy: 52.37\n",
    "AUG ---- Clean: 62.40, Noisy: 58.17\n",
    "MLP ---- Clean: 64.06, Noisy: 63.43, LDA Clean: 68.03, LDA Noisy: 70.74\n",
    "MLPB ---- Clean: 76.43, Noisy: 71.29, LDA Clean: 72.71, LDA Noisy: 71.91\n",
    "CNN ---- Clean: 67.57, Noisy: 66.86, LDA Clean: 64.43, LDA Noisy: 66.77"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
