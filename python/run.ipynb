{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf-gpu-1': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf-gpu-1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f346b235aae4bf1eed617d93c95175812ea1c504b547c015e724420a013989e9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Num GPUs Available:  1\nUsing TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from load_data import load_train_data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gpu import set_gpu\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "feat,params,daq = load_train_data('train_data_raw_AB.mat')"
   ]
  },
  {
   "source": [
    "## Run ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 60)                3660      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                1952      \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 231       \n=================================================================\nTotal params: 5,843\nTrainable params: 5,843\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build NN model\n",
    "model = Sequential([\n",
    "    Dense(units=60, input_shape=(60,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=7, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index data\n",
    "# grp = 1 (static), 2 (dynamic), 3 (0g), 4 (400g), 5 (500g), 6 (600g)\n",
    "sub = 15\n",
    "grp = 3\n",
    "ind = (params[:,0] < sub) & (params[:,3] == grp)\n",
    "\n",
    "# Shuffle and split data\n",
    "feat_s, label_s = shuffle(feat[ind,:],params[ind,-2,np.newaxis])\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_s, label_s, test_size=0.3, random_state=0, stratify=label_s)\n",
    "\n",
    "# Reshape and scale data\n",
    "y_train = np.squeeze(y_train-1)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n965/965 - 1s - loss: 1.6391 - accuracy: 0.3937 - val_loss: 1.3493 - val_accuracy: 0.5644\nEpoch 2/30\n965/965 - 1s - loss: 1.2032 - accuracy: 0.6452 - val_loss: 1.0489 - val_accuracy: 0.6994\nEpoch 3/30\n965/965 - 1s - loss: 0.9757 - accuracy: 0.7238 - val_loss: 0.8801 - val_accuracy: 0.7356\nEpoch 4/30\n965/965 - 1s - loss: 0.8430 - accuracy: 0.7514 - val_loss: 0.7850 - val_accuracy: 0.7595\nEpoch 5/30\n965/965 - 1s - loss: 0.7647 - accuracy: 0.7677 - val_loss: 0.7273 - val_accuracy: 0.7679\nEpoch 6/30\n965/965 - 1s - loss: 0.7162 - accuracy: 0.7767 - val_loss: 0.6920 - val_accuracy: 0.7793\nEpoch 7/30\n965/965 - 1s - loss: 0.6828 - accuracy: 0.7845 - val_loss: 0.6617 - val_accuracy: 0.7828\nEpoch 8/30\n965/965 - 1s - loss: 0.6577 - accuracy: 0.7899 - val_loss: 0.6397 - val_accuracy: 0.7878\nEpoch 9/30\n965/965 - 1s - loss: 0.6370 - accuracy: 0.7948 - val_loss: 0.6211 - val_accuracy: 0.7948\nEpoch 10/30\n965/965 - 1s - loss: 0.6192 - accuracy: 0.8001 - val_loss: 0.6033 - val_accuracy: 0.7980\nEpoch 11/30\n965/965 - 1s - loss: 0.6032 - accuracy: 0.8049 - val_loss: 0.5874 - val_accuracy: 0.7994\nEpoch 12/30\n965/965 - 1s - loss: 0.5899 - accuracy: 0.8085 - val_loss: 0.5768 - val_accuracy: 0.8096\nEpoch 13/30\n965/965 - 1s - loss: 0.5770 - accuracy: 0.8120 - val_loss: 0.5672 - val_accuracy: 0.8128\nEpoch 14/30\n965/965 - 1s - loss: 0.5661 - accuracy: 0.8139 - val_loss: 0.5539 - val_accuracy: 0.8125\nEpoch 15/30\n965/965 - 1s - loss: 0.5553 - accuracy: 0.8176 - val_loss: 0.5414 - val_accuracy: 0.8163\nEpoch 16/30\n965/965 - 1s - loss: 0.5451 - accuracy: 0.8203 - val_loss: 0.5316 - val_accuracy: 0.8175\nEpoch 17/30\n965/965 - 1s - loss: 0.5357 - accuracy: 0.8222 - val_loss: 0.5257 - val_accuracy: 0.8216\nEpoch 18/30\n965/965 - 1s - loss: 0.5269 - accuracy: 0.8252 - val_loss: 0.5151 - val_accuracy: 0.8222\nEpoch 19/30\n965/965 - 1s - loss: 0.5190 - accuracy: 0.8279 - val_loss: 0.5074 - val_accuracy: 0.8268\nEpoch 20/30\n965/965 - 1s - loss: 0.5111 - accuracy: 0.8300 - val_loss: 0.5066 - val_accuracy: 0.8280\nEpoch 21/30\n965/965 - 1s - loss: 0.5043 - accuracy: 0.8328 - val_loss: 0.4933 - val_accuracy: 0.8332\nEpoch 22/30\n965/965 - 1s - loss: 0.4973 - accuracy: 0.8359 - val_loss: 0.4877 - val_accuracy: 0.8294\nEpoch 23/30\n965/965 - 1s - loss: 0.4910 - accuracy: 0.8373 - val_loss: 0.4883 - val_accuracy: 0.8341\nEpoch 24/30\n965/965 - 1s - loss: 0.4850 - accuracy: 0.8388 - val_loss: 0.4788 - val_accuracy: 0.8367\nEpoch 25/30\n965/965 - 1s - loss: 0.4791 - accuracy: 0.8400 - val_loss: 0.4725 - val_accuracy: 0.8370\nEpoch 26/30\n965/965 - 1s - loss: 0.4735 - accuracy: 0.8422 - val_loss: 0.4660 - val_accuracy: 0.8364\nEpoch 27/30\n965/965 - 1s - loss: 0.4683 - accuracy: 0.8444 - val_loss: 0.4604 - val_accuracy: 0.8405\nEpoch 28/30\n965/965 - 1s - loss: 0.4632 - accuracy: 0.8457 - val_loss: 0.4563 - val_accuracy: 0.8417\nEpoch 29/30\n965/965 - 1s - loss: 0.4582 - accuracy: 0.8481 - val_loss: 0.4546 - val_accuracy: 0.8475\nEpoch 30/30\n965/965 - 1s - loss: 0.4536 - accuracy: 0.8490 - val_loss: 0.4521 - val_accuracy: 0.8414\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x184a96ae648>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(x=X_train_n, y=y_train, validation_split=0.1, batch_size=32, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.8455102040816327"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Evaluate test set\n",
    "out = np.argmax(model.predict(x=X_test_n), axis = 1)\n",
    "np.sum(out.reshape(y_test.shape) == y_test - 1)/y_test.shape[0]"
   ]
  },
  {
   "source": [
    "## Run LDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda import train_lda, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.7886285714285715"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "sub = 14\n",
    "grp = 3\n",
    "ind = (params[:,0] == sub) & (params[:,3] != grp)\n",
    "\n",
    "# Shuffle and split data\n",
    "X_test_new, y_test_new = shuffle(feat[ind,:],params[ind,-2,np.newaxis])\n",
    "X_test_new = scaler.transform(X_test_new)\n",
    "out = np.argmax(model.predict(x=X_test_new), axis = 1)\n",
    "np.sum(out.reshape(y_test_new.shape) == y_test_new - 1)/y_test_new.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9933333333333333"
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "w,c = train_lda(X_train_n,y_train)\n",
    "X_test_n = scaler.transform(X_test)\n",
    "out = predict(X_test_n,w,c)\n",
    "np.sum(out.reshape(y_test.shape) == y_test-1)/y_test.shape[0]"
   ]
  },
  {
   "source": [
    "## Run CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import process_daq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = process_daq(daq,params)\n",
    "raw = raw[::2,:,:].transpose(2,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_s, label_s = shuffle(raw,params[:,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_s, label_s, test_size=0.3, random_state=0, stratify=label_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0],-1)).reshape(X_test.shape)\n",
    "# X_train = X_train.transpose(1,2,0)\n",
    "# X_test = X_test.transpose(1,2,0)\n",
    "X_train = X_train[...,np.newaxis]\n",
    "X_test = X_test[...,np.newaxis]\n",
    "y_train = np.squeeze(y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(6,100,1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=7, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 6, 100, 32)        320       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 6, 100, 32)        128       \n_________________________________________________________________\nactivation (Activation)      (None, 6, 100, 32)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 3, 50, 32)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 3, 50, 32)         9248      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 3, 50, 32)         128       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 3, 50, 32)         0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 1, 25, 32)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 800)               0         \n_________________________________________________________________\ndense (Dense)                (None, 7)                 5607      \n=================================================================\nTotal params: 15,431\nTrainable params: 15,303\nNon-trainable params: 128\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/10\n11301/11301 - 25s - loss: 0.8935 - accuracy: 0.6905 - val_loss: 0.6401 - val_accuracy: 0.7875\nEpoch 2/10\n11301/11301 - 24s - loss: 0.6191 - accuracy: 0.7935 - val_loss: 0.5533 - val_accuracy: 0.8151\nEpoch 3/10\n11301/11301 - 24s - loss: 0.5472 - accuracy: 0.8198 - val_loss: 0.5149 - val_accuracy: 0.8262\nEpoch 4/10\n11301/11301 - 25s - loss: 0.5065 - accuracy: 0.8332 - val_loss: 0.4940 - val_accuracy: 0.8362\nEpoch 5/10\n11301/11301 - 25s - loss: 0.4802 - accuracy: 0.8424 - val_loss: 0.4633 - val_accuracy: 0.8460\nEpoch 6/10\n11301/11301 - 25s - loss: 0.4604 - accuracy: 0.8489 - val_loss: 0.4551 - val_accuracy: 0.8476\nEpoch 7/10\n11301/11301 - 25s - loss: 0.4449 - accuracy: 0.8539 - val_loss: 0.4521 - val_accuracy: 0.8496\nEpoch 8/10\n11301/11301 - 26s - loss: 0.4334 - accuracy: 0.8579 - val_loss: 0.4385 - val_accuracy: 0.8557\nEpoch 9/10\n11301/11301 - 25s - loss: 0.4239 - accuracy: 0.8618 - val_loss: 0.4265 - val_accuracy: 0.8611\nEpoch 10/10\n11301/11301 - 25s - loss: 0.4151 - accuracy: 0.8652 - val_loss: 0.5053 - val_accuracy: 0.8243\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1dc182a3608>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=16, validation_split=.1, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}