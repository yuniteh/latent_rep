{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('tf-gpu-1': conda)",
   "display_name": "Python 3.7.9 64-bit ('tf-gpu-1': conda)",
   "metadata": {
    "interpreter": {
     "hash": "f346b235aae4bf1eed617d93c95175812ea1c504b547c015e724420a013989e9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\nNum GPUs Available:  1\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from load_data import load_train_data\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gpu import set_gpu\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "feat,params,daq = load_train_data('train_data_raw_AB.mat')"
   ]
  },
  {
   "source": [
    "## Run ANN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense (Dense)                (None, 60)                3660      \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                1952      \n_________________________________________________________________\ndense_2 (Dense)              (None, 7)                 231       \n=================================================================\nTotal params: 5,843\nTrainable params: 5,843\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# Build NN model\n",
    "model = Sequential([\n",
    "    Dense(units=60, input_shape=(60,), activation='relu'),\n",
    "    Dense(units=32, activation='relu'),\n",
    "    Dense(units=7, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Index data\n",
    "# grp = 1 (static), 2 (dynamic), 3 (0g), 4 (400g), 5 (500g), 6 (600g)\n",
    "sub = 14\n",
    "grp = 1\n",
    "ind = (params[:,0] == sub) & (params[:,3] == grp)\n",
    "\n",
    "# Shuffle and split data\n",
    "feat_s, label_s = shuffle(feat[ind,:],params[ind,-2,np.newaxis])\n",
    "X_train, X_test, y_train, y_test = train_test_split(feat_s, label_s, test_size=0.3, random_state=0, stratify=label_s)\n",
    "X_train, X_test = feat_s, feat_s\n",
    "y_train, y_test = label_s, label_s\n",
    "\n",
    "# Reshape and scale data\n",
    "y_train = np.squeeze(y_train-1)\n",
    "y_test = np.squeeze(y_test-1)\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "scaler.fit(X_train)\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n110/110 - 0s - loss: 0.0898 - accuracy: 0.9846\nEpoch 2/30\n110/110 - 0s - loss: 0.0848 - accuracy: 0.9860\nEpoch 3/30\n110/110 - 0s - loss: 0.0802 - accuracy: 0.9857\nEpoch 4/30\n110/110 - 0s - loss: 0.0760 - accuracy: 0.9863\nEpoch 5/30\n110/110 - 0s - loss: 0.0721 - accuracy: 0.9863\nEpoch 6/30\n110/110 - 0s - loss: 0.0686 - accuracy: 0.9866\nEpoch 7/30\n110/110 - 0s - loss: 0.0656 - accuracy: 0.9874\nEpoch 8/30\n110/110 - 0s - loss: 0.0624 - accuracy: 0.9877\nEpoch 9/30\n110/110 - 0s - loss: 0.0597 - accuracy: 0.9886\nEpoch 10/30\n110/110 - 0s - loss: 0.0570 - accuracy: 0.9886\nEpoch 11/30\n110/110 - 0s - loss: 0.0545 - accuracy: 0.9897\nEpoch 12/30\n110/110 - 0s - loss: 0.0520 - accuracy: 0.9906\nEpoch 13/30\n110/110 - 0s - loss: 0.0499 - accuracy: 0.9903\nEpoch 14/30\n110/110 - 0s - loss: 0.0477 - accuracy: 0.9914\nEpoch 15/30\n110/110 - 0s - loss: 0.0461 - accuracy: 0.9917\nEpoch 16/30\n110/110 - 0s - loss: 0.0441 - accuracy: 0.9920\nEpoch 17/30\n110/110 - 0s - loss: 0.0423 - accuracy: 0.9926\nEpoch 18/30\n110/110 - 0s - loss: 0.0407 - accuracy: 0.9926\nEpoch 19/30\n110/110 - 0s - loss: 0.0392 - accuracy: 0.9929\nEpoch 20/30\n110/110 - 0s - loss: 0.0375 - accuracy: 0.9940\nEpoch 21/30\n110/110 - 0s - loss: 0.0361 - accuracy: 0.9940\nEpoch 22/30\n110/110 - 0s - loss: 0.0349 - accuracy: 0.9946\nEpoch 23/30\n110/110 - 0s - loss: 0.0335 - accuracy: 0.9946\nEpoch 24/30\n110/110 - 0s - loss: 0.0323 - accuracy: 0.9940\nEpoch 25/30\n110/110 - 0s - loss: 0.0311 - accuracy: 0.9949\nEpoch 26/30\n110/110 - 0s - loss: 0.0299 - accuracy: 0.9951\nEpoch 27/30\n110/110 - 0s - loss: 0.0291 - accuracy: 0.9951\nEpoch 28/30\n110/110 - 0s - loss: 0.0281 - accuracy: 0.9954\nEpoch 29/30\n110/110 - 0s - loss: 0.0270 - accuracy: 0.9957\nEpoch 30/30\n110/110 - 0s - loss: 0.0260 - accuracy: 0.9954\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x202896b7d48>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Fit model\n",
    "model.fit(x=X_train_n, y=y_train, batch_size=32, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.996"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# Evaluate test set\n",
    "out = np.argmax(model.predict(x=X_test_n), axis = 1)\n",
    "np.sum(out.reshape(y_test.shape) == y_test)/y_test.shape[0]"
   ]
  },
  {
   "source": [
    "## Run LDA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lda import train_lda, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 14\n",
    "grp = 1\n",
    "ind = (params[:,0] == sub) & (params[:,3] == grp)\n",
    "X_train, y_train = shuffle(feat[ind,:],params[ind,-2])\n",
    "# X_train = feat[ind,:]\n",
    "# y_train = params[ind,-2] - 1\n",
    "y_train_lda = y_train[...,np.newaxis] - 1\n",
    "y_test_lda = y_train[...,np.newaxis] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9928571428571429"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "w,c = train_lda(X_train,y_train_lda)\n",
    "# X_test_n = scaler.transform(X_test)\n",
    "out = predict(X_train,w,c)\n",
    "np.sum(out.reshape(y_test_lda.shape) == y_test_lda)/y_test_lda.shape[0]"
   ]
  },
  {
   "source": [
    "## Run CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import process_daq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = process_daq(daq,params)\n",
    "raw = raw[::2,:,:].transpose(2,1,0)\n",
    "sub = 14\n",
    "grp = 1\n",
    "ind = (params[:,0] == sub) & (params[:,3] == grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_s, label_s = shuffle(raw[ind,:,:],params[ind,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat_s, label_s, test_size=0.3, random_state=0, stratify=label_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = feat_s, feat_s\n",
    "y_train, y_test = label_s, label_s\n",
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(X_test.shape[0],-1)).reshape(X_test.shape)\n",
    "# X_train = X_train.transpose(1,2,0)\n",
    "# X_test = X_test.transpose(1,2,0)\n",
    "X_train = X_train[...,np.newaxis]\n",
    "X_test = X_test[...,np.newaxis]\n",
    "y_train = np.squeeze(y_train-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3,3), padding='same', input_shape=(6,100,1)),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPool2D(pool_size=(2,2), strides=2),\n",
    "    Flatten(),\n",
    "    Dense(units=7, activation='softmax'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 6, 100, 32)        320       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 6, 100, 32)        128       \n_________________________________________________________________\nactivation (Activation)      (None, 6, 100, 32)        0         \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 3, 50, 32)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 3, 50, 32)         9248      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 3, 50, 32)         128       \n_________________________________________________________________\nactivation_1 (Activation)    (None, 3, 50, 32)         0         \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 1, 25, 32)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 800)               0         \n_________________________________________________________________\ndense (Dense)                (None, 7)                 5607      \n=================================================================\nTotal params: 15,431\nTrainable params: 15,303\nNon-trainable params: 128\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/30\n110/110 - 0s - loss: 0.4290 - accuracy: 0.8923\nEpoch 2/30\n110/110 - 0s - loss: 0.3940 - accuracy: 0.9043\nEpoch 3/30\n110/110 - 0s - loss: 0.3579 - accuracy: 0.9163\nEpoch 4/30\n110/110 - 0s - loss: 0.3254 - accuracy: 0.9191\nEpoch 5/30\n110/110 - 0s - loss: 0.2941 - accuracy: 0.9357\nEpoch 6/30\n110/110 - 0s - loss: 0.2720 - accuracy: 0.9426\nEpoch 7/30\n110/110 - 0s - loss: 0.2535 - accuracy: 0.9457\nEpoch 8/30\n110/110 - 0s - loss: 0.2353 - accuracy: 0.9491\nEpoch 9/30\n110/110 - 0s - loss: 0.2203 - accuracy: 0.9540\nEpoch 10/30\n110/110 - 0s - loss: 0.2070 - accuracy: 0.9571\nEpoch 11/30\n110/110 - 0s - loss: 0.1950 - accuracy: 0.9591\nEpoch 12/30\n110/110 - 0s - loss: 0.1782 - accuracy: 0.9634\nEpoch 13/30\n110/110 - 0s - loss: 0.1693 - accuracy: 0.9680\nEpoch 14/30\n110/110 - 0s - loss: 0.1625 - accuracy: 0.9674\nEpoch 15/30\n110/110 - 0s - loss: 0.1516 - accuracy: 0.9720\nEpoch 16/30\n110/110 - 0s - loss: 0.1413 - accuracy: 0.9757\nEpoch 17/30\n110/110 - 0s - loss: 0.1370 - accuracy: 0.9740\nEpoch 18/30\n110/110 - 0s - loss: 0.1296 - accuracy: 0.9791\nEpoch 19/30\n110/110 - 0s - loss: 0.1258 - accuracy: 0.9791\nEpoch 20/30\n110/110 - 0s - loss: 0.1156 - accuracy: 0.9829\nEpoch 21/30\n110/110 - 0s - loss: 0.1111 - accuracy: 0.9849\nEpoch 22/30\n110/110 - 0s - loss: 0.1088 - accuracy: 0.9823\nEpoch 23/30\n110/110 - 0s - loss: 0.1008 - accuracy: 0.9854\nEpoch 24/30\n110/110 - 0s - loss: 0.0989 - accuracy: 0.9851\nEpoch 25/30\n110/110 - 0s - loss: 0.0915 - accuracy: 0.9883\nEpoch 26/30\n110/110 - 0s - loss: 0.0905 - accuracy: 0.9871\nEpoch 27/30\n110/110 - 0s - loss: 0.0827 - accuracy: 0.9891\nEpoch 28/30\n110/110 - 0s - loss: 0.0801 - accuracy: 0.9917\nEpoch 29/30\n110/110 - 0s - loss: 0.0794 - accuracy: 0.9909\nEpoch 30/30\n110/110 - 0s - loss: 0.0743 - accuracy: 0.9900\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x19baed97448>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=30, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.9937142857142857"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "out = np.argmax(model.predict(x=X_train), axis = 1)\n",
    "np.sum(out.reshape(y_train.shape) == y_train)/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}