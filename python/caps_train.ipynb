{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from gpu import set_gpu\n",
    "import copy as cp\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from DL_utils import MLP, MLPbeta, CNN, eval_nn, get_train\n",
    "from manual_nn import nn_pass\n",
    "import session_new as session\n",
    "\n",
    "import process_data as prd\n",
    "from lda import train_lda, predict, eval_lda, eval_lda_ch\n",
    "\n",
    "set_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, params = prd.load_caps_train('traindata.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainmlp, traincnn, y_train, x_train_mlp, x_train_cnn, x_train_lda, y_train_lda, x_train_aug, emg_scale, scaler, x_min, x_max = prd.prep_train_caps(data, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.88, Accuracy: 24.55 \n",
      "Epoch 30, Loss: 0.55, Accuracy: 79.33 \n",
      "Epoch 1, Loss: 1.07, Accuracy: 62.46 \n",
      "Epoch 30, Loss: 0.13, Accuracy: 95.28 \n",
      "Epoch 1, Loss: 0.92, Accuracy: 69.97 \n",
      "Epoch 30, Loss: 0.11, Accuracy: 96.03 \n"
     ]
    }
   ],
   "source": [
    "ep = 30\n",
    "n_dof = np.max(params[:,0])\n",
    "    \n",
    "# Train NNs\n",
    "mlp = MLP(n_class=n_dof)\n",
    "mlp_beta = MLPbeta(n_class=n_dof)\n",
    "cnn = CNN(n_class=n_dof)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "trainmlp, traincnn, y_train, x_train_mlp, x_train_cnn, x_train_lda, y_train_lda, x_train_aug, emg_scale, scaler, x_min, x_max = prd.prep_train_caps(data, params)\n",
    "\n",
    "# Train neural networks\n",
    "models = [mlp, mlp_beta, cnn]\n",
    "for model in models:\n",
    "    if isinstance(model,CNN):\n",
    "        ds = traincnn\n",
    "    else:\n",
    "        ds = trainmlp\n",
    "    \n",
    "    train_mod = get_train()\n",
    "\n",
    "    for epoch in range(ep):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "\n",
    "        for x, y in ds:\n",
    "            train_mod(x, y, model, optimizer, train_loss, train_accuracy)\n",
    "\n",
    "        if epoch == 0 or epoch == ep-1:\n",
    "            print(\n",
    "                f'Epoch {epoch + 1}, '\n",
    "                f'Loss: {train_loss.result():.2f}, '\n",
    "                f'Accuracy: {train_accuracy.result() * 100:.2f} '\n",
    "            )\n",
    "    del train_mod\n",
    "\n",
    "# Train aligned LDA\n",
    "y_train_aug = np.argmax(y_train, axis=1)[...,np.newaxis]\n",
    "\n",
    "mlp_enc = mlp.get_layer(name='enc')\n",
    "w_mlp, c_mlp,_, _, _ = train_lda(mlp_enc(x_train_mlp).numpy(),y_train_aug)\n",
    "\n",
    "mlpbeta_enc = mlp_beta.get_layer(name='enc')\n",
    "w_mlpbeta, c_mlpbeta,_, _, _ = train_lda(mlpbeta_enc(x_train_mlp).numpy(),y_train_aug)\n",
    "\n",
    "cnn_enc = cnn.get_layer(name='enc')\n",
    "temp = cnn_enc(x_train_cnn[:x_train_cnn.shape[0]//2,...]).numpy()\n",
    "temp2 = np.vstack((temp,cnn_enc(x_train_cnn[x_train_cnn.shape[0]//2:,...]).numpy()))\n",
    "w_cnn, c_cnn,_, _, _ = train_lda(temp2,y_train_aug)\n",
    "\n",
    "# Train LDA\n",
    "w,c, _, _, _ = train_lda(x_train_lda,y_train_lda)\n",
    "w_aug,c_aug, _, _, _ = train_lda(x_train_aug,y_train_aug)\n",
    "\n",
    "mlp_w = mlp.get_weights()\n",
    "mlpb_w = mlp_beta.get_weights()\n",
    "cnn_w = cnn.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_in = mlpb_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architectures\n",
    "arch = ['RELU1','BN1','RELU2','BN2','RELU3','BN3','DEN','BN4','SOFTMAX']\n",
    "w = {}\n",
    "\n",
    "i = 0\n",
    "for l in arch:\n",
    "    w_layer = []\n",
    "    if 'BN' in l:\n",
    "        w_layer = np.vstack((np.vstack((np.vstack((w_in[i],w_in[i+1])),w_in[i+2])),w_in[i+3]))\n",
    "        i += 4\n",
    "    else:\n",
    "        w_layer = np.vstack((w_in[i],w_in[i+1]))\n",
    "        i += 2\n",
    "    with open('weights/' + l + '.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # write multiple rows\n",
    "        writer.writerows(w_layer)\n",
    "    w[l] = w_layer\n",
    "\n",
    "with open('weights/ARCH.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f,delimiter=',')\n",
    "        # write multiple rows\n",
    "        for i in arch:\n",
    "            writer.writerow([i])\n",
    "\n",
    "fill = len(emg_scale) - len(x_min)\n",
    "temp = np.hstack((np.vstack((x_min[...,np.newaxis],np.zeros((fill,1)))), np.vstack((x_max[...,np.newaxis],np.zeros((fill,1))))))\n",
    "scales = np.hstack((emg_scale,temp))\n",
    "\n",
    "with open('weights/scales.csv', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # write multiple rows\n",
    "        writer.writerows(scales)\n",
    "\n",
    "with open('weights.p','wb') as f:\n",
    "    pickle.dump(emg_scale,f,protocol=2, fix_imports=True)\n",
    "    # pickle.dump([w,arch,emg_scale,x_min,x_max],f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.ones((1,2)).astype('byte')\n",
    "test = 1.2\n",
    "# with open('weights.p','rb') as f:\n",
    "#     emg_scale = pickle.load(f)\n",
    "with open('weights.p','wb') as f:\n",
    "    pickle.dump(test,f,protocol=2, fix_imports=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-5cdf508b52d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtemp_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmlpb_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmlpb_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtemp_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtemp_max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_max\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtemp_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "temp_max = -1*np.ones((1,2))\n",
    "for i in range(len(mlpb_w)):\n",
    "    temp_size = mlpb_w[i].shape\n",
    "    if mlpb_w[i].ndim < 2:\n",
    "        temp_size = temp_size[...,np.newaxis]\n",
    "    temp_max = np.vstack((temp_max,temp_size)).max(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill = len(emg_scale) - len(x_min)\n",
    "temp = np.hstack((np.vstack((x_min[...,np.newaxis],np.zeros((fill,1)))), np.vstack((x_max[...,np.newaxis],np.zeros((fill,1))))))\n",
    "test = np.hstack((emg_scale,temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 246)\n",
      "(246,)\n",
      "(246,)\n",
      "(246,)\n",
      "(246,)\n",
      "(246,)\n",
      "(246, 128)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128,)\n",
      "(128, 16)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16,)\n",
      "(16, 4)\n",
      "(4,)\n",
      "(4,)\n",
      "(4,)\n",
      "(4,)\n",
      "(4,)\n",
      "(4, 7)\n",
      "(7,)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mlpb_w)):\n",
    "    print(mlpb_w[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('mlpb_w.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(mlpb_w)\n",
    "\n",
    "with open('scales.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "\n",
    "    # write multiple rows\n",
    "    writer.writerows(test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a3b9df806ffb9c99dfd499571232d2e3ccda8a03627b2b254cd16611a407c53"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('tf-2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
